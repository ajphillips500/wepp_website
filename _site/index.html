<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Weekly Reports – AJ Phillips Thesis Project</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">AJ Phillips Thesis Project</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page"> 
<span class="menu-text">Weekly</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./dictionary.html"> 
<span class="menu-text">Data Dictionary</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./imgpe.html"> 
<span class="menu-text">iMGPE</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./ref.html"> 
<span class="menu-text">References</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#july-10-2025" id="toc-july-10-2025" class="nav-link active" data-scroll-target="#july-10-2025">July 10, 2025</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  <li><a href="#prep-for-factorial-study" id="toc-prep-for-factorial-study" class="nav-link" data-scroll-target="#prep-for-factorial-study">Prep for Factorial Study</a></li>
  <li><a href="#ddcrp" id="toc-ddcrp" class="nav-link" data-scroll-target="#ddcrp">DDCRP</a>
  <ul class="collapse">
  <li><a href="#june-26-2025" id="toc-june-26-2025" class="nav-link" data-scroll-target="#june-26-2025">June 26, 2025</a></li>
  </ul></li>
  <li><a href="#summary-1" id="toc-summary-1" class="nav-link" data-scroll-target="#summary-1">Summary</a></li>
  <li><a href="#distance-dependent-crp" id="toc-distance-dependent-crp" class="nav-link" data-scroll-target="#distance-dependent-crp">Distance Dependent CRP</a></li>
  <li><a href="#prior-analysis-of-mcrp" id="toc-prior-analysis-of-mcrp" class="nav-link" data-scroll-target="#prior-analysis-of-mcrp">Prior Analysis of MCRP</a></li>
  <li><a href="#monte-carlo-methods-on-new-data" id="toc-monte-carlo-methods-on-new-data" class="nav-link" data-scroll-target="#monte-carlo-methods-on-new-data">Monte Carlo Methods on New Data</a>
  <ul class="collapse">
  <li><a href="#june-19-2025" id="toc-june-19-2025" class="nav-link" data-scroll-target="#june-19-2025">June 19, 2025</a></li>
  </ul></li>
  <li><a href="#summary-2" id="toc-summary-2" class="nav-link" data-scroll-target="#summary-2">Summary</a></li>
  <li><a href="#mcrp-prior" id="toc-mcrp-prior" class="nav-link" data-scroll-target="#mcrp-prior">MCRP Prior</a></li>
  <li><a href="#updated-experimental-results" id="toc-updated-experimental-results" class="nav-link" data-scroll-target="#updated-experimental-results">Updated Experimental Results</a></li>
  <li><a href="#new-simulated-data-set" id="toc-new-simulated-data-set" class="nav-link" data-scroll-target="#new-simulated-data-set">New Simulated Data Set</a>
  <ul class="collapse">
  <li><a href="#june-12-2025" id="toc-june-12-2025" class="nav-link" data-scroll-target="#june-12-2025">June 12, 2025</a></li>
  </ul></li>
  <li><a href="#summary-3" id="toc-summary-3" class="nav-link" data-scroll-target="#summary-3">Summary</a></li>
  <li><a href="#data-model" id="toc-data-model" class="nav-link" data-scroll-target="#data-model">Data Model</a></li>
  <li><a href="#prior-of-mcrp" id="toc-prior-of-mcrp" class="nav-link" data-scroll-target="#prior-of-mcrp">Prior of MCRP</a></li>
  <li><a href="#imgpe-with-stan-slice-sampling" id="toc-imgpe-with-stan-slice-sampling" class="nav-link" data-scroll-target="#imgpe-with-stan-slice-sampling">iMGPE with STAN &amp; Slice Sampling</a>
  <ul class="collapse">
  <li><a href="#may-29-2025" id="toc-may-29-2025" class="nav-link" data-scroll-target="#may-29-2025">May 29, 2025</a></li>
  </ul></li>
  <li><a href="#summary-4" id="toc-summary-4" class="nav-link" data-scroll-target="#summary-4">Summary</a></li>
  <li><a href="#data-model-description" id="toc-data-model-description" class="nav-link" data-scroll-target="#data-model-description">Data Model Description</a></li>
  <li><a href="#the-imgpe-algorithm" id="toc-the-imgpe-algorithm" class="nav-link" data-scroll-target="#the-imgpe-algorithm">The iMGPE Algorithm</a></li>
  <li><a href="#analysis-of-current-results" id="toc-analysis-of-current-results" class="nav-link" data-scroll-target="#analysis-of-current-results">Analysis of Current Results</a></li>
  <li><a href="#single-gp-comparison" id="toc-single-gp-comparison" class="nav-link" data-scroll-target="#single-gp-comparison">Single GP Comparison</a>
  <ul class="collapse">
  <li><a href="#may-15-2025" id="toc-may-15-2025" class="nav-link" data-scroll-target="#may-15-2025">May 15, 2025</a></li>
  </ul></li>
  <li><a href="#summary-5" id="toc-summary-5" class="nav-link" data-scroll-target="#summary-5">Summary</a></li>
  <li><a href="#explanation-of-data-model" id="toc-explanation-of-data-model" class="nav-link" data-scroll-target="#explanation-of-data-model">Explanation of Data Model</a></li>
  <li><a href="#the-imgpe-algorithm-1" id="toc-the-imgpe-algorithm-1" class="nav-link" data-scroll-target="#the-imgpe-algorithm-1">The iMGPE Algorithm</a></li>
  <li><a href="#experiments-on-simulated-data" id="toc-experiments-on-simulated-data" class="nav-link" data-scroll-target="#experiments-on-simulated-data">Experiments on Simulated Data</a></li>
  <li><a href="#experiments-on-motorcycle-data" id="toc-experiments-on-motorcycle-data" class="nav-link" data-scroll-target="#experiments-on-motorcycle-data">Experiments on Motorcycle Data</a>
  <ul class="collapse">
  <li><a href="#may-8-2025" id="toc-may-8-2025" class="nav-link" data-scroll-target="#may-8-2025">May 8, 2025</a></li>
  </ul></li>
  <li><a href="#summary-6" id="toc-summary-6" class="nav-link" data-scroll-target="#summary-6">Summary</a></li>
  <li><a href="#explanation-of-data-model-1" id="toc-explanation-of-data-model-1" class="nav-link" data-scroll-target="#explanation-of-data-model-1">Explanation of Data Model</a></li>
  <li><a href="#credible-interval-investigation" id="toc-credible-interval-investigation" class="nav-link" data-scroll-target="#credible-interval-investigation">Credible interval investigation</a>
  <ul class="collapse">
  <li><a href="#may-1-2025" id="toc-may-1-2025" class="nav-link" data-scroll-target="#may-1-2025">May 1, 2025</a></li>
  </ul></li>
  <li><a href="#summary-7" id="toc-summary-7" class="nav-link" data-scroll-target="#summary-7">Summary</a></li>
  <li><a href="#marginal-data-model" id="toc-marginal-data-model" class="nav-link" data-scroll-target="#marginal-data-model">Marginal Data Model</a></li>
  <li><a href="#posterior-predictive-distribution" id="toc-posterior-predictive-distribution" class="nav-link" data-scroll-target="#posterior-predictive-distribution">Posterior Predictive Distribution</a></li>
  <li><a href="#simulated-example" id="toc-simulated-example" class="nav-link" data-scroll-target="#simulated-example">Simulated Example</a>
  <ul class="collapse">
  <li><a href="#april-24-2025" id="toc-april-24-2025" class="nav-link" data-scroll-target="#april-24-2025">April 24, 2025</a></li>
  </ul></li>
  <li><a href="#summary-8" id="toc-summary-8" class="nav-link" data-scroll-target="#summary-8">Summary</a></li>
  <li><a href="#data-model-1" id="toc-data-model-1" class="nav-link" data-scroll-target="#data-model-1">Data Model</a></li>
  <li><a href="#description-of-simulated-data" id="toc-description-of-simulated-data" class="nav-link" data-scroll-target="#description-of-simulated-data">Description of Simulated Data</a></li>
  <li><a href="#application-of-imgpe" id="toc-application-of-imgpe" class="nav-link" data-scroll-target="#application-of-imgpe">Application of iMGPE</a>
  <ul class="collapse">
  <li><a href="#april-17-2025" id="toc-april-17-2025" class="nav-link" data-scroll-target="#april-17-2025">April 17, 2025</a></li>
  </ul></li>
  <li><a href="#summary-9" id="toc-summary-9" class="nav-link" data-scroll-target="#summary-9">Summary</a></li>
  <li><a href="#data-model-2" id="toc-data-model-2" class="nav-link" data-scroll-target="#data-model-2">Data Model</a></li>
  <li><a href="#handling-alpha-and-phi" id="toc-handling-alpha-and-phi" class="nav-link" data-scroll-target="#handling-alpha-and-phi">Handling Alpha and Phi</a></li>
  <li><a href="#practical-experiment" id="toc-practical-experiment" class="nav-link" data-scroll-target="#practical-experiment">Practical Experiment</a>
  <ul class="collapse">
  <li><a href="#april-10-2025" id="toc-april-10-2025" class="nav-link" data-scroll-target="#april-10-2025">April 10, 2025</a></li>
  </ul></li>
  <li><a href="#summary-10" id="toc-summary-10" class="nav-link" data-scroll-target="#summary-10">Summary</a></li>
  <li><a href="#full-data-model" id="toc-full-data-model" class="nav-link" data-scroll-target="#full-data-model">Full Data Model</a></li>
  <li><a href="#distribution-functions" id="toc-distribution-functions" class="nav-link" data-scroll-target="#distribution-functions">Distribution Functions</a></li>
  <li><a href="#practical-experiments" id="toc-practical-experiments" class="nav-link" data-scroll-target="#practical-experiments">Practical Experiments</a>
  <ul class="collapse">
  <li><a href="#april-3-2025" id="toc-april-3-2025" class="nav-link" data-scroll-target="#april-3-2025">April 3, 2025</a></li>
  </ul></li>
  <li><a href="#summary-11" id="toc-summary-11" class="nav-link" data-scroll-target="#summary-11">Summary</a></li>
  <li><a href="#data-model-3" id="toc-data-model-3" class="nav-link" data-scroll-target="#data-model-3">Data Model</a></li>
  <li><a href="#updated-results" id="toc-updated-results" class="nav-link" data-scroll-target="#updated-results">Updated Results</a>
  <ul class="collapse">
  <li><a href="#march-27-2025" id="toc-march-27-2025" class="nav-link" data-scroll-target="#march-27-2025">March 27, 2025</a></li>
  </ul></li>
  <li><a href="#summary-12" id="toc-summary-12" class="nav-link" data-scroll-target="#summary-12">Summary</a></li>
  <li><a href="#changes-to-dp-parameter-updates" id="toc-changes-to-dp-parameter-updates" class="nav-link" data-scroll-target="#changes-to-dp-parameter-updates">Changes to DP Parameter Updates</a></li>
  <li><a href="#updated-results-1" id="toc-updated-results-1" class="nav-link" data-scroll-target="#updated-results-1">Updated Results</a>
  <ul class="collapse">
  <li><a href="#march-13-2025" id="toc-march-13-2025" class="nav-link" data-scroll-target="#march-13-2025">March 13, 2025</a></li>
  </ul></li>
  <li><a href="#summary-13" id="toc-summary-13" class="nav-link" data-scroll-target="#summary-13">Summary</a></li>
  <li><a href="#initial-results" id="toc-initial-results" class="nav-link" data-scroll-target="#initial-results">Initial Results</a></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions">Conclusions</a>
  <ul class="collapse">
  <li><a href="#february-27-2025" id="toc-february-27-2025" class="nav-link" data-scroll-target="#february-27-2025">February 27, 2025</a></li>
  </ul></li>
  <li><a href="#summary-14" id="toc-summary-14" class="nav-link" data-scroll-target="#summary-14">Summary</a></li>
  <li><a href="#theory-of-algorithm" id="toc-theory-of-algorithm" class="nav-link" data-scroll-target="#theory-of-algorithm">Theory of Algorithm</a></li>
  <li><a href="#future-work" id="toc-future-work" class="nav-link" data-scroll-target="#future-work">Future Work</a>
  <ul class="collapse">
  <li><a href="#february-13-2025" id="toc-february-13-2025" class="nav-link" data-scroll-target="#february-13-2025">February 13, 2025</a></li>
  </ul></li>
  <li><a href="#summary-15" id="toc-summary-15" class="nav-link" data-scroll-target="#summary-15">Summary</a></li>
  <li><a href="#summary-of-alternative-imgpe" id="toc-summary-of-alternative-imgpe" class="nav-link" data-scroll-target="#summary-of-alternative-imgpe">Summary of Alternative iMGPE</a></li>
  <li><a href="#summary-of-variational-inference" id="toc-summary-of-variational-inference" class="nav-link" data-scroll-target="#summary-of-variational-inference">Summary of Variational Inference</a></li>
  <li><a href="#viability-for-qualitative-inputs" id="toc-viability-for-qualitative-inputs" class="nav-link" data-scroll-target="#viability-for-qualitative-inputs">Viability for Qualitative Inputs</a>
  <ul class="collapse">
  <li><a href="#february-6-2025" id="toc-february-6-2025" class="nav-link" data-scroll-target="#february-6-2025">February 6, 2025</a></li>
  </ul></li>
  <li><a href="#summary-16" id="toc-summary-16" class="nav-link" data-scroll-target="#summary-16">Summary</a></li>
  <li><a href="#mixture-of-experts-overview" id="toc-mixture-of-experts-overview" class="nav-link" data-scroll-target="#mixture-of-experts-overview">Mixture of Experts Overview</a></li>
  <li><a href="#infinite-mixture-of-gp-experts" id="toc-infinite-mixture-of-gp-experts" class="nav-link" data-scroll-target="#infinite-mixture-of-gp-experts">Infinite Mixture of GP Experts</a></li>
  <li><a href="#application-to-qualitative-inputs" id="toc-application-to-qualitative-inputs" class="nav-link" data-scroll-target="#application-to-qualitative-inputs">Application to Qualitative Inputs</a>
  <ul class="collapse">
  <li><a href="#january-30-2025" id="toc-january-30-2025" class="nav-link" data-scroll-target="#january-30-2025">January 30, 2025</a></li>
  </ul></li>
  <li><a href="#summary-17" id="toc-summary-17" class="nav-link" data-scroll-target="#summary-17">Summary</a></li>
  <li><a href="#latent-variable-method-example" id="toc-latent-variable-method-example" class="nav-link" data-scroll-target="#latent-variable-method-example">Latent Variable Method Example</a></li>
  <li><a href="#qualitative-gp-with-big-data" id="toc-qualitative-gp-with-big-data" class="nav-link" data-scroll-target="#qualitative-gp-with-big-data">Qualitative GP with Big Data</a>
  <ul class="collapse">
  <li><a href="#january-23-2025" id="toc-january-23-2025" class="nav-link" data-scroll-target="#january-23-2025">January 23, 2025</a></li>
  </ul></li>
  <li><a href="#summary-18" id="toc-summary-18" class="nav-link" data-scroll-target="#summary-18">Summary</a></li>
  <li><a href="#gp-vs-rf-on-simple-data" id="toc-gp-vs-rf-on-simple-data" class="nav-link" data-scroll-target="#gp-vs-rf-on-simple-data">GP vs RF on Simple Data</a>
  <ul class="collapse">
  <li><a href="#january-17-2025" id="toc-january-17-2025" class="nav-link" data-scroll-target="#january-17-2025">January 17, 2025</a></li>
  </ul></li>
  <li><a href="#summary-19" id="toc-summary-19" class="nav-link" data-scroll-target="#summary-19">Summary</a></li>
  <li><a href="#stratified-random-forest-models" id="toc-stratified-random-forest-models" class="nav-link" data-scroll-target="#stratified-random-forest-models">Stratified Random Forest Models</a></li>
  <li><a href="#model-structures-for-factor-fusion" id="toc-model-structures-for-factor-fusion" class="nav-link" data-scroll-target="#model-structures-for-factor-fusion">Model Structures for Factor Fusion</a></li>
  <li><a href="#further-research-options" id="toc-further-research-options" class="nav-link" data-scroll-target="#further-research-options">Further Research Options</a>
  <ul class="collapse">
  <li><a href="#december-19-2024" id="toc-december-19-2024" class="nav-link" data-scroll-target="#december-19-2024">December 19, 2024</a></li>
  </ul></li>
  <li><a href="#summary-20" id="toc-summary-20" class="nav-link" data-scroll-target="#summary-20">Summary</a></li>
  <li><a href="#random-forest-model-comparisons" id="toc-random-forest-model-comparisons" class="nav-link" data-scroll-target="#random-forest-model-comparisons">Random Forest Model Comparisons</a></li>
  <li><a href="#combining-factor-levels" id="toc-combining-factor-levels" class="nav-link" data-scroll-target="#combining-factor-levels">Combining Factor Levels</a></li>
  <li><a href="#spacial-shrinkage-prior-by-cruz-reyes-et-al." id="toc-spacial-shrinkage-prior-by-cruz-reyes-et-al." class="nav-link" data-scroll-target="#spacial-shrinkage-prior-by-cruz-reyes-et-al.">Spacial Shrinkage Prior by Cruz-Reyes et al.</a>
  <ul class="collapse">
  <li><a href="#december-10-2024" id="toc-december-10-2024" class="nav-link" data-scroll-target="#december-10-2024">December 10, 2024</a></li>
  </ul></li>
  <li><a href="#summary-21" id="toc-summary-21" class="nav-link" data-scroll-target="#summary-21">Summary</a></li>
  <li><a href="#crop-by-till" id="toc-crop-by-till" class="nav-link" data-scroll-target="#crop-by-till">Crop by Till</a></li>
  <li><a href="#nle-by-crop-and-till" id="toc-nle-by-crop-and-till" class="nav-link" data-scroll-target="#nle-by-crop-and-till">NLE by Crop and Till</a>
  <ul class="collapse">
  <li><a href="#december-3-2024" id="toc-december-3-2024" class="nav-link" data-scroll-target="#december-3-2024">December 3, 2024</a></li>
  </ul></li>
  <li><a href="#summary-22" id="toc-summary-22" class="nav-link" data-scroll-target="#summary-22">Summary</a></li>
  <li><a href="#gp-of-soil-loss-by-till" id="toc-gp-of-soil-loss-by-till" class="nav-link" data-scroll-target="#gp-of-soil-loss-by-till">GP of Soil Loss by Till</a></li>
  <li><a href="#gp-of-average-detachment-by-crop-and-till" id="toc-gp-of-average-detachment-by-crop-and-till" class="nav-link" data-scroll-target="#gp-of-average-detachment-by-crop-and-till">GP of Average Detachment by Crop and Till</a></li>
  <li><a href="#linear-models-with-interactions" id="toc-linear-models-with-interactions" class="nav-link" data-scroll-target="#linear-models-with-interactions">Linear Models with Interactions</a>
  <ul class="collapse">
  <li><a href="#november-19-2024" id="toc-november-19-2024" class="nav-link" data-scroll-target="#november-19-2024">November 19, 2024</a></li>
  </ul></li>
  <li><a href="#summary-23" id="toc-summary-23" class="nav-link" data-scroll-target="#summary-23">Summary</a></li>
  <li><a href="#latent-variable-approach-to-gp-modeling" id="toc-latent-variable-approach-to-gp-modeling" class="nav-link" data-scroll-target="#latent-variable-approach-to-gp-modeling">Latent Variable Approach to GP Modeling</a></li>
  <li><a href="#additional-exploratory-analysis" id="toc-additional-exploratory-analysis" class="nav-link" data-scroll-target="#additional-exploratory-analysis">Additional Exploratory Analysis</a>
  <ul class="collapse">
  <li><a href="#november-12-2024" id="toc-november-12-2024" class="nav-link" data-scroll-target="#november-12-2024">November 12, 2024</a></li>
  </ul></li>
  <li><a href="#summary-24" id="toc-summary-24" class="nav-link" data-scroll-target="#summary-24">Summary</a></li>
  <li><a href="#soil-loss-prediction-with-random-forest" id="toc-soil-loss-prediction-with-random-forest" class="nav-link" data-scroll-target="#soil-loss-prediction-with-random-forest">Soil Loss Prediction with Random Forest</a></li>
  <li><a href="#gp-nle-by-crop" id="toc-gp-nle-by-crop" class="nav-link" data-scroll-target="#gp-nle-by-crop">GP NLE by Crop</a></li>
  <li><a href="#dpmm-with-unbalanced-cluster-sizes" id="toc-dpmm-with-unbalanced-cluster-sizes" class="nav-link" data-scroll-target="#dpmm-with-unbalanced-cluster-sizes">DPMM with Unbalanced Cluster Sizes</a>
  <ul class="collapse">
  <li><a href="#november-5-2024" id="toc-november-5-2024" class="nav-link" data-scroll-target="#november-5-2024">November 5, 2024</a></li>
  </ul></li>
  <li><a href="#summary-25" id="toc-summary-25" class="nav-link" data-scroll-target="#summary-25">Summary</a></li>
  <li><a href="#dirichlet-process-mixture-modeling" id="toc-dirichlet-process-mixture-modeling" class="nav-link" data-scroll-target="#dirichlet-process-mixture-modeling">Dirichlet Process Mixture Modeling</a></li>
  <li><a href="#tilling-operations-description" id="toc-tilling-operations-description" class="nav-link" data-scroll-target="#tilling-operations-description">Tilling Operations Description</a></li>
  <li><a href="#categorical-and-numeric-data-merge" id="toc-categorical-and-numeric-data-merge" class="nav-link" data-scroll-target="#categorical-and-numeric-data-merge">Categorical and Numeric Data Merge</a>
  <ul class="collapse">
  <li><a href="#october-29-2024" id="toc-october-29-2024" class="nav-link" data-scroll-target="#october-29-2024">October 29, 2024</a></li>
  </ul></li>
  <li><a href="#summary-26" id="toc-summary-26" class="nav-link" data-scroll-target="#summary-26">Summary</a></li>
  <li><a href="#wepp-crops-deep-dive" id="toc-wepp-crops-deep-dive" class="nav-link" data-scroll-target="#wepp-crops-deep-dive">WEPP Crops Deep Dive</a></li>
  <li><a href="#tilling-operations-analysis" id="toc-tilling-operations-analysis" class="nav-link" data-scroll-target="#tilling-operations-analysis">Tilling Operations Analysis</a>
  <ul class="collapse">
  <li><a href="#october-22-2024" id="toc-october-22-2024" class="nav-link" data-scroll-target="#october-22-2024">October 22, 2024</a></li>
  </ul></li>
  <li><a href="#summary-27" id="toc-summary-27" class="nav-link" data-scroll-target="#summary-27">Summary</a></li>
  <li><a href="#qualitative-input-lit-review" id="toc-qualitative-input-lit-review" class="nav-link" data-scroll-target="#qualitative-input-lit-review">Qualitative Input Lit Review</a></li>
  <li><a href="#wepp-qualitative-variables" id="toc-wepp-qualitative-variables" class="nav-link" data-scroll-target="#wepp-qualitative-variables">WEPP Qualitative Variables</a></li>
  <li><a href="#dirichlet-process-example" id="toc-dirichlet-process-example" class="nav-link" data-scroll-target="#dirichlet-process-example">Dirichlet Process Example</a>
  <ul class="collapse">
  <li><a href="#october-15-2024" id="toc-october-15-2024" class="nav-link" data-scroll-target="#october-15-2024">October 15, 2024</a></li>
  </ul></li>
  <li><a href="#summary-28" id="toc-summary-28" class="nav-link" data-scroll-target="#summary-28">Summary</a></li>
  <li><a href="#additive-gp-with-mixed-data" id="toc-additive-gp-with-mixed-data" class="nav-link" data-scroll-target="#additive-gp-with-mixed-data">Additive GP with Mixed Data</a></li>
  <li><a href="#gp-with-gowerpam-clustering" id="toc-gp-with-gowerpam-clustering" class="nav-link" data-scroll-target="#gp-with-gowerpam-clustering">GP with Gower/PAM Clustering</a>
  <ul class="collapse">
  <li><a href="#october-8-2024" id="toc-october-8-2024" class="nav-link" data-scroll-target="#october-8-2024">October 8, 2024</a></li>
  </ul></li>
  <li><a href="#summary-29" id="toc-summary-29" class="nav-link" data-scroll-target="#summary-29">Summary</a>
  <ul class="collapse">
  <li><a href="#gp-with-interaction-variable" id="toc-gp-with-interaction-variable" class="nav-link" data-scroll-target="#gp-with-interaction-variable">GP with Interaction Variable</a></li>
  <li><a href="#gp-with-gower-dist" id="toc-gp-with-gower-dist" class="nav-link" data-scroll-target="#gp-with-gower-dist">GP with Gower Dist</a></li>
  <li><a href="#october-1-2024" id="toc-october-1-2024" class="nav-link" data-scroll-target="#october-1-2024">October 1, 2024</a></li>
  </ul></li>
  <li><a href="#summary-30" id="toc-summary-30" class="nav-link" data-scroll-target="#summary-30">Summary</a></li>
  <li><a href="#covariate-interactions-in-a-gp" id="toc-covariate-interactions-in-a-gp" class="nav-link" data-scroll-target="#covariate-interactions-in-a-gp">Covariate Interactions in a GP</a></li>
  <li><a href="#gp-with-two-factor-inputs" id="toc-gp-with-two-factor-inputs" class="nav-link" data-scroll-target="#gp-with-two-factor-inputs">GP with Two Factor Inputs</a>
  <ul class="collapse">
  <li><a href="#september-24-2024" id="toc-september-24-2024" class="nav-link" data-scroll-target="#september-24-2024">September 24, 2024</a></li>
  </ul></li>
  <li><a href="#summary-31" id="toc-summary-31" class="nav-link" data-scroll-target="#summary-31">Summary</a></li>
  <li><a href="#distances-for-functional-data" id="toc-distances-for-functional-data" class="nav-link" data-scroll-target="#distances-for-functional-data">Distances for Functional Data</a></li>
  <li><a href="#gp-with-categorical-mean-fn" id="toc-gp-with-categorical-mean-fn" class="nav-link" data-scroll-target="#gp-with-categorical-mean-fn">GP with Categorical Mean Fn</a></li>
  <li><a href="#categorical-data-with-similar-levels" id="toc-categorical-data-with-similar-levels" class="nav-link" data-scroll-target="#categorical-data-with-similar-levels">Categorical Data with Similar Levels</a></li>
  <li><a href="#september-17-2024" id="toc-september-17-2024" class="nav-link" data-scroll-target="#september-17-2024">September 17, 2024</a>
  <ul class="collapse">
  <li><a href="#summary-32" id="toc-summary-32" class="nav-link" data-scroll-target="#summary-32">Summary</a></li>
  <li><a href="#clustering-mixed-data" id="toc-clustering-mixed-data" class="nav-link" data-scroll-target="#clustering-mixed-data">Clustering Mixed Data</a></li>
  <li><a href="#gp-with-categorical-data" id="toc-gp-with-categorical-data" class="nav-link" data-scroll-target="#gp-with-categorical-data">GP with Categorical Data</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Weekly Reports</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="july-10-2025" class="level3">
<h3 class="anchored" data-anchor-id="july-10-2025">July 10, 2025</h3>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>This week I built a Quarto website to store my thesis work and weekly reports going forward. I also investigated possible factors for a factorial study of the existing variations of the iMGPE algorithm. Lastly, I began coding an iMGPE algorithm that uses the distance dependent Chinese Restaurant Process.</p>
</section>
<section id="prep-for-factorial-study" class="level2">
<h2 class="anchored" data-anchor-id="prep-for-factorial-study">Prep for Factorial Study</h2>
<p>While I was not able to arrange a factorial study of the iMGPE algorithm, I did put some thought into what levels should be included in one. The first part of the algorithm I can vary is how the GP experts are fit, the options being ML optimization, STAN, and my homemade slice sampler. The next is the priors placed on <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\phi\)</span>, and <span class="math inline">\(\theta\)</span>. A third, discussed by this method’s authors, is to cap the number of points that can be included in a single cluster with the parameter “maxSize”.</p>
<p>I considered a couple of other factors such as the initial cluster assignment and the proposal distributions for the samplers for <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\phi\)</span>, and the GPs. I decided not to include the first since testing indicated that the initial cluster assignment has no real effect on future clusters. Likewise, I decided against the latter since a good proposal distribution shouldn’t affect the output of the sampler.</p>
<p>I settled on testing two plausible priors for <span class="math inline">\(\theta\)</span>: an InverseGamma<span class="math inline">\((1,1)\)</span> distribution and a LogNormal<span class="math inline">\((\mu=0,\sigma^2=1)\)</span> distribution. The lognormal distribution should have heavier tails than the inverse gamma. Both priors were tested with each GP fitting method. Additionally, each GP method was tested with unlimited cluster sizes and with cluster size capped at <span class="math inline">\(50\)</span>.</p>
<p>For the slice sampler version, I completed three runs: one with the default inverse gamma prior on <span class="math inline">\(\theta\)</span>, one with the lognormal prior on <span class="math inline">\(\theta\)</span>, and one with the inverse gamma prior and “maxSize” set to <span class="math inline">\(50\)</span>. The estimated mean functions of each run are plotted below.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot1-7-10-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot2-7-10-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot3-7-10-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>For the STAN version, I completed three runs: one with the default inverse gamma prior on <span class="math inline">\(\theta\)</span>, one with the lognormal prior on <span class="math inline">\(\theta\)</span>, and one with “maxSize” set to <span class="math inline">\(50\)</span>. The estimated mean functions of each run are plotted below.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot4-7-10-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot5-7-10-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot7-7-10-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The clearest takeaway was that my jury-rigged slice sampling method to fit a GP expert is not very effective. The slice sampler version displays much more oversmoothing, to the point of seeming to ignore the data entirely. Interestingly, the slice sampler version seems to break entirely when the <span class="math inline">\(\theta\)</span> prior or the maximum cluster size is changed, while the STAN version handles both just fine.</p>
</section>
<section id="ddcrp" class="level2">
<h2 class="anchored" data-anchor-id="ddcrp">DDCRP</h2>
<p>Based on R code provided by Blei and Frazier, I was able to begin coding a version of the iMGPE algorithm that uses the Distance Dependent Chinese Restaurant Process to cluster the data. This required rewriting the cluster update step, the <span class="math inline">\(\phi\)</span> sampling step, and the posterior prediction step.</p>
<p>The code is not quite functional yet, but it will be soon.</p>
<section id="june-26-2025" class="level3">
<h3 class="anchored" data-anchor-id="june-26-2025">June 26, 2025</h3>
</section>
</section>
<section id="summary-1" class="level2">
<h2 class="anchored" data-anchor-id="summary-1">Summary</h2>
<p>This week I researched variations of the Chinese Restaurant Process similar to the modified CRP presented by Rasmussen and Ghahramani. The MCRP does not appear to be well documented in literature, but I found a similar algorithm called the distance dependent CRP. These variations appear to be a subset of the field of random measures over data partitions.</p>
<p>I also studied the MCRP prior on a new data set with three distinct clusters, allowing me to examine how well the true cluster structure is captured for varying parameter values. As expected, <span class="math inline">\(\phi\)</span> controls the degree to which similar points prefer to cluster together, seen in how two measures of cluster quality improve as <span class="math inline">\(\phi\)</span> decreases.</p>
<p>I continue to test the STAN and slice sampler iMGPE algorithms on strictly positive data sets. The STAN version performs very well, while the slice sampler version struggled.</p>
</section>
<section id="distance-dependent-crp" class="level2">
<h2 class="anchored" data-anchor-id="distance-dependent-crp">Distance Dependent CRP</h2>
<p>There does not appear to be any research about the specific modified Chinese Restaurant Process used by Rasmussen and Ghahramani. However, the authors Blei and Frazier discuss a similar algorithm in their 2010 paper <a href="https://jmlr.org/papers/volume12/blei11a/blei11a.pdf">Distance Dependent Chinese Restaurant Processes</a>. The DD-CRP differs from the regular and modified CRP in that rather than assigning customers to tables (that is, points to clusters) each customer is assigned to sit with another customer or with themselves. The table assignments are derived from these customer assignments by grouping together all customers who can be linked by a sequence of customer pairings into a table.</p>
<p>The conditional probability of customer <span class="math inline">\(i\)</span> being assigned to sit with customer <span class="math inline">\(j\)</span> is dependent on the distance between them. Given <span class="math inline">\(D\)</span>, the pairwise distances between all points, and <span class="math inline">\(\alpha\)</span>, the concentration parameter, the probability of customer assignment <span class="math inline">\(c_i\)</span> is as follows.</p>
<p><span class="math display">\[p(c_i=j|D,\alpha)\propto \begin{cases}
f(d_{ij}) \text{ if } j\neq i\\
\alpha \text{ if } i=j
\end{cases}\]</span></p>
<p>Note that this differs from our MCRP (and the regular CRP) in that the assignment of point <span class="math inline">\(i\)</span> only depends on the distances between customers, and not on the customer or cluster assignments of other points.</p>
<p>The authors demonstrate that the regular CRP can be characterized as a special case of the distance dependent CRP. Furthermore, the regular CRP is the only marginally invariate distance dependent CRP, meaning that marginalizing over a particular customer yields the same probability distribution as if the customer was not included at all. They go on to describe a Gibbs sampling formula for the DD-CRP.</p>
<p>The modified CRP used by Rasmussen and Ghahramani does not appear to have been theoretically explored or justified. While certain properties of the DD-CRP are described, it is not considered in the context of a mixture of experts model. I came across a decent sized array of papers on the more general topic of using random measures to define probability distributions over partitions, of which the CRP, DD-CRP, and probably the MCRP are all examples, but did not have time to synthesize them here.</p>
<p>In summary, I think it may be worthwhile to consider reworking the iMGPE algorithm to use a clustering process that is more clearly defined or easier to work with in terms of theoretical properties. It may be possible to prove, or disprove, qualities such as consistency for the MCRP as well.</p>
</section>
<section id="prior-analysis-of-mcrp" class="level2">
<h2 class="anchored" data-anchor-id="prior-analysis-of-mcrp">Prior Analysis of MCRP</h2>
<p>I tested the prior distribution of the Modified Chinese Restaurant Process on a fully segregated data set with three clusters and 100 points in total. I drew <span class="math inline">\(40\)</span> points from the interval <span class="math inline">\([0,1]\)</span>, <span class="math inline">\(30\)</span> from <span class="math inline">\([3,4]\)</span>, and <span class="math inline">\(30\)</span> from <span class="math inline">\([7,8]\)</span>. Since this data set has a true cluster structure, we can evaluate how well the MCRP captures it through metrics such as cluster purity and the Adjusted Rand Index.</p>
<p>Cluster purity is calculated by assigning each cluster to the class most frequent in that cluster and then finding the percent of the total number of data points that were classified correctly given those assignments. It ranges from <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span> with <span class="math inline">\(1\)</span> being perfect purity, though this is also affected by the number of points and clusters. The Adjusted Rand Index is a measure of the similarity between the generated partition and the true partition of the data set. It ranges from roughly <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span>, with <span class="math inline">\(1\)</span> indicating a perfect match to the true cluster structure and <span class="math inline">\(0\)</span> indicating a fully random cluster assignment.</p>
<p>After correcting a typo in my MCRP code, my initial intuition was borne out. I started with all points assigned to the same cluster and performed <span class="math inline">\(1000\)</span> MCMC updates according to the MCRP algorithm. I repeated this process with a few different initial states, but they did not have a significant effect on the final state. Considering the results, we see both metrics of cluster accuracy improve as <span class="math inline">\(\phi\)</span> gets smaller, while the number of clusters varied only with <span class="math inline">\(\alpha\)</span> staying at about <span class="math inline">\(5\)</span> clusters for <span class="math inline">\(\alpha=1\)</span>, <span class="math inline">\(17\)</span> clusters for <span class="math inline">\(\alpha=3\)</span>, and <span class="math inline">\(34\)</span> clusters for <span class="math inline">\(\alpha=9\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot1-6-26-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot2-6-26-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Note that cluster purity tends to increase slightly as the number of clusters increases (most visible on the third column) since smaller clusters are more likely to be pure even when generated randomly.</p>
</section>
<section id="monte-carlo-methods-on-new-data" class="level2">
<h2 class="anchored" data-anchor-id="monte-carlo-methods-on-new-data">Monte Carlo Methods on New Data</h2>
<p>This week I tested the STAN and slice sampler versions of the iMGPE on strictly positive data sets. The slice sampler version was tested on the data set from last week, where <span class="math inline">\(f(x)=1+0.3x+\frac{\sin(\pi x)}{5x}\)</span>. The STAN version was tested on a similar data set where <span class="math inline">\(f(x)=1+0.5x+\frac{\sin(\pi x)}{2x}\)</span>. The posterior estimate of the function is plotted below for each, with STAN on the left and the slice sampler on the right.</p>
<p>The blue line is the true function path and the red line is the median of the fitted values. The upper grey band shows the <span class="math inline">\(95\%\)</span> credible band over the test set while the lower grey band is a visual aid displaying the width of the credible band with its lower bound fixed at a level below the graph. The mean MC standard error for the <span class="math inline">\(2.5th\)</span>, <span class="math inline">\(50th\)</span> and <span class="math inline">\(97.5th\)</span> quantiles is <span class="math inline">\(0.03\)</span>, <span class="math inline">\(0.02\)</span>, and <span class="math inline">\(0.01\)</span> for the STAN version and <span class="math inline">\(0.04\)</span>, <span class="math inline">\(0.02\)</span> and <span class="math inline">\(0.01\)</span> for the slice sampler version.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot3-6-26-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot4-6-26-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
</div>
<p>Unexpectedly, the slice sampler displays high uncertainty towards the right half of the plot and severely underestimates not just the true function path but the observed data as well. I suspect the algorithm’s estimating function, which takes a weighted mean of every cluster’s estimate of a new data point, is still putting too much weight on the estimates of distant clusters. I am not sure why it would perform so much worse than the STAN version though.</p>
<section id="june-19-2025" class="level3">
<h3 class="anchored" data-anchor-id="june-19-2025">June 19, 2025</h3>
</section>
</section>
<section id="summary-2" class="level2">
<h2 class="anchored" data-anchor-id="summary-2">Summary</h2>
<p>This week I investigated the Modified Chinese Restaurant Process and attempted to reason out the role of the parameter <span class="math inline">\(\phi\)</span> with some success. I also studied the new versions of iMGPE with STAN and slice sampling based GP experts and analyzed the Monte Carlo standard error of their quantile estimates.</p>
<p>I also tested the iMGPE algorithm on a new simulated data set with a strictly positive response. The predictive posterior distribution appears to be over-smoothed; it estimates the function as closer to a straight line than it really is.</p>
</section>
<section id="mcrp-prior" class="level2">
<h2 class="anchored" data-anchor-id="mcrp-prior">MCRP Prior</h2>
<p>This week, I continued to study the Modified Chinese Restaurant Process used in the iMGPE algorithm with particular attention to the role played by the parameter <span class="math inline">\(\phi\)</span>. Previous experiments had indicated that <span class="math inline">\(\phi\)</span> had little influence on the average number of clusters and new experiments over a broader range of <span class="math inline">\(\phi\)</span> values simply bore this out.</p>
<p>Upon reflection, I realized that <span class="math inline">\(\phi\)</span> shouldn’t have a major impact on the number of clusters anyways. The number of clusters would be controlled primarily by the probability of assigning a point to a new cluster, which is <span class="math inline">\(\alpha/(n+\alpha)\)</span>; that is, it depends only on <span class="math inline">\(\alpha\)</span> and the number of data points. Experimentation with the MCRP conditional assignment probabilities seemed to indicate that <span class="math inline">\(\phi\)</span> controls how strongly a point prefers to join a cluster that is close to it in <span class="math inline">\(X\)</span>. As <span class="math inline">\(\phi\)</span> approaches infinity, the probability of a point joining a specific cluster approaches proportionality with the number of points in that cluster. As <span class="math inline">\(\phi\)</span> approaches zero, the probability of a point joining a specific cluster approaches one if that cluster is the closest to the point and zero otherwise.</p>
<p>Therefore, we should not expect <span class="math inline">\(\phi\)</span> to influence the number of clusters generated but rather how points are distributed among the clusters, with smaller <span class="math inline">\(\phi\)</span> values tending towards more separation between clusters. Unfortunately, I struggled to verify this empirically, as it is difficult to express separation between clusters in a compact form. I found one single number summary, the Dunn Index, but when tested on a range of values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\phi\)</span> (<span class="math inline">\((0.1,1,10)\)</span> and <span class="math inline">\((1,10,100)\)</span> respectively) it did not vary significantly.</p>
<p>A clearer pattern was visible when considering the spread of cluster sizes, that is, the difference between the largest cluster size and the smallest. As <span class="math inline">\(\phi\)</span> increases, the spread shrinks slightly. Admittedly, I am not certain why this would be the case. Further investigation is warranted.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot3-6-19-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Another plausible use of the <span class="math inline">\(\phi\)</span> parameter would be if we had multiple input variables. Then <span class="math inline">\(\phi\)</span> could determine the importance of a given <span class="math inline">\(X\)</span> input for clustering.</p>
</section>
<section id="updated-experimental-results" class="level2">
<h2 class="anchored" data-anchor-id="updated-experimental-results">Updated Experimental Results</h2>
<p>I tested the STAN version of the iMGPE algorithm with <span class="math inline">\(2000\)</span> iterations per expert instead of <span class="math inline">\(500\)</span>. For now, I am still limited to <span class="math inline">\(500\)</span> MCMC iterations in total due to memory restraints. The results did not appear to be significantly different.</p>
<p>The predicted posterior of the STAN version is plotted below on the left. The blue line is the true function path and the red line is the median of the fitted values. The upper grey band shows the <span class="math inline">\(95\%\)</span> credible band over the test set while the lower grey band is a visual aid displaying the width of the credible band with its lower bound fixed at <span class="math inline">\(-2\)</span>.</p>
<p>On the right is a plot of the <span class="math inline">\(95\%\)</span> credible intervals for the median and <span class="math inline">\(95\%\)</span> credible interval bounds of the estimated function given the Monte Carlo standard error. The blue line is the true function path. The solid red line is the median of the fitted vales and the two dashed red lines are the <span class="math inline">\(2.5th\)</span> and <span class="math inline">\(97.5th\)</span> quantiles of the fitted values. The grey ribbons are the MCSE credible intervals.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot1-6-19-25-2.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot2-6-19-25-2.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
</div>
<p>The average MC standard error for the median was <span class="math inline">\(0.012\)</span> and for the <span class="math inline">\(2.5th\)</span> and <span class="math inline">\(97.5th\)</span> quantiles it was <span class="math inline">\(0.019\)</span> and <span class="math inline">\(0.022\)</span> respectively.</p>
<p>For comparison, here are the same plots for the slice sampling version. On the left is the predicted posterior with the true and median fitted functions plotted in blue and red and the <span class="math inline">\(95\%\)</span> credible band as the grey band. On the right is the plot of the <span class="math inline">\(95\%\)</span> credible bands for the <span class="math inline">\(2.5th\)</span>, <span class="math inline">\(50th\)</span>, and <span class="math inline">\(97.5th\)</span> percentiles given the Monte Carlo standard error of their estimates.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot4-6-19-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot5-6-19-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
</div>
<p>The average MC standard error for the median was <span class="math inline">\(0.015\)</span> and for the <span class="math inline">\(2.5th\)</span> and <span class="math inline">\(97.5th\)</span> quantiles it was <span class="math inline">\(0.022\)</span> and <span class="math inline">\(0.027\)</span> respectively.</p>
</section>
<section id="new-simulated-data-set" class="level2">
<h2 class="anchored" data-anchor-id="new-simulated-data-set">New Simulated Data Set</h2>
<p>Our previous experiments on both the motorcycle data and the simulated data set displayed an unexpected shrinkage effect on the fitted function line, which we theorized to be towards the grand mean or perhaps <span class="math inline">\(y=0\)</span>. To distinguish between these possibilities, I fit a new instance to a new data set with a strictly positive response. A new data set of <span class="math inline">\(100\)</span> points was generated by the following function.</p>
<p><span class="math display">\[f(x)=1+0.3x+\frac{\sin(\pi x)}{5x}\]</span></p>
<p>I tried fitting this data set with the optimization method and obtained the plot of the fitted function below. The fitted line undershoots the true line (in blue) in some places but overshoots in others. Overall, it seems to under-fit the function line, preferring a straighter path than what the function actually follows.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot6-6-19-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="june-12-2025" class="level3">
<h3 class="anchored" data-anchor-id="june-12-2025">June 12, 2025</h3>
</section>
</section>
<section id="summary-3" class="level2">
<h2 class="anchored" data-anchor-id="summary-3">Summary</h2>
<p>This week I refined my data model description and DAG chart. I also evaluated the prior distribution of the number of clusters generated by the modified Chinese Restaurant Process used in iMGPE, finding that it is often much higher than the observed number of clusters.</p>
<p>I developed two new versions of the iMGPE algorithm that replaced the optimization-based GP evaluation step with two sampling-based methods: STAN and slice sampling. These methods take much longer to run than the optimization version but produce estimates of similar quality.</p>
</section>
<section id="data-model" class="level2">
<h2 class="anchored" data-anchor-id="data-model">Data Model</h2>
<p>We have an <span class="math inline">\(n\times 1\)</span> continuous response vector <span class="math inline">\(y\)</span> and an <span class="math inline">\(n\times d\)</span> data matrix <span class="math inline">\(X\)</span>. The estimated value of a data point <span class="math inline">\(y_i\)</span> under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing <span class="math inline">\(y_i\)</span> and weighted by a Dirichlet process. Let <span class="math inline">\(z\)</span> represent a possible vector of cluster assignments and let <span class="math inline">\(z^{(k)}\)</span> be the <span class="math inline">\(k^{th}\)</span> element of some ordered list of all possible <span class="math inline">\(z\)</span>. Then <span class="math inline">\(j=1,\dots,J_k\)</span> index the clusters within <span class="math inline">\(z^{(k)}\)</span>. Let <span class="math inline">\(C_j^{(k)}\)</span> be the number of observations in cluster <span class="math inline">\(j\)</span> given assignment <span class="math inline">\(z^{(k)}\)</span>. Then we have as follows.</p>
<p><span class="math display">\[y\sim \sum_{k=1}^{p(n)} \left[\prod_{j=1}^{J_k} N_{C_j^{(k)}}(0,\Sigma_{\theta_j}) \right] w_k\]</span> <span class="math display">\[w_k=P(z=z^{(k)}|\alpha,\phi)\]</span> where the <span class="math inline">\(w_z\)</span> are the marginal probabilities of a modified Chinese Restaurant Process that generates cluster assignments <span class="math inline">\(z\)</span> and <span class="math inline">\(p(n)\)</span> is the <a href="https://en.wikipedia.org/wiki/Partition_function_(number_theory)">partition function</a>. The CRP used here has been modified to depend on the input data <span class="math inline">\(X\)</span>. It is controlled by two parameters, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\phi\)</span>, the first being the usual concentration parameter and the second controlling the cluster occupancy estimates. A more in-depth explanation of this CRP is provided in the iMGPE Algorithm section from last week. It should also be noted that there is no known closed form for <span class="math inline">\(w_k\)</span>. The closest I could find was the multivariate <a href="https://en.wikipedia.org/wiki/Ewens%27s_sampling_formula">Ewen’s distribution</a>, which describes the distribution on the set of <span class="math inline">\(p(n)\)</span> that arises from a regular Chinese Restaurant Process. However, it only accounts for the number of clusters of different sizes, and does not serve as a marginal distribution of <span class="math inline">\(z\)</span>.</p>
<p>The full joint distribution, including the priors for all parameters, is as follows. Here, <span class="math inline">\(\Phi\)</span> is the pdf of a normal distribution.</p>
<p><span class="math display">\[p(y,\theta,\phi,\alpha)=\left[ \sum_{k=1}^{p(n)} \left[\prod_{j=1}^{J_k} \Phi_{C_j^{(k)}}(0,\Sigma_{\theta_j}) \right] p(z(k)|\alpha,\phi)\right] p(\theta)p(\phi)p(\alpha)\]</span></p>
<p>Alternatively, the model can be expressed in hierarchical terms where <span class="math inline">\(J_z\)</span> is the number of clusters in <span class="math inline">\(z\)</span> and <span class="math inline">\(C_{j,z}\)</span> is the number of observations in the <span class="math inline">\(j^{th}\)</span> cluster in <span class="math inline">\(z\)</span>.</p>
<p><span class="math display">\[y|z\sim \prod_{j=1}^{J_z} N_{C_{j,z}}(0,\Sigma_{\theta_{j}})\]</span> <span class="math display">\[z|\alpha,\phi \sim MCRP(\alpha,\phi)\]</span></p>
<p>Here, <span class="math inline">\(N_{C_j^{(z)}}(0,\Sigma_{\theta_j})\)</span> is the <span class="math inline">\(C_j^{(z)}\)</span>-dimensional multivariate normal distribution with covariance matrix <span class="math inline">\(\Sigma_{\theta_j}\)</span> defined by a Gaussian kernel function with parameters <span class="math inline">\(\theta_j\)</span>.</p>
<p>Then <span class="math inline">\(\theta_j\)</span> is the parameter vector for the GP expert assigned to cluster <span class="math inline">\(j\)</span>, while <span class="math inline">\(\alpha\)</span> is the CRP concentration parameter and <span class="math inline">\(\phi\)</span> is the parameter vector for the CRP’s occupation number estimate. Note that <span class="math inline">\(\phi\)</span> is purely a vector of lengthscales for a Gaussian kernel. The priors on <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\alpha\)</span>, and <span class="math inline">\(\phi\)</span> are described below.</p>
<p><span class="math display">\[\theta_{j,k}\stackrel{ind}{\sim} Gamma(a_k,b_k) \text{ for } k=1,\dots,d\]</span> <span class="math display">\[\alpha\sim Inv.Gam(1,1),\text{   } \phi_k\stackrel{iid}{\sim} LogN(0,1) \text{ for } k=1,\dots,d\]</span> That is, each element <span class="math inline">\(k\)</span> of <span class="math inline">\(\theta_j\)</span> (the dimension of <span class="math inline">\(X\)</span> plus a noise parameter) is assigned a independent Gamma prior with fixed parameters <span class="math inline">\(a_k\)</span> and <span class="math inline">\(b_k\)</span>. Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of <span class="math inline">\(\phi\)</span> receives an independent log-normal prior.</p>
<p>A third visualization of the model structure is a directed acyclic graph, shown below. Starting with the priors for <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\phi\)</span>, and <span class="math inline">\(\theta\)</span>, we can draw their values and generate our latent variables <span class="math inline">\(z\)</span> and our true variables <span class="math inline">\(y\)</span>. Note that <span class="math inline">\(\theta\)</span> depends on <span class="math inline">\(z\)</span> as well as its prior, as <span class="math inline">\(z\)</span> defines the number of clusters and thus the number of GP parameters to be drawn. The red and blue boxes indicate the quantities which are drawn multiple times for each of the <span class="math inline">\(n\)</span> data points or each of the <span class="math inline">\(J\)</span> clusters.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="prior-of-mcrp" class="level2">
<h2 class="anchored" data-anchor-id="prior-of-mcrp">Prior of MCRP</h2>
<p>The modified Chinese Restaurant process is dependent on the input data <span class="math inline">\(X\)</span> through the parameter <span class="math inline">\(\phi\)</span> in addition to the parameter <span class="math inline">\(\alpha\)</span> used by a regular CRP. It is worth considering how this affects the prior distribution of our cluster assignment vector <span class="math inline">\(z\)</span>. While there is no closed form for the marginal distribution of <span class="math inline">\(z\)</span>, we can study it empirically by generating samples of <span class="math inline">\(z\)</span> using the algorithm described in the iMGPE Algorithm section last week.</p>
<p>Using the simulated dataset, described in the April 24 section, I studied the average number of clusters generated by the MCRP. I chose the values <span class="math inline">\(\alpha=1,3,5\)</span> and <span class="math inline">\(\phi=2.5,5,7.5\)</span> to cover the most common range of values observed in our data and generated a set of <span class="math inline">\(1000\)</span> <span class="math inline">\(z\)</span> vectors for each combination of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\phi\)</span>. The mean and standard deviation of the number of clusters in those sets are shown below.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><span class="math inline">\(\alpha\)</span></th>
<th><span class="math inline">\(\phi\)</span></th>
<th>mean</th>
<th>sd</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>2.5</td>
<td>4.42</td>
<td>1.86</td>
</tr>
<tr class="even">
<td>1</td>
<td>5</td>
<td>4.86</td>
<td>1.84</td>
</tr>
<tr class="odd">
<td>1</td>
<td>7.5</td>
<td>5.04</td>
<td>1.99</td>
</tr>
<tr class="even">
<td>3</td>
<td>2.5</td>
<td>17.6</td>
<td>3.26</td>
</tr>
<tr class="odd">
<td>3</td>
<td>5</td>
<td>17.8</td>
<td>3.55</td>
</tr>
<tr class="even">
<td>3</td>
<td>7.5</td>
<td>17.9</td>
<td>3.27</td>
</tr>
<tr class="odd">
<td>5</td>
<td>2.5</td>
<td>27.1</td>
<td>3.66</td>
</tr>
<tr class="even">
<td>5</td>
<td>5</td>
<td>27.1</td>
<td>3.87</td>
</tr>
<tr class="odd">
<td>5</td>
<td>7.5</td>
<td>26.8</td>
<td>3.75</td>
</tr>
</tbody>
</table>
<p>These results can be compared to the expected numbers of clusters from a standard CRP with the same <span class="math inline">\(\alpha\)</span> value, which is <span class="math inline">\(\alpha(\psi(n+\alpha)-\psi(\alpha))\)</span> where <span class="math inline">\(n\)</span> is the number of data points and <span class="math inline">\(\psi\)</span> is the digamma function. For <span class="math inline">\(\alpha\)</span> values of <span class="math inline">\(1\)</span>, <span class="math inline">\(3\)</span>, and <span class="math inline">\(5\)</span>, the expected numbers of clusters are <span class="math inline">\(5.2\)</span>, <span class="math inline">\(11.1\)</span>, and <span class="math inline">\(15.7\)</span>. We can see that the data dependency in the MCRP has increased the expected number of clusters and the inflation gets bigger as <span class="math inline">\(\alpha\)</span> increases. However, <span class="math inline">\(\phi\)</span> does not appear to have a strong influence on the number of clusters, at least within the range I examined.</p>
</section>
<section id="imgpe-with-stan-slice-sampling" class="level2">
<h2 class="anchored" data-anchor-id="imgpe-with-stan-slice-sampling">iMGPE with STAN &amp; Slice Sampling</h2>
<p>I coded two new versions of the iMGPE algorithm: one that fit GP experts using Hybrid Monte Carlo using STAN and one that fit GP experts by sampling their parameters with elliptical slice sampling. The STAN experts ran for <span class="math inline">\(500\)</span> iterations each while the slice sampler drew <span class="math inline">\(100\)</span> sample parameter values from the GP posterior and averaged them. Both took much longer to run than the optimization version and had to be run for fewer iterations. Particularly, running the STAN version for more than <span class="math inline">\(600\)</span> or so iterations caused a “Maximum number of DLLs reached” error that I haven’t yet determined how to fix. Thus, I am presenting a <span class="math inline">\(500\)</span> iteration run of the STAN version and a <span class="math inline">\(1000\)</span> iteration run of the slice sampling version.</p>
<p>Both methods were trained on the simulated data set and compared on a test set of <span class="math inline">\(500\)</span> evenly spaced points between <span class="math inline">\(0\)</span> and <span class="math inline">\(5\)</span>. The predicted posterior of the STAN version is plotted below. The blue line is the true function path and the red line is the median of the fitted values (the mean proved to be much less coherent). The upper grey band shows the <span class="math inline">\(95\%\)</span> credible band over the test set while the lower grey band is a visual aid displaying the width of the credible band with its lower bound fixed at <span class="math inline">\(-2\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot4-6-12-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The predictive posterior of the slice sampling version is plotted here. Unlike the STAN version, the red line here does portray the mean rather than the median.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot5-6-12-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The STAN and slice sampling versions are a bit less accurate than the optimization version in terms of their estimated function. That is, the red line strays further from the blue line in these versions. The STAN version, however, has equal or less uncertainty in its estimate than the optimization version, while the slice sampling version has more.</p>
<section id="may-29-2025" class="level3">
<h3 class="anchored" data-anchor-id="may-29-2025">May 29, 2025</h3>
</section>
</section>
<section id="summary-4" class="level2">
<h2 class="anchored" data-anchor-id="summary-4">Summary</h2>
<p>Since our last meeting, I have improved the data model and algorithm descriptions and added a directed acyclic graph to portray the relationships between the model components. I also investigated the cause of high uncertainty in the estimated function occurring in areas with many data points. This does not seem to be caused by interplay between the estimated nugget and the function, as the nugget estimates are both small and highly consistent across the test data. More likely, it is an artifact of the clustering process, where points are occasionally put in the ‘wrong’ cluster or a cluster is stacked with disparate points, whose GP subsequently has greater uncertainty in its estimates.</p>
<p>I also coded a functioning iMGPE algorithm that fits GP experts with an MCMC process via STAN. I have not yet completed a full run of the new model as it is much slower than the original.</p>
</section>
<section id="data-model-description" class="level2">
<h2 class="anchored" data-anchor-id="data-model-description">Data Model Description</h2>
<p>We have an <span class="math inline">\(n\times 1\)</span> continuous response vector <span class="math inline">\(y\)</span> and an <span class="math inline">\(n\times d\)</span> data matrix <span class="math inline">\(X\)</span>. The estimated value of a data point <span class="math inline">\(y_i\)</span> under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing <span class="math inline">\(y_i\)</span> and weighted by a Dirichlet process. Let <span class="math inline">\(z\)</span> represent a possible vector of cluster assignments and let <span class="math inline">\(z^{(k)}\)</span> be the <span class="math inline">\(k^{th}\)</span> element of some ordered list of all possible <span class="math inline">\(z\)</span>. Then <span class="math inline">\(j=1,\dots,J_k\)</span> index the clusters within <span class="math inline">\(z^{(k)}\)</span>. Let <span class="math inline">\(C_j^{(k)}\)</span> be the number of observations in cluster <span class="math inline">\(j\)</span> given assignment <span class="math inline">\(z^{(k)}\)</span>. Then we have as follows.</p>
<p><span class="math display">\[y\sim \sum_{k=1}^{p(n)} \left[\prod_{j=1}^{J_k} N_{C_j^{(k)}}(0,\Sigma_{\theta_j}) \right] w_k\]</span> <span class="math display">\[w_k=P(z=z^{(k)}|\alpha,\phi)\]</span> where the <span class="math inline">\(w_z\)</span> are the marginal probabilities of a modified Chinese Restaurant Process that generates cluster assignments <span class="math inline">\(z\)</span> and <span class="math inline">\(p(n)\)</span> is the <a href="https://en.wikipedia.org/wiki/Partition_function_(number_theory)">partition function</a>. The CRP used here has been modified to depend on two parameters, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\phi\)</span>, the first being the usual concentration parameter and the second controlling the cluster occupancy estimates. A more in-depth explanation of this CRP is provided in the next section. It should also be noted that there is no known closed form for <span class="math inline">\(w_k\)</span>. The closest I could find was the multivariate <a href="https://en.wikipedia.org/wiki/Ewens%27s_sampling_formula">Ewen’s distribution</a>, which describes the distribution on the set of <span class="math inline">\(p(n)\)</span> that arises from a regular Chinese Restaurant Process. However, it only accounts for the number of clusters of different sizes, and does not serve as a marginal distribution of <span class="math inline">\(z\)</span>.</p>
<p>The full joint distribution, including the priors for all parameters, is as follows. Here, <span class="math inline">\(\Phi\)</span> is the pdf of a normal distribution.</p>
<p><span class="math display">\[p(y)=\left[ \sum_{k=1}^{p(n)} \left[\prod_{j=1}^{J_k} \Phi_{C_j^{(k)}}(0,\Sigma_{\theta_j}) \right] P(z(k)|\alpha,\phi)\right] p(\theta)p(\phi)p(\alpha)\]</span></p>
<p>Alternatively, the model can be expressed in hierarchical terms where <span class="math inline">\(J_z\)</span> is the number of clusters in <span class="math inline">\(z\)</span> and <span class="math inline">\(C_{j,z}\)</span> is the number of observations in the <span class="math inline">\(j^{th}\)</span> cluster in <span class="math inline">\(z\)</span>.</p>
<p><span class="math display">\[y|z\sim \prod_{j=1}^{J_z} N_{C_{j,z}}(0,\Sigma_{\theta_{j}})\]</span> <span class="math display">\[z|\alpha,\phi \sim MCRP(\alpha,\phi)\]</span></p>
<p>Here, <span class="math inline">\(N_{C_j^{(z)}}(0,\Sigma_{\theta_j})\)</span> is the <span class="math inline">\(C_j^{(z)}\)</span>-dimensional multivariate normal distribution with covariance matrix <span class="math inline">\(\Sigma_{\theta_j}\)</span> defined by a Gaussian kernel function with parameters <span class="math inline">\(\theta_j\)</span>.</p>
<p>Then <span class="math inline">\(\theta_j\)</span> is the parameter vector for the GP expert assigned to cluster <span class="math inline">\(j\)</span>, while <span class="math inline">\(\alpha\)</span> is the CRP concentration parameter and <span class="math inline">\(\phi\)</span> is the parameter vector for the CRP’s occupation number estimate. Note that <span class="math inline">\(\phi\)</span> is purely a vector of lengthscales for a Gaussian kernel. The priors on <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\alpha\)</span>, and <span class="math inline">\(\phi\)</span> are described below.</p>
<p><span class="math display">\[\theta_{j,k}\stackrel{ind}{\sim} Gamma(a_k,b_k) \text{ for } k=1,\dots,d\]</span> <span class="math display">\[\alpha\sim Inv.Gam(1,1),\text{   } \phi_k\stackrel{iid}{\sim} LogN(0,1) \text{ for } k=1,\dots,d\]</span> That is, each element <span class="math inline">\(k\)</span> of <span class="math inline">\(\theta_j\)</span> (the dimension of <span class="math inline">\(X\)</span> plus a noise parameter) is assigned a independent Gamma prior with fixed parameters <span class="math inline">\(a_k\)</span> and <span class="math inline">\(b_k\)</span>. Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of <span class="math inline">\(\phi\)</span> receives an independent log-normal prior.</p>
<p>A third visualization of the model structure is a directed acyclic graph, shown below. Starting with the priors for <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\phi\)</span>, and <span class="math inline">\(\theta\)</span>, we can draw their values and generate our latent variables <span class="math inline">\(z\)</span> and our true variables <span class="math inline">\(y\)</span>. Note that <span class="math inline">\(\theta\)</span> depends on <span class="math inline">\(z\)</span> as well as its prior, as <span class="math inline">\(z\)</span> defines the number of clusters and thus the number of GP parameters to be drawn.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="the-imgpe-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="the-imgpe-algorithm">The iMGPE Algorithm</h2>
<p>The modified CRP used in this algorithm is defined by <a href="https://www.tandfonline.com/doi/abs/10.1080/10618600.2000.10474879">“R. M. Neal”</a> (Algorithm 8 in that paper with <span class="math inline">\(m=1\)</span>) and works as follows:</p>
<p>We represent the current cluster state with assignment labels <span class="math inline">\(z=(z_1,\dots,z_n)\)</span> and GP parameter vectors <span class="math inline">\(\theta_1,\dots,\theta_J\)</span> where <span class="math inline">\(J\)</span> is the number of clusters in the current state. For <span class="math inline">\(i=1,\dots,n\)</span>, repeat the following. Let <span class="math inline">\(J^{-i}\)</span> be the number of clusters in <span class="math inline">\(z\)</span> with point <span class="math inline">\(i\)</span> removed. Let <span class="math inline">\(\theta_{J^{-i}+1}\)</span> be a parameter vector drawn from its prior distribution, in this case <span class="math inline">\(Gamma^d(a,b)\)</span>. Draw a new value for <span class="math inline">\(z_i\)</span> with the following conditional probabilities:</p>
<p><span class="math display">\[P(z_i=j|z_{-i},y_i,\dots)\propto \begin{cases}
\frac{n-1}{n+\alpha-1}\frac{\sum_{i'\neq i,z_{i'}=j} K_{\phi}(X_i,X_{i'})}{\sum_{i'\neq i} K_{\phi}(X_i,X_{i'})} f(y_i|\theta_j) \text{ for } j=1,\dots,J^{-i}\\
\frac{\alpha}{n+\alpha-1}f(y_i|\theta_{J^{-i}+1}) \text{ for } j=J^{-i}+1
\end{cases}\]</span></p>
<p>where <span class="math inline">\(f(y_i|\theta_j)\)</span> is the normal density of <span class="math inline">\(y_i\)</span> given the kriging equations with parameter vector <span class="math inline">\(\theta_j\)</span> defining the kernel function.</p>
<p>I have implemented the Infinite Mixture of Gaussian Process Experts algorithm mostly as described by the authors Rasmussen and Ghahramani, though with a few alterations of my own which are noted below. First, I initialize indicator variables <span class="math inline">\(z\)</span> to a set of values. I generally start by assigning all points to a single cluster. I set gamma prior distributions on the lengthscale and nugget parameters of the GP experts, using the ‘darg’ and ‘garg’ functions of the package ‘laGP’ and set initial values for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\phi\)</span>. This approach then iterates through the following MCMC algorithm.</p>
<ol type="1">
<li>Perform a Gibbs sampling sweep over the cluster assignment indicators, using the modified Chinese Restaurant Process described in the model explanation, to generate a new cluster assignment vector <span class="math inline">\(z\)</span>.</li>
<li>Fit a Gaussian process expert to each cluster in <span class="math inline">\(z\)</span> and get ML estimates of each expert’s parameters <span class="math inline">\(\theta_j\)</span>. Note that this is not a sampling step but maximization.</li>
<li>Sample the Dirichlet process concentration parameter, <span class="math inline">\(\alpha\)</span>, using quantile slice sampling with a <span class="math inline">\(Gamma(1,1)\)</span> proposal distribution. The posterior distribution of <span class="math inline">\(\alpha\)</span> we sample from is <span class="math display">\[p(\alpha|n, J)\propto \alpha^{J-3/2}\exp(-1/2\alpha)\Gamma(\alpha)/\Gamma(n+\alpha)\]</span></li>
<li>Sample the other CRP parameter <span class="math inline">\(\phi\)</span> via random walk Monte Carlo. The random walk step uses a normal proposal distribution centered at the current value and with variance equal to <span class="math inline">\((2.38^2/d)H^{-1}\)</span>, <span class="math inline">\(d\)</span> being the number of inputs and <span class="math inline">\(H\)</span> the Hessian matrix of the distribution of <span class="math inline">\(\phi\)</span>. The posterior distribution of <span class="math inline">\(\phi\)</span> we are sampling from is <span class="math display">\[p(\phi|z,\alpha,\dots)\propto p(z|y,\phi,\alpha)p(\phi)\approx \left[\prod_{i=1}^n p(z_i|y,\phi,\alpha) \right] p(\phi)\]</span></li>
<li>Repeat from step 1 until the MCMC output has converged.</li>
</ol>
</section>
<section id="analysis-of-current-results" class="level2">
<h2 class="anchored" data-anchor-id="analysis-of-current-results">Analysis of Current Results</h2>
<p>In the current algorithm, we see variation in the uncertainty of the fitted mean on the test data that is hard to explain given our data, with patches of relatively high uncertainty in areas with many data points (such as around <span class="math inline">\(x=3\)</span>). This does not seem to be due to variation in the nugget parameter estimates, as a plot of the average and <span class="math inline">\(95\%\)</span> credible band for the estimated nugget for each observation is strongly uniform and centered around the relatively small value of <span class="math inline">\(0.01\)</span>. On a positive note, this is quite close to the true constant nugget value of <span class="math inline">\(0.02\)</span>.</p>
<p>As before, we drew an estimated value for each point on the test set every fifth iteration, using the formula <span class="math inline">\(\hat\mu_i = \sum_{j=1}^J p(z_i=j|\alpha,\phi)\mu_{i,j}\)</span> where <span class="math inline">\(\hat\mu_i\)</span> is the estimate for observation <span class="math inline">\(i\)</span> in the test set, <span class="math inline">\(\mu_{i,j}\)</span> is the fitted value of point <span class="math inline">\(i\)</span> under the GP for cluster <span class="math inline">\(j\)</span>, and <span class="math inline">\(p(z_i=j|\alpha,\phi)\)</span> is the conditional probability of point <span class="math inline">\(i\)</span> being assigned to cluster <span class="math inline">\(j\)</span> under our modified CRP.</p>
<p>The resultant credible band for the simulated function is graphed below, based on <span class="math inline">\(1000\)</span> draws from the predictive posterior. The blue line is the true function path and the red line is the mean of the fitted values. The upper grey band shows the <span class="math inline">\(95\%\)</span> credible band over the test set while the lower grey band is a visual aid displaying the width of the credible band with its lower bound fixed at <span class="math inline">\(-2\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot1-5-29-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>I also recorded the nugget parameter estimates for each point in each iteration. I had hoped that this could explain some of the variation we see in fitted function uncertainty. The solid line and the grey band represent the mean of the nugget and the <span class="math inline">\(95\%\)</span> credible band on the nugget across all values in the test set respectively.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot2-5-29-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The nugget likely has little effect on fitted function uncertainty as it is both small and uniform. A more likely candidate is unlucky cluster assignments creating clusters that cannot be fit with a high degree of certainty. For example, the plot below displays our data set colored by the cluster assignments of the 200th iteration. Note that the eighth cluster, the purple dots, contains several points in the upper left of the plot and one in the dip around <span class="math inline">\(x=3\)</span> and another in the peak around <span class="math inline">\(x=4\)</span>. These points likely cannot be estimated with confidence given the other points in the cluster and their estimates could be significantly inaccurate and distort the tails of the overall function estimate.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot5-5-29-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="single-gp-comparison" class="level2">
<h2 class="anchored" data-anchor-id="single-gp-comparison">Single GP Comparison</h2>
<p>For comparison, I fit a single Gaussian process model to the same data set and plotted its predictive mean and uncertainty on the test data. I found that a single GP model arguably does a better job of matching the estimated function (in red) to the true function (in blue). However, its uncertainty in that estimate is much wider than in the iMGPE model almost everywhere. As before, the red and blue lines represent the estimated and true functions, the grey band around them represents the <span class="math inline">\(95\%\)</span> credible interval in the estimated function, and the grey band at the bottom displays the width of the credible band across <span class="math inline">\(X\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot3-5-29-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="may-15-2025" class="level3">
<h3 class="anchored" data-anchor-id="may-15-2025">May 15, 2025</h3>
</section>
</section>
<section id="summary-5" class="level2">
<h2 class="anchored" data-anchor-id="summary-5">Summary</h2>
<p>This week I refined my explanation of the data model to be more coherent and readable. I then wrote out the full update step for the MCMC algorithm, detailing how each component of the model is updated.</p>
<p>Lastly, I developed a method to estimate values on a test set to replace my old practice of fitting a credible interval around each data point. The new method produces a much smoother and narrower credible band for the estimated function on both the simulated data set and the motorcycle data set.</p>
</section>
<section id="explanation-of-data-model" class="level2">
<h2 class="anchored" data-anchor-id="explanation-of-data-model">Explanation of Data Model</h2>
<p>We have an <span class="math inline">\(n\times 1\)</span> continuous response vector <span class="math inline">\(y\)</span> and an <span class="math inline">\(n\times d\)</span> data matrix <span class="math inline">\(X\)</span>. The estimated value of a data point <span class="math inline">\(y_i\)</span> under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing <span class="math inline">\(y_i\)</span> and weighted by a Dirichlet process. Let <span class="math inline">\(z\)</span> represent a possible vector of cluster assignments and let <span class="math inline">\(z^{(k)}\)</span> be the <span class="math inline">\(k^{th}\)</span> element of some ordered list of all possible <span class="math inline">\(z\)</span>. Then <span class="math inline">\(j=1,\dots,J_k\)</span> index the clusters within <span class="math inline">\(z^{(k)}\)</span>. Let <span class="math inline">\(C_j^{(k)}\)</span> be the number of observations in cluster <span class="math inline">\(j\)</span> given assignment <span class="math inline">\(z^{(k)}\)</span>. Then we have as follows.</p>
<p><span class="math display">\[y\sim \sum_{k=1}^{p(n)} \left[\prod_{j=1}^{J_k} N_{C_j^{(k)}}(0,\Sigma_{\theta_j}) \right] w_k\]</span> <span class="math display">\[w_k=P(z=z^{(k)}|\alpha,\phi,\dots)\]</span> where the <span class="math inline">\(w_z\)</span> are the marginal probabilities of a Chinese Restaurant Process that generates cluster assignments <span class="math inline">\(z\)</span> and <span class="math inline">\(p(n)\)</span> is the <a href="https://en.wikipedia.org/wiki/Partition_function_(number_theory)">partition function</a>. The CRP used here has been modified to depend on two parameters, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\phi\)</span>, the first being the usual concentration parameter and the second controlling the cluster occupancy estimates. A more in-depth explanation of this CRP will be provided shortly.</p>
<p>Alternatively, the model can be expressed in hierarchical terms where <span class="math inline">\(J_z\)</span> is the number of clusters in <span class="math inline">\(z\)</span> and <span class="math inline">\(C_{j,z}\)</span> is the number of observations in the <span class="math inline">\(j^{th}\)</span> cluster in <span class="math inline">\(z\)</span>.</p>
<p><span class="math display">\[y|z\sim \prod_{j=1}^{J_z} N_{C_{j,z}}(0,\Sigma_{\theta_{j}})\]</span> <span class="math display">\[z|\alpha,\phi \sim CRP(\alpha,\phi)\]</span></p>
<p>Here, <span class="math inline">\(N_{C_j^{(z)}}(0,\Sigma_{\theta_j})\)</span> is the <span class="math inline">\(C_j^{(z)}\)</span>-dimensional multivariate normal distribution with covariance matrix <span class="math inline">\(\Sigma_{\theta_j}\)</span> defined by a Gaussian kernel function with parameters <span class="math inline">\(\theta_j\)</span>. Similarly, <span class="math inline">\(Gamma^d(a,b)\)</span> is the joint prior over the GP parameters and is the product of <span class="math inline">\(d\)</span> Gamma distributions.</p>
<p>Then <span class="math inline">\(\theta_j\)</span> is the parameter vector for the GP expert assigned to cluster <span class="math inline">\(j\)</span>, while <span class="math inline">\(\alpha\)</span> is the CRP concentration parameter and <span class="math inline">\(\phi\)</span> is the parameter vector for the CRP’s occupation number estimate. Note that <span class="math inline">\(\phi\)</span> is purely a vector of lengthscales for a Gaussian kernel. The priors on <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\alpha\)</span>, and <span class="math inline">\(\phi\)</span> are described below.</p>
<p><span class="math display">\[\theta_{j,k}\stackrel{ind}{\sim} Gamma(a_k,b_k) \text{ for } k=1,\dots,d\]</span> <span class="math display">\[\alpha\sim Inv.Gam(1,1),\text{   } \phi_k\stackrel{iid}{\sim} LogN(0,1) \text{ for } k=1,\dots,d\]</span> That is, each element <span class="math inline">\(k\)</span> of <span class="math inline">\(\theta_j\)</span> (the dimension of <span class="math inline">\(X\)</span> plus a noise parameter) is assigned a independent Gamma prior with fixed parameters <span class="math inline">\(a_k\)</span> and <span class="math inline">\(b_k\)</span>. Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of <span class="math inline">\(\phi\)</span> receives an independent log-normal prior.</p>
<p>The modified CRP used in this algorithm is defined by <a href="https://www.tandfonline.com/doi/abs/10.1080/10618600.2000.10474879">“R. M. Neal”</a> (Algorithm 8 in that paper with <span class="math inline">\(m=1\)</span>) and works as follows:</p>
<p>We represent the current cluster state with assignment labels <span class="math inline">\(z=(z_1,\dots,z_n)\)</span> and GP parameter vectors <span class="math inline">\(\theta_1,\dots,\theta_J\)</span> where <span class="math inline">\(J\)</span> is the number of clusters in the current state. For <span class="math inline">\(i=1,\dots,n\)</span>, repeat the following. Let <span class="math inline">\(J^{-i}\)</span> be the number of clusters in <span class="math inline">\(z\)</span> with point <span class="math inline">\(i\)</span> removed. Let <span class="math inline">\(\theta_{J^{-i}+1}\)</span> be a parameter vector drawn from its prior distribution, in this case <span class="math inline">\(Gamma^d(a,b)\)</span>. Draw a new value for <span class="math inline">\(z_i\)</span> with the following conditional probabilities:</p>
<p><span class="math display">\[P(z_i=j|z_{-i},y_i,\dots)\propto \begin{cases}
\frac{n-1}{n+\alpha-1}\frac{\sum_{i'\neq i,z_{i'}=j} K_{\phi}(X_i,X_{i'})}{\sum_{i'\neq i} K_{\phi}(X_i,X_{i'})} f(y_i|\theta_j) \text{ for } j=1,\dots,J^{-i}\\
\frac{\alpha}{n+\alpha-1}f(y_i|\theta_{J^{-i}+1}) \text{ for } j=J^{-i}+1
\end{cases}\]</span></p>
<p>where <span class="math inline">\(f(y_i|\theta_j)\)</span> is the normal density of <span class="math inline">\(y_i\)</span> given the kriging equations with parameter vector <span class="math inline">\(\theta_j\)</span> defining the kernel function.</p>
</section>
<section id="the-imgpe-algorithm-1" class="level2">
<h2 class="anchored" data-anchor-id="the-imgpe-algorithm-1">The iMGPE Algorithm</h2>
<p>I have implemented the Infinite Mixture of Gaussian Process Experts algorithm mostly as described by the authors Rasmussen and Ghahramani, though with a few alterations of my own which are noted below. This approach then iterates through the following MCMC algorithm.</p>
<ol type="1">
<li>Initialize indicator variables <span class="math inline">\(z\)</span> to a set of values. I generally start by assigning all points to a single cluster. Set gamma prior distributions on the lengthscale and nugget parameters of the GP experts, using the ‘darg’ and ‘garg’ functions of the package ‘laGP’. Set initial values for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\phi\)</span>.</li>
<li>Perform a Gibbs sampling sweep over the cluster assignment indicators, using the Chinese Restaurant Process described in the model explanation, to generate a new cluster assignment vector <span class="math inline">\(z\)</span>.</li>
<li>Fit a Gaussian process expert to each cluster in <span class="math inline">\(z\)</span> and get ML estimates of each expert’s parameters <span class="math inline">\(\theta_j\)</span>.</li>
<li>Use these GP experts to generate estimates on the test set. More detail on this step is included in the next section.</li>
<li>Sample the Dirichlet process concentration parameter, <span class="math inline">\(\alpha\)</span>, using quantile slice sampling with a <span class="math inline">\(Gamma(1,1)\)</span> proposal distribution. The posterior distribution of <span class="math inline">\(\alpha\)</span> we sample from is <span class="math display">\[p(\alpha|n, J)\propto \alpha^{J-3/2}\exp(-1/2\alpha)\Gamma(\alpha)/\Gamma(n+\alpha)\]</span></li>
<li>Sample the other CRP parameter <span class="math inline">\(\phi\)</span> via random walk Monte Carlo. The random walk step uses a normal proposal distribution centered at the current value and with variance equal to <span class="math inline">\((2.38^2/d)H^{-1}\)</span>, <span class="math inline">\(d\)</span> being the number of inputs and <span class="math inline">\(H\)</span> the Hessian matrix of the distribution of <span class="math inline">\(\phi\)</span>. The posterior distribution of <span class="math inline">\(\phi\)</span> we are sampling from is <span class="math display">\[p(\phi|z,\alpha,\dots)\propto p(z|y,\phi,\alpha)p(\phi)\approx \left[\prod_{i=1}^n p(z_i|y,\phi,\alpha) \right] p(\phi)\]</span></li>
<li>Repeat from step 2 until the MCMC output has converged.</li>
</ol>
</section>
<section id="experiments-on-simulated-data" class="level2">
<h2 class="anchored" data-anchor-id="experiments-on-simulated-data">Experiments on Simulated Data</h2>
<p>I set up a new method of sampling posterior predictive values on a test set and applied it to the simulated data set, using a test set of <span class="math inline">\(500\)</span> evenly spaced points between <span class="math inline">\(0\)</span> and <span class="math inline">\(5\)</span>. Every fifth iteration, I would estimate fitted values on the test set for each GP expert currently in use. Then, for each point in the test set, I would calculate the conditional probabilities of it belonging to each of the current clusters. Lastly, I found the sum of the GP estimates for each point weighted by the probabilities of their respective clusters and record the resulting set of values for that iteration.</p>
<p>The resultant credible band for the simulated function is graphed below, based on <span class="math inline">\(1000\)</span> draws from the predictive posterior. The blue line is the true function path and the red line is the mean of the fitted values. The upper grey band shows the <span class="math inline">\(95\%\)</span> credible band over the test set while the lower grey band is a visual aid displaying the width of the credible band with its lower bound fixed at <span class="math inline">\(-2\)</span>. Uncertainty is greatest at the edges of the training data and around the rightmost peak, where there are few points. This is as we would expect.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot1-5-15-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="experiments-on-motorcycle-data" class="level2">
<h2 class="anchored" data-anchor-id="experiments-on-motorcycle-data">Experiments on Motorcycle Data</h2>
<p>I tested this posterior prediction method on the motorcycle data set, using a test set of <span class="math inline">\(561\)</span> evenly spaced points between <span class="math inline">\(2\)</span> and <span class="math inline">\(58\)</span>. I have made some changes to the body of the algorithm since I last tested on the motorcycle data, so the trace plots for <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\alpha\)</span> are also included. Both parameters have converged, and the algorithm heavily favors dividing the training data into two clusters.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot2-5-15-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot3-5-15-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot4-5-15-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
</div>
<p>I obtained <span class="math inline">\(1000\)</span> draws from the predictive posterior distribution as described in the previous section and have plotted their mean and <span class="math inline">\(95\%\)</span> credible band below. As before, the grey band at the bottom of the graph is a visual aid displaying the width of the credible interval across time.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Reports/plot5-5-15-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="may-8-2025" class="level3">
<h3 class="anchored" data-anchor-id="may-8-2025">May 8, 2025</h3>
</section>
</section>
<section id="summary-6" class="level2">
<h2 class="anchored" data-anchor-id="summary-6">Summary</h2>
<p>This week I updated my marginal model notation to be more readable and included a brief description of the hierarchical model. I also found and fixed the bug in my code that was causing unusually wide credible intervals.</p>
<p>I am working on converting the algorithm fully to Rcpp and implementing support for multivariate and categorical <span class="math inline">\(X\)</span> inputs. Now that the code structure of the algorithm has been developed, this should proceed quickly.</p>
</section>
<section id="explanation-of-data-model-1" class="level2">
<h2 class="anchored" data-anchor-id="explanation-of-data-model-1">Explanation of Data Model</h2>
<p>We have an <span class="math inline">\(n\times 1\)</span> continuous response vector <span class="math inline">\(y\)</span> and an <span class="math inline">\(n\times d\)</span> data matrix <span class="math inline">\(X\)</span>. The estimated value of a data point <span class="math inline">\(y_i\)</span> under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing <span class="math inline">\(y_i\)</span> and weighted by a Dirichlet process. Let <span class="math inline">\(z\)</span> represent a possible vector of cluster assignments and let <span class="math inline">\(z^{(k)}\)</span> be the <span class="math inline">\(k^{th}\)</span> element of some ordered list of all possible <span class="math inline">\(z\)</span>. Then <span class="math inline">\(j=1,\dots,J_k\)</span> index the clusters within <span class="math inline">\(z^{(k)}\)</span>. Let <span class="math inline">\(C_j^{(k)}\)</span> be the number of observations in cluster <span class="math inline">\(j\)</span> given assignment <span class="math inline">\(z(k)\)</span>. Then we have as follows.</p>
<p><span class="math display">\[y\sim \sum_{k=1}^{p(n)} \left[\prod_{j=1}^{J_k} N_{C_j^{(k)}}(0,\Sigma_{\theta_j}) \right] w_k\]</span> <span class="math display">\[w_k=P(z=z^{(k)}|\alpha,\phi,\dots)\]</span> where the <span class="math inline">\(w_z\)</span> are the marginal probabilities of a Chinese Restaurant Process that generates cluster assignments <span class="math inline">\(z\)</span> and <span class="math inline">\(p(n)\)</span> is the <a href="https://en.wikipedia.org/wiki/Partition_function_(number_theory)">partition function</a>. Alternatively, the model can be expressed in hierarchical terms as</p>
<p><span class="math display">\[y|z\sim \prod_{j=1}^J N_{C_j}(0,\Sigma_{\theta_j})\]</span> <span class="math display">\[z|\alpha,\dots \sim CRP(\alpha,\phi)\]</span></p>
<p>The specific CRP is defined by <a href="https://www.tandfonline.com/doi/abs/10.1080/10618600.2000.10474879">“R. M. Neal”</a> (Algorithm 8 in that paper with <span class="math inline">\(m=1\)</span>) and works as follows:</p>
<p>We represent the current cluster state with assignment labels <span class="math inline">\(z=(z_1,\dots,z_n)\)</span> and GP parameter vectors <span class="math inline">\(\theta_1,\dots,\theta_J\)</span> where <span class="math inline">\(J\)</span> is the number of clusters in the current state. For <span class="math inline">\(i=1,\dots,n\)</span>, repeat the following. Let <span class="math inline">\(J^{-i}\)</span> be the number of clusters in <span class="math inline">\(z\)</span> with point <span class="math inline">\(i\)</span> removed. Let <span class="math inline">\(\theta_{J^{-i}+1}\)</span> be a parameter vector drawn from its prior distribution, in this case <span class="math inline">\(Gamma^d(a,b)\)</span>. Draw a new value for <span class="math inline">\(z_i\)</span> with the following conditional probabilities:</p>
<p><span class="math display">\[P(z_i=j|z_{-i},y_i,\dots)\propto \begin{cases}
\frac{n-1}{n+\alpha-1}\frac{\sum_{i'\neq i,z_{i'}=j} K_{\phi}(X_i,X_{i'})}{\sum_{i'\neq i} K_{\phi}(X_i,X_{i'})} f(y_i|\theta_j) \text{ for } j=1,\dots,J^{-i}\\
\frac{\alpha}{n+\alpha-1}f(y_i|\theta_{J^{-i}+1}) \text{ for } j=J^{-i}+1
\end{cases}\]</span></p>
<p>where <span class="math inline">\(f(y_i|\theta_j)\)</span> is the normal density of <span class="math inline">\(y_i\)</span> given the kriging equations with parameter vector <span class="math inline">\(\theta_j\)</span> defining the kernel function. The kriging equations are explained in the posterior predictive distribution section.</p>
<p>Meanwhile, <span class="math inline">\(N_{C_j^{(z)}}(0,\Sigma_{\theta_j})\)</span> is the <span class="math inline">\(C_j^{(z)}\)</span>-dimensional multivariate normal distribution with covariance matrix <span class="math inline">\(\Sigma_{\theta_j}\)</span> defined by a Gaussian kernel function with parameters <span class="math inline">\(\theta_j\)</span>. Similarly, <span class="math inline">\(Gamma^d(a,b)\)</span> is the joint prior over the GP parameters and is the product of <span class="math inline">\(d\)</span> Gamma distributions.</p>
<p>Above, <span class="math inline">\(\theta_j\)</span> is the parameter vector for the GP expert assigned to cluster <span class="math inline">\(j\)</span>, while <span class="math inline">\(\alpha\)</span> is the DP concentration parameter and <span class="math inline">\(\phi\)</span> is the parameter vector for the DP’s occupation number estimate. Note that <span class="math inline">\(\phi\)</span> is purely a vector of lengthscales for a Gaussian kernel. The priors on <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\alpha\)</span>, and <span class="math inline">\(\phi\)</span> are described below.</p>
<p><span class="math display">\[\theta_{j,k}\stackrel{ind}{\sim} Gamma(a_k,b_k) \text{ for } k=1,\dots,d\]</span> <span class="math display">\[\alpha\sim Inv.Gam(1,1),\text{   } \phi_k\stackrel{iid}{\sim} LogN(0,1) \text{ for } k=1,\dots,d\]</span> That is, each element <span class="math inline">\(k\)</span> of <span class="math inline">\(\theta_j\)</span> (the dimension of <span class="math inline">\(X\)</span> plus a noise parameter) is assigned a independent Gamma prior with fixed parameters <span class="math inline">\(a_k\)</span> and <span class="math inline">\(b_k\)</span>. Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of <span class="math inline">\(\phi\)</span> receives an independent log-normal prior.</p>
</section>
<section id="credible-interval-investigation" class="level2">
<h2 class="anchored" data-anchor-id="credible-interval-investigation">Credible interval investigation</h2>
<p>Up to now, the posterior distributions of the fitted values have been highly skewed by outliers, leading to unusually large credible intervals. After investigation, I determined that this was due to an error in the posterior sampling code.</p>
<p>Every so often, the algorithm will generate a cluster with only one data point in it and attempt to fit a GP to that cluster. I learned early on to wrap the model code in a try-catch loop in case the GP function failed. My sampler from the <span class="math inline">\(y\)</span> posterior assumed that any cluster of size 1 could not fit a GP, but this was not always true! In those cases, the sampler used the wrong GP, resulting in inaccurate estimates that defaulted towards the GP mean of zero.</p>
<p>After correcting this, I tested my code on the simulated dataset and found that the credible band was dramatically narrower.</p>
<p>For every fifth iteration, I took the fitted values of the response at each point in time, resulting in a sample of 1000 fitted values for each point. I took the 2.5th and 97.5th quantiles of this sample to get a credible interval for the fitted value at each point. Below, I have plotted the true data as black dots, the mean fitted values as a red line, and the <span class="math inline">\(95\%\)</span> credible intervals as the gray ribbon.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot4-5-8-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="may-1-2025" class="level3">
<h3 class="anchored" data-anchor-id="may-1-2025">May 1, 2025</h3>
</section>
</section>
<section id="summary-7" class="level2">
<h2 class="anchored" data-anchor-id="summary-7">Summary</h2>
<p>This week I updated my explanation of the data model and added a description of the posterior predictive distribution. I also revised my choice of standard deviation for the proposal distribution of <span class="math inline">\(\phi\)</span>’s random walk sampler, bringing its acceptance rate back up to acceptable levels.</p>
</section>
<section id="marginal-data-model" class="level2">
<h2 class="anchored" data-anchor-id="marginal-data-model">Marginal Data Model</h2>
<p>We have an <span class="math inline">\(n\times 1\)</span> continuous response vector <span class="math inline">\(y\)</span> and an <span class="math inline">\(n\times d\)</span> data matrix <span class="math inline">\(X\)</span>. The estimated value of a data point <span class="math inline">\(y_i\)</span> under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing <span class="math inline">\(y_i\)</span> and weighted by a Dirichlet process. Let <span class="math inline">\(z\)</span> represent a possible vector of cluster assignments and let <span class="math inline">\(z(k)\)</span> be the <span class="math inline">\(k^{th}\)</span> element of some ordered list of all possible <span class="math inline">\(z\)</span>. Then <span class="math inline">\(j=1,\dots,J(k)\)</span> index the clusters within <span class="math inline">\(z(k)\)</span>. Let <span class="math inline">\(C_j^{(z(k))}\)</span> be the number of observations in cluster <span class="math inline">\(j\)</span> given assignment <span class="math inline">\(z(k)\)</span>. Then we have as follows.</p>
<p><span class="math display">\[y\sim \sum_{k=1}^{p(n)} \left[\prod_{j=1}^{J(k)} N_{C_j^{(z(k))}}(0,\Sigma_{\theta_j}) \right] w_{z(k)}\]</span> <span class="math display">\[w_Z=P(z=Z|\alpha,\phi)\]</span> where the <span class="math inline">\(w_z\)</span> are the marginal probabilities of a Chinese Restaurant Process that generates cluster assignments <span class="math inline">\(z\)</span> and <span class="math inline">\(p(n)\)</span> is the <a href="https://en.wikipedia.org/wiki/Partition_function_(number_theory)">partition function</a>. The specific CRP is defined by <a href="https://www.tandfonline.com/doi/abs/10.1080/10618600.2000.10474879">“R. M. Neal”</a> (Algorithm 8 in that paper with <span class="math inline">\(m=1\)</span>) and works as follows:</p>
<p>We represent the current cluster state with assignment labels <span class="math inline">\(z=(z_1,\dots,z_n)\)</span> and GP parameter vectors <span class="math inline">\(\theta_1,\dots,\theta_J\)</span> where <span class="math inline">\(J\)</span> is the number of clusters in the current state. For <span class="math inline">\(i=1,\dots,n\)</span>, repeat the following. Let <span class="math inline">\(J^{-}\)</span> be the number of clusters in <span class="math inline">\(z\)</span> with point <span class="math inline">\(i\)</span> removed. Let <span class="math inline">\(\theta_{J^{-}+1}\)</span> be a parameter vector drawn from its prior distribution, in this case <span class="math inline">\(Gamma^d(a,b)\)</span>. Draw a new value for <span class="math inline">\(z_i\)</span> with the following conditional probabilities:</p>
<p><span class="math display">\[P(z_i=j|z_{-i},y_i,\theta)=\begin{cases}
b\frac{n-1}{n+\alpha-1}\frac{\sum_{i'\neq i,z_{i'}=j} K_{\phi}(X_i,X_{i'})}{\sum_{i'\neq i} K_{\phi}(X_i,X_{i'})} F(y_i|\theta_j) \text{ for } j=1,\dots,J^{-}\\
b\frac{\alpha}{n+\alpha-1}F(y_i|\theta_{J^{-}+1}) \text{ for } j=J^{-}+1
\end{cases}\]</span> where <span class="math inline">\(b\)</span> is a normalizing constant. Note that <span class="math inline">\(b\)</span> need not be calculated; we can just calculate the un-normalized probability vector and then normalize it.</p>
<p>Meanwhile, <span class="math inline">\(N_{C_j^{(z)}}(0,\Sigma_{\theta_j})\)</span> is the <span class="math inline">\(C_j^{(z)}\)</span>-dimensional multivariate normal distribution with covariance matrix <span class="math inline">\(\Sigma_{\theta_j}\)</span> defined by a Gaussian kernel function with parameters <span class="math inline">\(\theta_j\)</span>. Similarly, <span class="math inline">\(Gamma^d(a,b)\)</span> is the joint prior over the GP parameters and is the product of <span class="math inline">\(d\)</span> Gamma distributions.</p>
<p>Above, <span class="math inline">\(\theta_j\)</span> is the parameter vector for the GP expert assigned to cluster <span class="math inline">\(j\)</span>, while <span class="math inline">\(\alpha\)</span> is the DP concentration parameter and <span class="math inline">\(\phi\)</span> is the parameter vector for the DP’s occupation number estimate. Note that <span class="math inline">\(\phi\)</span> is purely a vector of lengthscales for a Gaussian kernel. The priors on <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\alpha\)</span>, and <span class="math inline">\(\phi\)</span> are described below.</p>
<p><span class="math display">\[\theta_{j_k}\stackrel{ind}{\sim} Gamma(a_k,b_k) \text{ for } k=1,\dots,d\]</span> <span class="math display">\[\alpha\sim Inv.Gam(1,1),\text{   } \phi_k\stackrel{iid}{\sim} LogN(0,1) \text{ for } k=1,\dots,d\]</span> That is, each element <span class="math inline">\(k\)</span> of <span class="math inline">\(\theta_j\)</span> (the dimension of <span class="math inline">\(X\)</span> plus a noise parameter) is assigned a independent Gamma prior with fixed parameters <span class="math inline">\(a_k\)</span> and <span class="math inline">\(b_k\)</span>. Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of <span class="math inline">\(\phi\)</span> receives an independent log-normal prior.</p>
</section>
<section id="posterior-predictive-distribution" class="level2">
<h2 class="anchored" data-anchor-id="posterior-predictive-distribution">Posterior Predictive Distribution</h2>
<p>The paper by Rasmussen and Ghahramani does not discuss what a posterior predictive distribution would look like. However, it can be deduced based on the posterior predictive distribution of a lone Gaussian process. With <span class="math inline">\(p(n)\)</span> and <span class="math inline">\(w_{z(k)}\)</span> defined the same as in the original data model, we determine that the distribution of a new point <span class="math inline">\(y^*\)</span> with data <span class="math inline">\(x^*\)</span> is as follows.</p>
<p><span class="math display">\[y^*\sim \sum_{k=1}^{p(n)} \left[\prod_{j=1}^{J_k} N(\mu_j^*,\sigma_j^*) \right] w_k\]</span> where <span class="math inline">\(\mu_j^*\)</span> and <span class="math inline">\(\Sigma_j^*\)</span> are found according to the kriging equations of a Gaussian process. That is,</p>
<p><span class="math display">\[\mu_j^*=K(x^*,X^{(j)})^TK^{-1}y^{(j)}\text{ and } \sigma_j^*=K(x^*,x^*)-K(x^*,X^{(j)})^T K^{-1}K(x^*,X^{(j)})\]</span> where <span class="math inline">\(K\)</span> is the covariance matrix based on parameters <span class="math inline">\(\theta_j\)</span> and <span class="math inline">\(X^{(j)}\subset X\)</span> and <span class="math inline">\(y^{(j)} \subset y\)</span> are the data and response values associated with cluster <span class="math inline">\(j\)</span>.</p>
</section>
<section id="simulated-example" class="level2">
<h2 class="anchored" data-anchor-id="simulated-example">Simulated Example</h2>
<p>We previously had a very small acceptance rate in the random walk sampler for <span class="math inline">\(\phi\)</span>, which seems to have been due to tiny Hessian values that result in a giant standard deviation in the (Normal) proposal distribution. Consequently, most proposals were far too large to be accepted. My current solution was to put a cap on the proposal standard deviation equal to the range of <span class="math inline">\(x\)</span>. As <span class="math inline">\(\phi\)</span> represents the rate of decay in correlation between neighboring points, values greater than the range of the data would be largely indistinguishable anyways. In result, <span class="math inline">\(\phi\)</span> seems well mixed and has an acceptance rate of <span class="math inline">\(69.7\%\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot1-5-1-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot2-5-1-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot3-5-1-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
</div>
<p>For every fifth iteration, I took the fitted values of the response at each point in time, resulting in a sample of 1000 fitted values for each point. I took the 2.5th and 97.5th quantiles of this sample to get a credible interval for the fitted value at each point. Below, I have plotted the true data as black dots, the mean fitted values as a red line, and the <span class="math inline">\(95\%\)</span> credible intervals as the gray ribbon. As the generating function was known, that too is plotted as a blue line.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot4-5-1-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The fitted mean line still does a good job of approximating the true function.</p>
<section id="april-24-2025" class="level3">
<h3 class="anchored" data-anchor-id="april-24-2025">April 24, 2025</h3>
</section>
</section>
<section id="summary-8" class="level2">
<h2 class="anchored" data-anchor-id="summary-8">Summary</h2>
<p>This week, I further refined my explanation of the data model, particularly the base distribution of the Dirichlet process. I studied the cluster assignment update algorithm described by Rasmussen and <a href="https://www.tandfonline.com/doi/abs/10.1080/10618600.2000.10474879">“R. M. Neal”</a>, which clarified that the base distribution is used to generate the parameters controlling <span class="math inline">\(y_i\)</span> given a new cluster. Based on this, I can identify the base distribution to be the prior on the Gaussian process parameters: a product of independent Gamma priors.</p>
<p>While reviewing Neal’s explanation, I identified an error in my implementation of the cluster update step, stemming from how the ‘garg’ function in laGP may select <span class="math inline">\((0,0)\)</span> for the parameters of the Gamma prior on the GP nugget. While laGP considers this to be “no prior” on the nugget, other parts of my algorithm that drew on this prior did not. Correcting this had a noticeable impact on the number of clusters generated.</p>
<p>Lastly, I simulated a new data set of <span class="math inline">\(100\)</span> points using a custom mean function and tested the iMGPE algorithm on it. This allows me to examine how well my algorithm approximates the true mean function.</p>
</section>
<section id="data-model-1" class="level2">
<h2 class="anchored" data-anchor-id="data-model-1">Data Model</h2>
<p>We have an <span class="math inline">\(n\times 1\)</span> continuous response vector <span class="math inline">\(y\)</span> and an <span class="math inline">\(n\times d\)</span> data matrix <span class="math inline">\(X\)</span>. The estimated value of a data point <span class="math inline">\(y_i\)</span> under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing <span class="math inline">\(y_i\)</span> and weighted by a Dirichlet process. Let <span class="math inline">\(z\)</span> represent a possible vector of cluster assignments and <span class="math inline">\(j=1,\dots,|z|\)</span> index the clusters within <span class="math inline">\(z\)</span>, where <span class="math inline">\(|z|\)</span> is the number of clusters. Let <span class="math inline">\(C_j^{(z)}\)</span> be the number of observations in cluster <span class="math inline">\(j\)</span> given assignment <span class="math inline">\(z\)</span>. Then we have as follows.</p>
<p><span class="math display">\[y|X\sim \sum_z \left[\prod_{j=1}^{|z|} N_{C_j^{(z)}}(0,\Sigma_{\theta_j}) \right] w_z\]</span> <span class="math display">\[w\sim DP_{\phi}(\alpha,  Gamma^d(a,b))\]</span> where <span class="math inline">\(w\)</span> is a vector of probabilities defining a multinomial distribution drawn from a Dirichlet process and <span class="math inline">\(w_z\)</span> is the probability of observing cluster assignment <span class="math inline">\(z\)</span>. That is, <span class="math inline">\(P(z=Z|\alpha,\phi)=w_Z\)</span> and so <span class="math inline">\(z|\alpha,\phi\)</span> is generated by a Chinese Restaurant process based on a <span class="math inline">\(DP_{\phi}(\alpha, Gamma^d(a,b))\)</span> distribution.</p>
<p>Meanwhile, <span class="math inline">\(N_{C_j^{(z)}}(0,\Sigma_{\theta_j})\)</span> is the <span class="math inline">\(C_j^{(z)}\)</span>-dimensional multivariate normal distribution with covariance matrix <span class="math inline">\(\Sigma_{\theta_j}\)</span> defined by a Gaussian kernel function with parameters <span class="math inline">\(\theta_j\)</span>. Similarly, <span class="math inline">\(Gamma^d(a,b)\)</span> is the joint prior over the GP parameters and is the product of <span class="math inline">\(d\)</span> Gamma distributions.</p>
<p>Above, <span class="math inline">\(\theta_j\)</span> is the parameter vector for the GP expert assigned to cluster <span class="math inline">\(j\)</span>, while <span class="math inline">\(\alpha\)</span> is the DP concentration parameter and <span class="math inline">\(\phi\)</span> is the parameter vector for the DP’s occupation number estimate. Note that <span class="math inline">\(\phi\)</span> is purely a vector of lengthscales for a Gaussian kernel. The priors on <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\alpha\)</span>, and <span class="math inline">\(\phi\)</span> are described below.</p>
<p><span class="math display">\[\theta_{j_k}\stackrel{ind}{\sim} Gamma(a_k,b_k) \text{ for } k=1,\dots,d\]</span> <span class="math display">\[\alpha\sim Inv.Gam(1,1),\text{   } \phi_k\stackrel{iid}{\sim} LogN(0,1) \text{ for } k=1,\dots,d\]</span> That is, each element <span class="math inline">\(k\)</span> of <span class="math inline">\(\theta_j\)</span> (the dimension of <span class="math inline">\(X\)</span> plus a noise parameter) is assigned a independent Gamma prior with fixed parameters <span class="math inline">\(a_k\)</span> and <span class="math inline">\(b_k\)</span>. Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of <span class="math inline">\(\phi\)</span> receives an independent log-normal prior.</p>
</section>
<section id="description-of-simulated-data" class="level2">
<h2 class="anchored" data-anchor-id="description-of-simulated-data">Description of Simulated Data</h2>
<p>This week, I simulated a sample data set for use with the iMGPE algorithm. Whereas with the motorcycle data set we lack knowledge of the true generating function, the simulated data set is generated by a known custom function. Assuming that <span class="math inline">\(y=f(X)+\epsilon(X)\)</span>, for the simulated data set we can compare the fitted estimate of <span class="math inline">\(f(X)\)</span> with the truth. The generating function is <span class="math display">\[f(x)= \begin{cases}
\frac{7}{3}-\frac{2}{3}x\text{ if } x\leq 2\\
\cos(\pi x)\text{ if } x&gt;2
\end{cases}\]</span></p>
<p>I drew a random uniform sample of <span class="math inline">\(x\)</span> of size <span class="math inline">\(100\)</span> from the interval <span class="math inline">\((0,5)\)</span> and added iid random noise <span class="math inline">\(\epsilon\)</span> drawn from a <span class="math inline">\(N(0,0.04)\)</span> distribution. With <span class="math inline">\(y=f(X)+\epsilon\)</span>, the simulated data set and the function <span class="math inline">\(f\)</span> are plotted below.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot1-4-24-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="application-of-imgpe" class="level2">
<h2 class="anchored" data-anchor-id="application-of-imgpe">Application of iMGPE</h2>
<p>I was using the ‘darg’ and ‘garg’ functions from the ‘laGP’ R package to select priors for the GP lengthscale and nugget parameters. However, the ‘garg’ function was setting a <span class="math inline">\(Gamma(0,0)\)</span> prior on the nugget. For the purposes of fitting a GP, this is treated as no prior on the nugget, but I was drawing values from these priors elsewhere in the algorithm and ‘rgamma(n,0,0)’ always returns <span class="math inline">\(0\)</span>. Once I corrected this by specifying a vague prior for the nugget should ‘garg’ fail, the algorithm’s behavior changed significantly with respect to the value of <span class="math inline">\(\alpha\)</span> and the number of clusters.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot5-4-24-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot4-4-24-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot3-4-24-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
</div>
<p>For every fifth iteration, I took the fitted values of the response at each point in time, resulting in a sample of 1000 fitted values for each point. I took the 2.5th and 97.5th quantiles of this sample to get a credible interval for the fitted value at each point. Below, I have plotted the true data as black dots, the mean fitted values as a red line, and the <span class="math inline">\(95\%\)</span> credible intervals as the gray ribbon. As the generating function was known, that too is plotted as a blue line.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot2-4-24-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The fitted mean line is following the true function fairly closely, though the credible bounds are still wider than seems reasonable.</p>
<section id="april-17-2025" class="level3">
<h3 class="anchored" data-anchor-id="april-17-2025">April 17, 2025</h3>
</section>
</section>
<section id="summary-9" class="level2">
<h2 class="anchored" data-anchor-id="summary-9">Summary</h2>
<p>This week I improved my breakdown of the data model and identified the base distribution of the Dirichlet process, which I believe to be a multivariate normal distribution. I detailed the derivation of the pseudo-posterior density for <span class="math inline">\(\phi\)</span> and explain that the full conditional density is never directly defined and thus not trivial to derive.</p>
<p>I also determined why my trace plot for <span class="math inline">\(\alpha\)</span> seemed to hit a ‘ceiling’, which was that the proposal distribution for my quantile slice sampler was misspecified. I substituted a proposal with heavier tails and the apparent ceiling disappeared. However, this had significant repercussions on my parameter values. Now, <span class="math inline">\(\alpha\)</span> took on much larger average values, and the number of clusters increased dramatically. This resulted in many more experts fit to only a handful of data points, which had negative repercussions for the accuracy of the fitted model.</p>
</section>
<section id="data-model-2" class="level2">
<h2 class="anchored" data-anchor-id="data-model-2">Data Model</h2>
<p>We have an <span class="math inline">\(n\times 1\)</span> continuous response vector <span class="math inline">\(y\)</span> and an <span class="math inline">\(n\times d\)</span> data matrix <span class="math inline">\(X\)</span>. The estimated value of a data point <span class="math inline">\(y_i\)</span> under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing <span class="math inline">\(y_i\)</span> and weighted by a Dirichlet process. Let <span class="math inline">\(z\)</span> represent a possible vector of cluster assignments and <span class="math inline">\(j\)</span> index the clusters within <span class="math inline">\(z\)</span>. Let <span class="math inline">\(C_j\)</span> be the number of observations in cluster <span class="math inline">\(j\)</span>. Then we have as follows.</p>
<p><span class="math display">\[y\sim \sum_z \left[\prod_j N_{C_j}(0,\Sigma_{\theta_j}) \right] w_z\]</span> <span class="math display">\[w\sim DP_{\phi}(\alpha, N_n(0,I_n))\]</span> where <span class="math inline">\(w\)</span> is a vector of probabilities defining a multinomial distribution drawn from a Dirichlet process and <span class="math inline">\(w_z\)</span> is the probability of observing cluster assignment <span class="math inline">\(z\)</span>. That is, <span class="math inline">\(P(z=Z|\alpha,\phi)=w_Z\)</span> and so <span class="math inline">\(z|\alpha,\phi\)</span> is generated by a <span class="math inline">\(DP_{\phi}(\alpha, N_n(0,I_n))\)</span> clustering process. Meanwhile, <span class="math inline">\(N_{C_j}(0,\Sigma_{\theta_j})\)</span> is the <span class="math inline">\(C_j\)</span>-dimensional multivariate normal distribution with covariance matrix <span class="math inline">\(\Sigma_{\theta_j}\)</span> defined by a Gaussian kernel function with parameters <span class="math inline">\(\theta_j\)</span>. Similarly, <span class="math inline">\(N_n(0,I_n)\)</span> is the <span class="math inline">\(n\)</span>-dimensional normal distribution where <span class="math inline">\(I_n\)</span> is the <span class="math inline">\(n\)</span>-dimensional identity matrix.</p>
<p>Above, <span class="math inline">\(\theta_j\)</span> is the parameter vector for the GP expert assigned to cluster <span class="math inline">\(j\)</span>, while <span class="math inline">\(\alpha\)</span> is the DP concentration parameter and <span class="math inline">\(\phi\)</span> is the parameter vector for the DP’s occupation number estimate. Note that <span class="math inline">\(\phi\)</span> is purely a vector of lengthscales for a Gaussian kernel. The priors on <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\alpha\)</span>, and <span class="math inline">\(\phi\)</span> are described below.</p>
<p><span class="math display">\[\theta_{j_k}\stackrel{ind}{\sim} Gamma(a_k,b_k)\]</span> <span class="math display">\[\alpha\sim Inv.Gam(1,1),\text{   } \phi_k\stackrel{iid}{\sim} LogN(0,1)\]</span> That is, each element <span class="math inline">\(k\)</span> of <span class="math inline">\(\theta_j\)</span> (the dimension of <span class="math inline">\(X\)</span> plus a noise parameter) is assigned a independent Gamma prior with fixed parameters <span class="math inline">\(a_k\)</span> and <span class="math inline">\(b_k\)</span>. Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of <span class="math inline">\(\phi\)</span> receives an independent log-normal prior.</p>
</section>
<section id="handling-alpha-and-phi" class="level2">
<h2 class="anchored" data-anchor-id="handling-alpha-and-phi">Handling Alpha and Phi</h2>
<p>Last week, the trace plot of <span class="math inline">\(\alpha\)</span> appeared to hit a ‘ceiling’ at around <span class="math inline">\(3.67\)</span>. After some experimentation, I determined that my quantile slice sampling function was numerically unstable above that ceiling. The reason was that the quantile slice sampler relies on a proposal distribution and my initial choice of <span class="math inline">\(Gamma(1,10)\)</span> was too light-tailed. I chose a more reasonable proposal distribution of <span class="math inline">\(Gamma(1,1)\)</span> and the instability disappeared.</p>
<p>With regard to <span class="math inline">\(\phi\)</span>, we wanted to know its true full conditional distribution. The conditional pseudo-posterior distribution for <span class="math inline">\(\phi\)</span> that we draw from in its Gibbs sampling step is shown on the right side of the equation below. We use it for now because <span class="math inline">\(p(z|y,\phi,\alpha)\)</span> is not defined directly; only the conditionals <span class="math inline">\(p(z_i|y,\phi,\alpha)\)</span> are defined. As before, the prior <span class="math inline">\(p(\phi)\)</span> is a log-normal density with parameters <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma^2=1\)</span>.</p>
<p><span class="math display">\[p(\phi|z,\alpha,\dots)\propto p(z|y,\phi,\alpha)p(\phi)\approx \left[\prod_{i=1}^n p(z_i|y,\phi,\alpha) \right] p(\phi)\]</span> The individual conditionals are as follows, taking <span class="math inline">\(j\)</span> to be the current value of <span class="math inline">\(z_i\)</span> and <span class="math inline">\(y^{(j)}\)</span> to be the subset of <span class="math inline">\(y\)</span> belonging to cluster <span class="math inline">\(j\)</span>.</p>
<p><span class="math display">\[p(z_i=j|y,z_{-i},\dots) \propto p(y^{(j)}|z_i=j,z_{-i},\theta_j)p(z_i=j|\phi,\alpha)\]</span> <span class="math display">\[\text{where } p(y^{(j)}|z_i=j,z_{-i},\theta_j)=N_{C_j}(0,\Sigma_{\theta_j}) \text{ and}\]</span> <span class="math display">\[p(z_i=j|z_{-i},\phi,\alpha)=\frac{n-1}{n-1+\alpha}\frac{\sum_{i'\neq i,z_{i'}=j} K_{\phi}(X_i,X_{i'})}{\sum_{i'\neq i} K_{\phi}(X_i,X_{i'})}\]</span></p>
<p>Since <span class="math inline">\(p(y^{(j)}|\cdot)\)</span> does not depend on <span class="math inline">\(\phi\)</span>, it factors out of the posterior density and we are left with <span class="math inline">\(p(\phi|z,\alpha,\dots)\approx \left[\prod_{i=1}^n p(z_i|y,\phi,\alpha) \right] p(\phi)\)</span>.</p>
</section>
<section id="practical-experiment" class="level2">
<h2 class="anchored" data-anchor-id="practical-experiment">Practical Experiment</h2>
<p>I set the proposal distribution for <span class="math inline">\(\alpha\)</span> to <span class="math inline">\(Gamma(1,1)\)</span> and ran the algorithm on the motorcycle dataset again. The concentration parameter <span class="math inline">\(\alpha\)</span> is now centered around <span class="math inline">\(15\)</span> rather than <span class="math inline">\(3\)</span>, with a commensurate effect on the number of clusters, which now averages around <span class="math inline">\(30\)</span>. Though <span class="math inline">\(\alpha\)</span>’s trace plot now appears properly mixed, I am not sure I like the effect on the cluster output. With so many clusters and only <span class="math inline">\(94\)</span> data points, most clusters now contain only <span class="math inline">\(2\)</span> or <span class="math inline">\(3\)</span> data points.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot1-4-17-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot2-4-17-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot3-4-17-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
</div>
<p>For every fifth iteration, I took the fitted values of the response at each point in time, resulting in a sample of 1000 fitted values for each point. I took the 2.5th and 97.5th quantiles of this sample to get a credible interval for the fitted value at each point. Below, I have plotted the true data as black dots, the mean fitted values as a red line, and the <span class="math inline">\(95\%\)</span> credible intervals as the gray ribbon.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot4-4-17-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The fitted mean line is not following the data very well, and the credible band is much wider than before. I suspect this loss of precision is due to the excessive number of experts fit to little data.</p>
<section id="april-10-2025" class="level3">
<h3 class="anchored" data-anchor-id="april-10-2025">April 10, 2025</h3>
</section>
</section>
<section id="summary-10" class="level2">
<h2 class="anchored" data-anchor-id="summary-10">Summary</h2>
<p>This week, I expanded upon my description of the data model. I also corrected some numerical stability issues in the <span class="math inline">\(\phi\)</span> update step that resulted in significant improvements in its acceptance rate. Lastly, I plotted a heatmap of cluster memberships that displayed which points were most often clustered together.</p>
</section>
<section id="full-data-model" class="level2">
<h2 class="anchored" data-anchor-id="full-data-model">Full Data Model</h2>
<p>The estimated value of a data point <span class="math inline">\(y_i\)</span> under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing <span class="math inline">\(y_i\)</span> and weighted by a Dirichlet process. Letting <span class="math inline">\(\mathbf{z}\)</span> represent a possible vector of cluster assignments and <span class="math inline">\(j\)</span> index the clusters within <span class="math inline">\(\mathbf{z}\)</span>, we have as follows.</p>
<p><span class="math display">\[\mathbf{y}\sim \sum_{\mathbf{z}} \left[\prod_j GP_j(\theta_j) \right] w_{\mathbf{z}}\]</span> <span class="math display">\[\mathbf{w}\sim DP(\alpha, GP(\phi))\]</span> where <span class="math inline">\(\mathbf{w}\)</span> is a vector of probabilities defining a multinomial distribution drawn from a Dirichlet process and <span class="math inline">\(w_{\mathbf{z}}\)</span> is the probability of observing cluster assignment <span class="math inline">\(\mathbf{z}\)</span>. That is, <span class="math inline">\(P(\mathbf{z}=Z|\alpha,\phi)=w_Z\)</span> and so <span class="math inline">\(\mathbf{z}|\alpha,\phi \sim DP(\alpha, GP(\phi))\)</span>.</p>
<p>Above, <span class="math inline">\(\theta_j\)</span> is the parameter vector for the GP expert assigned to cluster <span class="math inline">\(j\)</span>, while <span class="math inline">\(\alpha\)</span> is the DP concentration parameter and <span class="math inline">\(\phi\)</span> is the parameter vector for the DP’s base distribution. Note that <span class="math inline">\(\phi\)</span> is purely a vector of lengthscales; the nugget is assumed to be zero. The priors on <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\alpha\)</span>, and <span class="math inline">\(\phi\)</span> are described below.</p>
<p><span class="math display">\[\theta_{j_k}\stackrel{ind}{\sim} Gamma(a_k,b_k)\]</span> <span class="math display">\[\alpha\sim Inv.Gam(1,1),\text{   } \phi_k\stackrel{iid}{\sim} LogN(0,1)\]</span> That is, each element <span class="math inline">\(k\)</span> of <span class="math inline">\(\theta_j\)</span> (the dimension of <span class="math inline">\(X\)</span> plus a noise parameter) is assigned a independent Gamma prior with fixed parameters <span class="math inline">\(a_k\)</span> and <span class="math inline">\(b_k\)</span>. Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of <span class="math inline">\(\phi\)</span> receives an independent log-normal prior.</p>
</section>
<section id="distribution-functions" class="level2">
<h2 class="anchored" data-anchor-id="distribution-functions">Distribution Functions</h2>
<p>The iMGPE algorithm is a Gibbs-sampling method, which means we must draw a new value of <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\alpha\)</span> from their conditional distributions each iteration. The conditional distribution of <span class="math inline">\(\alpha\)</span> only depends on the number of data points <span class="math inline">\(N\)</span> and the number of clusters <span class="math inline">\(J\)</span>, as seen here.</p>
<p><span class="math display">\[p(\alpha|n, J,\dots)\propto \alpha^{J-3/2}\exp(-1/2\alpha)\Gamma(\alpha)/\Gamma(n+\alpha)\]</span></p>
<p>The conditional distribution for <span class="math inline">\(\phi\)</span> is more difficult to obtain, as the Dirichlet process is only defined through the following conditional probabilities.</p>
<p><span class="math display">\[p(z_i=j|z_{-i},\phi,\alpha)=\frac{n-1}{n-1+\alpha}\frac{\sum_{i'\neq i,z_{i'}=j} K_{\phi}(X_i,X_{i'})}{\sum_{i'\neq i} K_{\phi}(X_i,X_{i'})}\]</span> <span class="math display">\[p(z_i\neq z_{i'}\text{ for all }i'\neq i|z_{-i},\alpha) =\frac{\alpha}{n-1+\alpha} \]</span></p>
<p>Therefore, Rasmussen et al chose to sample from the pseudo-posterior of <span class="math inline">\(\phi\)</span>, defined as the leave-one-out pseudo-likelihood (the product of the conditional probabilities of the <span class="math inline">\(z_i\)</span>) and the prior on <span class="math inline">\(\phi\)</span>. That is,</p>
<p><span class="math display">\[p^*(\phi|\mathbf{z},\alpha)=\prod_{i=1}^n p(z_i=j|\phi,\alpha)\times p(\phi)\]</span></p>
</section>
<section id="practical-experiments" class="level2">
<h2 class="anchored" data-anchor-id="practical-experiments">Practical Experiments</h2>
<p>Last week, I had difficulties with a low acceptance rate in <span class="math inline">\(\phi\)</span>’s random walk sampler. After examining my code, I determined that the cause was numerical instability in my R function for calculating <span class="math inline">\(\log(p^*(\phi|\cdot))\)</span>. Once I revised my posterior density function to replace values of ‘-Inf’ with <span class="math inline">\(-1000\)</span>, the sampler achieved an acceptance rate of <span class="math inline">\(34\%\)</span>.</p>
<p>With these revisions, I reran the motorcycle example from before.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot3-4-10-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot2-4-10-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
</div>
<p>The trace plots for both <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\phi\)</span> now appear to be well mixed, with <span class="math inline">\(\phi\)</span> achieving an acceptance rate of <span class="math inline">\(34\%\)</span>. As before, for every fifth iteration, I took the fitted values of the response at each point in time, resulting in a sample of 1000 fitted values for each point. I took the 2.5th and 97.5th quantiles of this sample to get a credible interval for the fitted value at each point. Below, I have plotted the true data as black dots, the mean predicted values as a red line, and the <span class="math inline">\(95\%\)</span> credible intervals as the gray ribbon.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot4-4-10-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The credible interval is still unusually wide, especially around the ‘dip’ at time 20.</p>
<p>This week I also prepared a visual to display ‘standard cluster memberships’. Below is shown a heatmap displaying the frequency of each pair of data points belonging to the same cluster. Points are ordered by time, so they appear in the same order from left to right as in the plot above. There are two distinct clusters near the beginning.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot1-4-10-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="april-3-2025" class="level3">
<h3 class="anchored" data-anchor-id="april-3-2025">April 3, 2025</h3>
</section>
</section>
<section id="summary-11" class="level2">
<h2 class="anchored" data-anchor-id="summary-11">Summary</h2>
<p>This week I elaborated upon the iMGPE model and further improved the algorithm. The trace plot for <span class="math inline">\(\phi\)</span> now appears to be converging. I should be able to convert the algorithm to Rcpp this week.</p>
</section>
<section id="data-model-3" class="level2">
<h2 class="anchored" data-anchor-id="data-model-3">Data Model</h2>
<p>We have an explanatory data matrix <span class="math inline">\(\mathbf{x}\)</span> and a scalar response <span class="math inline">\(\mathbf{y}\)</span>, indicators <span class="math inline">\(z_i\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>, representing the cluster assignments for each point. Clusters are determined by a Dirichlet process with concentration parameter <span class="math inline">\(\alpha\)</span> and a Gaussian kernel with lengthscale parameter vector <span class="math inline">\(\phi\)</span>. For each Gaussian process expert indexed by <span class="math inline">\(j\)</span>, we have a parameter vector <span class="math inline">\(\theta_j\)</span>. Given all this, the likelihood of the data is a sum over exponentially many possible cluster assignments.</p>
<p><span class="math display">\[p(\mathbf{y}|\mathbf{x},\theta)= \sum_{\mathbf{z}}\left[ \prod_j p(\{y_i:z_i=j\}| \{x_i:z_i=j\},\theta_j) \right]p(\mathbf{z}|\mathbf{x},\alpha,\phi)\]</span></p>
<p>In the above likelihood, <span class="math inline">\(p(\{y_i:z_i=j\}| \{x_i:z_i=j\},\theta_j)\)</span> is the likelihood of the Gaussian process fit to cluster <span class="math inline">\(j\)</span>. The iMGPE algorithm performs Gibbs sampling of this likelihood. Lastly, <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\phi\)</span>, and all <span class="math inline">\(\theta_j\)</span> are assigned priors. We give <span class="math inline">\(\alpha\)</span> a vague inverse gamma prior, while each component of <span class="math inline">\(\phi\)</span> is assigned a vague gamma prior. The components of the <span class="math inline">\(\theta_j\)</span> each receive a gamma prior as well, so that, for example, the nugget parameters of every expert share the same prior.</p>
</section>
<section id="updated-results" class="level2">
<h2 class="anchored" data-anchor-id="updated-results">Updated Results</h2>
<p>I adjusted the update step for <span class="math inline">\(\phi\)</span>. It still uses a Random Walk Metropolis-Hastings sampler with with a normal proposal, but the variance of the proposal is set to the optimal value of <span class="math inline">\(\frac{2.38^2}{d}\mathbf{H}^{-1}\)</span> where <span class="math inline">\(d\)</span> is the number of dimensions and <span class="math inline">\(\mathbf{H}\)</span> is the Hessian matrix of the posterior density for <span class="math inline">\(\phi\)</span>. I realized that I had forgotten to invert the Hessian in prior tests, which is why I was getting very small proposal variances.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot2-4-3-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot3-4-3-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
</div>
<p>The posterior distribution of <span class="math inline">\(\phi\)</span> has changed significantly, and now has a mean of about <span class="math inline">\(88\)</span>. Its acceptance probability is still very low, around <span class="math inline">\(3\%\)</span>.</p>
<p>For every fifth iteration, I took the fitted values of the response at each point in time, resulting in a sample of 1000 fitted values for each point. I took the 2.5th and 97.5th quantiles of this sample to get a credible interval for the fitted value at each point. Below, I have plotted the true data as black dots, the mean predicted values as a red line, and the <span class="math inline">\(95\%\)</span> credible intervals as the gray ribbon.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot1-4-3-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="march-27-2025" class="level3">
<h3 class="anchored" data-anchor-id="march-27-2025">March 27, 2025</h3>
</section>
</section>
<section id="summary-12" class="level2">
<h2 class="anchored" data-anchor-id="summary-12">Summary</h2>
<p>This week I improved the update steps for the parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\phi\)</span> in the Infinite Mixture of GP Experts function. I also altered the function output to report a credible interval for the value of each point in the data set.</p>
<p>I have begun converting parts of the iMGPE algorithm to C++ with Rcpp. Performance gains are modest for now, but may increase in the future.</p>
</section>
<section id="changes-to-dp-parameter-updates" class="level2">
<h2 class="anchored" data-anchor-id="changes-to-dp-parameter-updates">Changes to DP Parameter Updates</h2>
<p>I rewrote the code for updating the Dirichlet process parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\phi\)</span>. First, I implemented a slice sampling update for <span class="math inline">\(\alpha\)</span> and attempted the same for <span class="math inline">\(\phi\)</span>, though I wasn’t able to debug that in time. Instead, I adjusted the update step for <span class="math inline">\(\phi\)</span> by implementing a minimum standard deviation for the proposal density of its rejection sampling algorithm.</p>
<p>Currently, the proposal is a normal density with variance equal to the hessian of the density function for <span class="math inline">\(\phi\)</span>. This was recommended by Rasmussen et al since it removes the need for a tunable parameter, but I have found that the Hessian produces extremely small proposal variances, leading to extremely small step sizes. After making the changes described, I fit the model to the motorcycle dataset with 5000 iterations. The trace plots for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\phi\)</span> after these adjustments are shown below.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot3-3-27-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot4-3-27-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
</div>
<p>The trace plot for <span class="math inline">\(\alpha\)</span> looks good, but the one for <span class="math inline">\(\phi\)</span> failed to converge. It may be that the average step size is still too small.</p>
</section>
<section id="updated-results-1" class="level2">
<h2 class="anchored" data-anchor-id="updated-results-1">Updated Results</h2>
<p>The distribution of cluster counts is shown below. It is roughly the same as in last meeting, concentrating around 6 to 8 clusters.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot2-3-27-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>For every fifth iteration, I predicted the value of the response at each point in time. I took the 2.5th and 97.5th quantiles of this sample to get a credible interval for the fitted value at each point. Below, I have plotted the true data as black dots, the mean predicted values as a red line, and the <span class="math inline">\(95\%\)</span> credible intervals as the gray ribbon.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot1-3-27-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The credible interval bounds are much narrower than the prediction intervals plotted in the last meeting, especially for the early times.</p>
<section id="march-13-2025" class="level3">
<h3 class="anchored" data-anchor-id="march-13-2025">March 13, 2025</h3>
</section>
</section>
<section id="summary-13" class="level2">
<h2 class="anchored" data-anchor-id="summary-13">Summary</h2>
<p>This week I completed coding a version of the Infinite Mixture of GP Experts model proposed by Rasmussen and Ghahramani. I fit it to the motorcycle crash dataset, an example also used in the original paper. The algorithm is performing as intended, though the update steps for the Dirichlet process parameters may need to be adjusted.</p>
</section>
<section id="initial-results" class="level2">
<h2 class="anchored" data-anchor-id="initial-results">Initial Results</h2>
<p>I tested the algorithm first on the motorcycle crash dataset, a simple dataset of the acceleration of the head of a crash test dummy over time as its car hit a wall. There was thus one continuous input, time, and one continuous output, acceleration. Below is a plot of the motorcycle dataset.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot1-3-12-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>I fit a model to this dataset using the iMGPE algorithm, which ran for 5000 iterations. In each iteration, this algorithm generated a random assignment of data points to clusters using a modified Dirichlet process, fit a Gaussian process model to each cluster, and updated the cluster assignment parameters using MCMC. The posterior distributions of those parameters are discussed below, along with some other key features of the fitted model.</p>
<p>The cluster assignment probabilities were controlled by two parameters, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\phi\)</span>, where <span class="math inline">\(\alpha\)</span> is the Dirichlet process concentration parameter and <span class="math inline">\(\phi\)</span> is the lengthscale vector for a Gaussian kernel. In this example, <span class="math inline">\(\phi\)</span> is scalar since there is only one input variable. Their trace plots are shown here.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot2-3-12-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot3-3-12-25.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
</div>
<p>Below is a histogram of the number of clusters generated in each iteration. The algorithm generally used between six and seven clusters.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot4-3-12-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>At each step of the algorithm, I drew a random sample from the posterior distributions of each data point, for a new set of 94 points. I discarded the first 2500 draws as burn-in and recorded the mean, median, and the 2.5th and 97.5th quantiles for each data point. These are plotted below, alongside the original data. The red line is the median and the grey ribbon covers the <span class="math inline">\(95\%\)</span> confidence band.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot5-3-12-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>The trace plots for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\phi\)</span> indicate some problems with their MH updates. The plot for <span class="math inline">\(\phi\)</span> clearly shows that new proposals are being rejected far too often. I will have to experiment with the acceptance probability to bring it into a reasonable range. The plot for <span class="math inline">\(\alpha\)</span> could indicate that the step size of their random walk is too small, another parameter that may need fine tuning.</p>
<p>Rasmussen and Ghahramani’s own experiment on this data set yielded a much smoother median line. My algorithm may be overfitting compared to theirs.</p>
<p>Furthermore, it is not clear from Rasmussen’s paper if or how they intended to perform inference on new data given their model. Inference could be performed for any given set of experts but to draw from a posterior predictive distribution the same way the authors draw from the posterior would require the new data to be available during the model fitting process.</p>
<section id="february-27-2025" class="level3">
<h3 class="anchored" data-anchor-id="february-27-2025">February 27, 2025</h3>
</section>
</section>
<section id="summary-14" class="level2">
<h2 class="anchored" data-anchor-id="summary-14">Summary</h2>
<p>This week I worked on developing R code to run the iMGPE method by Rasmussen and Ghagramani. Most of the algorithm is complete, but I am not sure how to calculate a conditional probability necessary for the Gibbs sampling update of the indicator variables.</p>
</section>
<section id="theory-of-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="theory-of-algorithm">Theory of Algorithm</h2>
<p>For this model we have <span class="math inline">\(y\)</span>, a vector of <span class="math inline">\(n\)</span> outputs, and <span class="math inline">\(X\)</span>, an <span class="math inline">\(n\times d\)</span> matrix of inputs. The data will be partitioned into <span class="math inline">\(J\)</span> clusters and a GP expert fit to each cluster. Cluster assignments are represented by a vector of indicator variables <span class="math inline">\(z={z_i: i=1,\dots,n}\)</span>. These values are controlled by a Dirichlet process with concentration parameter <span class="math inline">\(\alpha\)</span> and a kernel function parameterized by a lengthscale vector <span class="math inline">\(\phi\)</span>. The parameters of each GP expert are represented by the vector <span class="math inline">\(\theta_j\)</span>, with <span class="math inline">\(\theta\)</span> representing all GP parameters.</p>
<p>To begin, <span class="math inline">\(y\)</span> and <span class="math inline">\(X\)</span> are given and we place priors on <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\phi\)</span>, and <span class="math inline">\(\theta\)</span> and choose an initial cluster assignment for <span class="math inline">\(z\)</span>. The authors suggest a vague inverse Gamma prior for <span class="math inline">\(\alpha\)</span> and vague independent log-normal priors for <span class="math inline">\(\phi\)</span>. Then, we run a Gibbs sampling sweep over each data point, updating their cluster assignments with Dirichlet process clustering. The conditional probability of point <span class="math inline">\(i\)</span> being assigned to a cluster <span class="math inline">\(j\)</span>, is expressed below. This probability is in two parts, the second of which is input dependent and can be modified to accommodate qualitative inputs without much trouble. The first part factors into the conditional probabilities of one output given all other outputs in the expert.</p>
<p><span class="math display">\[p(z_i=j|z_{-i},X,y,\theta,\phi,\alpha) \propto p(y|z_i=j,z_{-i},X,\theta)p(z_i=j|z_{-i}, x,\phi,\alpha)\]</span> <span class="math display">\[= p(y_i|y_{-i},x_j,\theta_j)p(z_i=j|z_{-i}, x,\phi,\alpha)\]</span> <span class="math display">\[\text{where } p(z_i=j|z_{-i},X,\phi,\alpha)=\frac{n-1}{n-1+\alpha}\frac{\sum_{i'\neq i,z_{i'}=j} K_{\phi}(X_i,X_{i'})}{\sum_{i'\neq i} K_{\phi}(X_i,X_{i'})}\]</span></p>
<p>Where <span class="math inline">\(K_{\phi}\)</span> is a Gaussian kernel function with lengthscales <span class="math inline">\(\phi\)</span>. I am not sure how best to sample the conditional probabilities above, given that they depend on the covariance matrix between all points except <span class="math inline">\(i\)</span> and given that the Gibbs sampling updates gradually change which points are assigned to which cluster. The authors remark that they can “reuse [covariance matrices] for consecutive Gibbs updates by performing rank one updates (since Gibbs sampling changes at most one indicator at a time),” but do not explain how to perform such updates.</p>
<p>Then, we fit a Gaussian process to each cluster. I have been using the package ‘laGP’ to fit a standard squared exponential model. However, ‘laGP’ doesn’t let me set priors for the GP parameters, so I may need to use a different package. I do not plan to include qualitative inputs in this step to save on computation time.</p>
<p>We then update the parameters of our gating function, the DP concentration parameter <span class="math inline">\(\alpha\)</span> and the gating kernel lengthscales <span class="math inline">\(\phi\)</span>. The posterior distribution of <span class="math inline">\(\alpha\)</span>, below, is sampled using Adaptive Rejection Sampling.</p>
<p><span class="math display">\[p(\alpha|n, J)\propto \alpha^{J-3/2}\exp(-1/2\alpha)\Gamma(\alpha)/\Gamma(n+\alpha)\]</span></p>
<p>For <span class="math inline">\(\phi\)</span>, we sample from the pseudo-posterior, which is the product of the conditional distributions of the indicator variables and the prior. We use vague independent log-normal priors for <span class="math inline">\(\phi\)</span>, with parameters <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma^2=1\)</span>.</p>
<p><span class="math display">\[p^*(\phi|\alpha, z, x) = \left[\prod_{i=1}^n p(z_i=j|z_{-i},\phi,\alpha)\right] p(\phi)\]</span></p>
<p>This completes one iteration of the iMGPE algorithm. We repeat until all parameters have converged.</p>
</section>
<section id="future-work" class="level2">
<h2 class="anchored" data-anchor-id="future-work">Future Work</h2>
<p>As stated above, the algorithm is incomplete. Once I cross the last hurdle, I will be able to begin testing the accuracy and efficiency of this method on different datasets. I can experiment with different approaches to involving qualitative inputs in the gating function to see how they affect the speed and performance of the algorithm.</p>
<section id="february-13-2025" class="level3">
<h3 class="anchored" data-anchor-id="february-13-2025">February 13, 2025</h3>
</section>
</section>
<section id="summary-15" class="level2">
<h2 class="anchored" data-anchor-id="summary-15">Summary</h2>
<p>This week I researched improvements on the iMGPE method proposed by Rasmussen and Ghahramani. I found two papers that looked particularly useful, a 2005 paper by Meeds and Osindero and a 2010 paper by Sun and Xu, which extend the approach through generative modeling and variational inference respectively. More recent papers exist, but are largely focused on directions not relevant to us, such as multivariate responses and general non-stationary probabilistic regression.</p>
<p>I have not yet found code for the iMGPE method. In the meantime, I have begun replicating the algorithm from Rasmussen and Ghahramani’s original paper.</p>
</section>
<section id="summary-of-alternative-imgpe" class="level2">
<h2 class="anchored" data-anchor-id="summary-of-alternative-imgpe">Summary of Alternative iMGPE</h2>
<p>The 2005 paper <a href="https://www.researchgate.net/publication/221619000_An_Alternative_Infinite_Mixture_Of_Gaussian_Process_Experts">“An Alternative Infinite Mixture of Gaussian Process Experts”</a> by Meeds and Osindero presents an extension of the iMGPE approach using generative modeling. The generative approach to Mixture of Experts modeling assumes that the experts generate the inputs which generate the outputs, rather than conditioning the experts on the inputs. This technique can handle missing or incomplete data easily and allows for reverse-conditioning: assessing where in the input space a particular output is likely to have originated.</p>
<p>The generative model is most easily explained as a data generation algorithm comprised of a series of conditionals. To generate a set of <span class="math inline">\(N\)</span> data points, we would take the following steps.</p>
<ol type="1">
<li>Sample the Dirichlet process concentration parameter <span class="math inline">\(\alpha_0\)</span> from a prior.</li>
<li>Partition a set of <span class="math inline">\(N\)</span> objects using a Dirichlet process, denoting the partitions with the indicator variables <span class="math inline">\(\{z_i\}_{i=1}^N\)</span> taking values <span class="math inline">\(r=1,\dots,E\)</span>.</li>
<li>Sample the gate hyperparameters <span class="math inline">\(\phi\)</span> from their priors.</li>
<li>For each partition, sample the input space parameters <span class="math inline">\(\psi_r\)</span> conditioned on <span class="math inline">\(\phi\)</span>. These define the density in each input space.</li>
<li>Given the parameters for each group, sample the locations of the input points <span class="math inline">\(X_r=\{x_i:z_i=r\}\)</span>.</li>
<li>For each group, sample the hyperparameters <span class="math inline">\(\theta_r\)</span> of the GP associated with it.</li>
<li>Given <span class="math inline">\(X_r\)</span> and <span class="math inline">\(\theta_r\)</span> for each group, formulate the GP output covariance matrix and sample the set of output values.</li>
</ol>
<p>Inference for this model is accomplished through an MCMC algorithm to identify the expert partition and hyperparameters most likely to have generated the training inputs and output. Qualitative inputs could possibly be incorporated into the GP experts. This would require defining a generative multinomial (or similar) distribution for each qualitative input that allows correlation between them and the quantitative inputs, as well as priors for the hyperparameters of those distributions.</p>
</section>
<section id="summary-of-variational-inference" class="level2">
<h2 class="anchored" data-anchor-id="summary-of-variational-inference">Summary of Variational Inference</h2>
<p>The paper <a href="https://ieeexplore.ieee.org/document/5664792">“Variational Inference for Infinite Mixture of Gaussian Processes”</a> by Sun and Xu further develops the generative mixture of experts method by employing a form of variational inference called mean field approximation to evaluate the model parameters. This is a method of approximating a probability distribution that can serve as a faster alternative to MCMC approximation.</p>
<p>Take some data <span class="math inline">\(X\)</span> generated by some latent variables <span class="math inline">\(\Omega=(\alpha_0, \phi)\)</span> through a hierarchical model. Suppose we have a joint prior on <span class="math inline">\(\Omega\)</span>, <span class="math inline">\(P(\Omega)\)</span> and we want to approximate the posterior <span class="math inline">\(P(\Omega|X)\)</span>. Standard variational inference defines a distribution <span class="math inline">\(Q(\Omega)\)</span> over <span class="math inline">\(\Omega\)</span> to be of a certain family of distributions similar to the posterior distribution. The similarity between them is measured through a dissimilarity function <span class="math inline">\(d(P;Q)\)</span> and inference is performed by selecting <span class="math inline">\(Q(\Omega)\)</span> so as to minimize <span class="math inline">\(d(P;Q)\)</span>.</p>
<p>For mean field approximation, <span class="math inline">\(Q(\Omega)\)</span> is assumed to factorize over some partition of the unobserved variables <span class="math inline">\(Z_1,\dots,Z_M\)</span>. It can be shown that the best distribution <span class="math inline">\(q_m^*\)</span> for some factor <span class="math inline">\(q_m\)</span>, in terms of Kullback-Leibler divergence, is</p>
<p><span class="math display">\[q_m^*(\Omega_m|X)= \frac{e^{E_{q^*_{-m}}[\ln p(\Omega,X)]}}{\int e^{E_{q^*_{-m}}[\ln p(\Omega,X)]} d\Omega_m}\]</span></p>
<p>or equivalently,</p>
<p><span class="math display">\[\ln q_m^*(\Omega_m|X) = E_{q^*_{-m}}[\ln p(\Omega,X)] + \text{constant}\]</span></p>
<p>where <span class="math inline">\(E_{q^*_{-m}}[\ln p(\Omega,X)]\)</span> is the expectation of the logarithm of the joint distribution over all data and variables taken with respect to <span class="math inline">\(q^*\)</span> over all variables not in the partition. This expectation can usually be determined to be of a known type of distribution. The factors can then be iteratively updated, much like the E-M algorithm.</p>
<p>In Sun and Xu’s case, they formulate the joint distribution of the inputs and outputs and then perform variational inference on each hyperparameter in turn. Their method also differs from the Alternative iMGPE by learning a support set for each GP expert that serves as the training data for that expert. These support sets are conditioned on the inputs and the expert partitions but are not synonymous with them.</p>
</section>
<section id="viability-for-qualitative-inputs" class="level2">
<h2 class="anchored" data-anchor-id="viability-for-qualitative-inputs">Viability for Qualitative Inputs</h2>
<p>I am ambivalent towards the generative model proposed by Meeds and Osindero. It seems to be more popular than Rasmussen’s conditional model but I’m not sure its advantages are really useful in the case of WEPP emulation. Missing data is not a problem for us, nor is imputing inputs from outputs. Furthermore, it no longer makes sense to condition the Dirichlet process gating function on the inputs, meaning we can’t reduce the number of parameters by only incorporating the qualitative inputs into the gating function.</p>
<p>Variational inference could be a faster alternative to MCMC, though the variational distributions have to be derived analytically, which could be challenging or even intractable depending on our model. Though Sun and Xu used a generative model, it is not needed to apply variational inference.</p>
<section id="february-6-2025" class="level3">
<h3 class="anchored" data-anchor-id="february-6-2025">February 6, 2025</h3>
</section>
</section>
<section id="summary-16" class="level2">
<h2 class="anchored" data-anchor-id="summary-16">Summary</h2>
<p>This week I studied the Mixture of Experts approach to modeling large datasets and considered its compatibility with qualitative inputs. A 2012 paper by Seniha Yuksel, et al, explained the fundamentals of ME modeling and surveyed the available variations. This paper is available at <a href="https://ieeexplore.ieee.org/abstract/document/6215056">IEEE</a>.</p>
<p>I considered the approaches described by Yuksel and settled on one developed by Rasmussen and Ghahramani, called an Infinite Mixture of GP Experts (iMGPE), which uses the Dirichlet Process as a gating function to partition the dataset among a potentially infinite number of Gaussian process experts. Their paper is available for download at the <a href="https://papers.nips.cc/paper_files/paper/2001/hash/9afefc52942cb83c7c1f14b2139b09ba-Abstract.html">NIPS website</a>.</p>
<p>Lastly, I considered how these methods could be applied to my own research. Options for incorporating qualitative inputs in a Mixture of Experts are presented, with an eye to minimizing computational complexity.</p>
</section>
<section id="mixture-of-experts-overview" class="level2">
<h2 class="anchored" data-anchor-id="mixture-of-experts-overview">Mixture of Experts Overview</h2>
<p>The mixture of expert (ME) approach uses a set of expert models and a gating function to perform regression or classification on a dataset. The gating function makes a soft split of the input space, meaning that the partitioned regions may overlap. The experts are trained on the data, each one focusing on a partition. Parameter estimation for both the gating function and the experts is most often done through the Expectation-Maximization algorithm.</p>
<p>Assume going forward that we have data <span class="math inline">\(\mathbf{x}\)</span> and response <span class="math inline">\(\mathbf{y}\)</span>, that <span class="math inline">\(n=1,\dots ,N\)</span> is the number of data points, and <span class="math inline">\(i=1,\dots, I\)</span> is the number of experts. Let <span class="math inline">\(\theta=(\theta_g,\theta_e)\)</span> represent the parameters of the gating function and the experts. In this case, the probability of observing <span class="math inline">\(\mathbf{y}\)</span> given <span class="math inline">\(\mathbf{x}\)</span> is</p>
<p><span class="math display">\[P(\mathbf{y}|\mathbf{x},\theta) = \sum_{i=1}^I g_i(\mathbf{x},\theta_g)P(\mathbf{y}|i, \mathbf{x},\theta_e)\]</span></p>
<p>For regression, the gate <span class="math inline">\(g_i(\cdot)\)</span> is generally defined as the softmax function.</p>
<p><span class="math display">\[g_i(\mathbf{x,v}) = \frac{\exp(\beta_i(\mathbf{x,v}))}{\sum_{j=1}^I \exp(\beta_j(\mathbf{x,v}))}\]</span> where <span class="math inline">\(\mathbf{v}\)</span> is the gate parameter and the functions of the gate parameters are linear: <span class="math inline">\(\beta_i(\mathbf{x,v})=\mathbf{v_i^T}[\mathbf{x},1]\)</span>. By introducing indicator variables <span class="math inline">\(Z=\{\{z_i^{(n)}\}_{n=1}^N\}_{i=1}^I\)</span> representing the expert assignment for each observation, we can write the full log likelihood of the data and solve it.</p>
<p><span class="math display">\[l(\mathbf{x},\mathbf{y},Z,\theta) = \sum_{n=1}^n \sum_{i=1}^I z_i^{(n)}\left[ \log g_i(\mathbf{x^{(n)}}, \theta_g) + \log P_i(\mathbf{y^{(n)}}) \right]\]</span></p>
<p>Note that in the original ME formulation the parameters for the experts and the gating function are learned simultaneously and each expert is trained on all data, not just the data “assigned” to it.</p>
</section>
<section id="infinite-mixture-of-gp-experts" class="level2">
<h2 class="anchored" data-anchor-id="infinite-mixture-of-gp-experts">Infinite Mixture of GP Experts</h2>
<p>The 2002 paper “Infinite Mixtures of Gaussian Process Experts” by Rasmussen and Ghahramani describes a modification of the ME approach that uses a Dirichlet process for the gating function and a potentially infinite number of Gaussian processes as experts, which they call iMGPE. The Dirichlet process works similarly to the clustering algorithm described on November 5, 2024, but is made input dependent through a squared exponential kernel parameterized by a lengthscale <span class="math inline">\(\phi\)</span>, rather than by a Gaussian density.</p>
<p>It places Bayesian priors on the model parameters: an inverse gamma prior for the DP concentration parameter <span class="math inline">\(\alpha\)</span>, inverse gamma priors on the spatial variance and error variance of each expert, with common hyperparameters <span class="math inline">\((a_1,b_1)\)</span> and <span class="math inline">\((a_2,b_2)\)</span>, and independent log normal priors for the lengthscale parameters of each expert and the gating kernel parameter <span class="math inline">\(\phi\)</span>.</p>
<p>This approach then iterates through the following MCMC algorithm.</p>
<ol type="1">
<li>Initialize indicator variables <span class="math inline">\(z_i^{(n)}\)</span> to a single value or set of values.</li>
<li>Update each of the indicators with a Gibbs sampling sweep.</li>
<li>Do Monte Carlo estimation of the parameters of each GP expert in turn.</li>
<li>Optimize the hyperparameters <span class="math inline">\((a_1,b_1)\)</span> and <span class="math inline">\((a_2,b_2)\)</span> of the GP variances.</li>
<li>Sample the Dirichlet process concentration parameter, <span class="math inline">\(\alpha\)</span>, using adaptive rejection sampling.</li>
<li>Sample the gating parameter <span class="math inline">\(\phi\)</span>.</li>
<li>Repeat from step 2 until the MCMC output has converged.</li>
</ol>
<p>This method has several advantages over the standard ME approach. It allows the Dirichlet process to determine the appropriate number of experts to represent the data rather than specifying a certain number of experts beforehand. It also fits each GP expert only to a subset of the data, speeding up computation significantly.</p>
</section>
<section id="application-to-qualitative-inputs" class="level2">
<h2 class="anchored" data-anchor-id="application-to-qualitative-inputs">Application to Qualitative Inputs</h2>
<p>My current objective is to incorporate qualitative inputs into Gaussian process regression in the context of large data sets. The mixture of experts approach, particularly the variation described by Rasmussen, is a promising avenue of research. The primary tension between qualitative inputs and big data is that every qualitative input requires many more parameters be estimated to capture the relationship between levels, which complicates and slows down model fitting. The iMGPE reduces computation time while having plenty of room for qualitative inputs.</p>
<p>As a mixture of experts model does not much care what its component experts are, the most straightforward approach to incorporating qualitative inputs is to incorporate them into the GP experts through any previously described approach (EC, MC, LV). The viability of this option would only depend on the complexity of the chosen approach and the minimum number of experts you permit.</p>
<p>A second option would be to utilize qualitative inputs in the gating function, the Dirichlet process, but not in the experts themselves. This would save significantly on computation as their parameters would only be estimated once per iteration instead of for each expert. Such an approach would be similar to the Naive Local Expert models I have already presented, but with two distinctions: the data partitions are learned rather than decided a priori and use all inputs, not just the qualitative ones. The downside is that we cannot easily interpret the relationship between qualitative inputs and the response.</p>
<section id="january-30-2025" class="level3">
<h3 class="anchored" data-anchor-id="january-30-2025">January 30, 2025</h3>
</section>
</section>
<section id="summary-17" class="level2">
<h2 class="anchored" data-anchor-id="summary-17">Summary</h2>
<p>This week I studied the latent variable approach to Gaussian process regression that was discussed on November 19. I downloaded the author’s code and ran it in MATLAB, replicating an example from the paper.</p>
<p>I also considered how we might approach the task of GP regression with qualitative variables in situations with massive data or many inputs. I compared the known methods by the number of parameters needed and the flexibility of their correlation structures. I then discussed several ways to build upon the existing research.</p>
</section>
<section id="latent-variable-method-example" class="level2">
<h2 class="anchored" data-anchor-id="latent-variable-method-example">Latent Variable Method Example</h2>
<p>I return to the paper “A Latent Variable Approach to Gaussian Process Modeling with Qualitative and Quantitative Factors” by Zhang et al, available at <a href="https://www.tandfonline.com/doi/10.1080/00401706.2019.1638834" class="uri">https://www.tandfonline.com/doi/10.1080/00401706.2019.1638834</a></p>
<p>I replicated one example from this paper related to beam bending: a metal beam with one of six different cross-sectional shapes is fixed at one end to a wall and downward pressure of <span class="math inline">\(600\)</span> N is applied to the other end. The response is the amount of deformation in the beam. The sixth shape is known to behave significantly differently from the others, but this is not included in the model.</p>
<p>I downloaded the author’s code and data and ran the example in MATLAB. It used a dataset with <span class="math inline">\(60\)</span> observations of two quantitative variables and one qualitative variable with six levels. The RMSE was <span class="math inline">\(8.46e-7\)</span> and the relative RMSE was <span class="math inline">\(0.0623\)</span>. The factor level latent variables were fit to 2D space as in the following plot:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot1-1-30-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">level</th>
<th style="text-align: center;">coords</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">(0,0)</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">(-0.0537,0)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;">(0.0121,-0.0001)</td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: center;">(-0.0290,0.0001)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5</td>
<td style="text-align: center;">(0.0403,-0.0007)</td>
</tr>
<tr class="even">
<td style="text-align: center;">6</td>
<td style="text-align: center;">(0.7025,-0.1555)</td>
</tr>
</tbody>
</table>
<p>Note that the sixth factor level is far from the others, indicating that our model has successfully captured the underlying physical structure.</p>
</section>
<section id="qualitative-gp-with-big-data" class="level2">
<h2 class="anchored" data-anchor-id="qualitative-gp-with-big-data">Qualitative GP with Big Data</h2>
<p>Most methods of fitting a GP with qualitative inputs are not designed for situations with large <span class="math inline">\(n\)</span>. While adding one more quantitative variable to a model rarely means more than one additional parameter to estimate, as with a separated squared exponential kernel function, adding one qualitative variable can easily add half a dozen or more parameters, depending on the number of levels it possesses. The exact number of parameters used varies significantly depending on the method of estimation chosen. Here, I compare some of the methods discussed on October 27 to the latent variable method.</p>
<p>The exchangeable covariance (EC) method uses just one parameter per factor variable. The unrestricted covariance (UC) method uses <span class="math inline">\(\sum_{j=1}^r m_j(m_j-1)/2\)</span> parameters, assuming <span class="math inline">\(r\)</span> factors and <span class="math inline">\(m_j\)</span> levels in the <span class="math inline">\(r^{th}\)</span> factor. The multiplicative covariance (MC) method uses <span class="math inline">\(\sum_{j=1}^r m_j\)</span> parameters. The latent variable (LV) method uses <span class="math inline">\(\sum_{j=1}^r 2m_j-3\)</span> parameters. Thus, EC is best in terms of the number of parameters needed, followed by MC, but both are unable to model the full range of possible relationships between levels. The LV method, meanwhile, is superior to the UC method when at least one factor has four or more levels.</p>
<p>In an ideal situation we would be able to do the following:</p>
<p>One research direction available to us is to develop a statistical test to determine whether a qualitative variable can be safely excluded from a regression model. Similarly, we could develop a test to determine whether the correlation structure of a factor is significantly different from the exchangeable correlation structure.</p>
<p>Another is to investigate how any of the factor modeling methods described could be integrated into known big data techniques. We have attempted one form of integration with local expert models by stratifying by factor levels, but there may be better ways of combining these methods.</p>
<section id="january-23-2025" class="level3">
<h3 class="anchored" data-anchor-id="january-23-2025">January 23, 2025</h3>
</section>
</section>
<section id="summary-18" class="level2">
<h2 class="anchored" data-anchor-id="summary-18">Summary</h2>
<p>This week, I developed a tentative academic plan, which is attached separately. I also experimented with local expert methods on a simple dataset with no outliers. I compared using random forest models for each factor level to Gaussian process models and found that they performed equally well on a well-behaved dataset.</p>
</section>
<section id="gp-vs-rf-on-simple-data" class="level2">
<h2 class="anchored" data-anchor-id="gp-vs-rf-on-simple-data">GP vs RF on Simple Data</h2>
<p>I simulated a dataset consisting of one continuous variable x1 and one categorical variable x2. The categorical variable had three levels: <span class="math inline">\({1,2,3}\)</span>. The response was generated using a function proposed by Han, et al, where a different quadratic function of the continuous variable is used for each level of the categorical variable. I generated a training dataset with <span class="math inline">\(900\)</span> observations, and two test datasets, each with 93. The x1 values in the training set and one of the test sets were generated from a <span class="math inline">\(N(0,1)\)</span> distribution, while those in the second test set were evenly spaced between <span class="math inline">\(-3\)</span> and <span class="math inline">\(3\)</span>. The training data is plotted below.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot1-1-23-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>I then stratified the training data by factor and compared fitting a GP to each group with fitting a random forest model to each. I compared the predictive accuracy of these models on both test sets, starting with the random set. For the random forest models, the RMSE was 0.68 and the MAE was 0.55. For the Gaussian process models, the RMSE was 0.30 and the MAE was 0.18. Comparing on the regularly spaced set, the RF models had an RMSE of 4.66 and an MAE of 2.35 and the GP models had an RMSE of 8.83 and an MAE of 4.38.</p>
<p>The x1 values of the random test set are clustered around zero, where most of the training data is, while those of the regular test set extend into regions with little training data. GPs seem to outperform RF models on predicting data points similar to the training data, but this relationship is reversed for data points outside the training data.</p>
<section id="january-17-2025" class="level3">
<h3 class="anchored" data-anchor-id="january-17-2025">January 17, 2025</h3>
</section>
</section>
<section id="summary-19" class="level2">
<h2 class="anchored" data-anchor-id="summary-19">Summary</h2>
<p>This week I continued my comparison of local expert Gaussian process models to random forest models. The random forest models previously outperformed the LE GP models, which we hypothesized to be because the RF models were fit to the whole training data set while the GP models were each fit to a single strata. However, when I stratified the data by qualitative variables and fit one RF model to each strata, the accuracy on the test data did not significantly change.</p>
<p>Comparing the predicted by actual plots for each method showed that the difference is mostly in how they respond to large (&gt;100) soil loss values, which make up about <span class="math inline">\(2\%\)</span> of observations. The RF method predicts all values as small while the GP method predicts all large values as small and some small values as large, resulting in a higher RMSE. There may simply not be enough large values in our dataset to accurately model them with a Gaussian process. Data augmentation to increase the number of large soil loss values in the training data may help.</p>
<p>I also studied the factor level combination methods proposed by Cruz-Reyes and Pauger and Wagner, focusing on their design of prior correlation structures for factor effects and their relative advantages. Lastly, I considered how these methods could be adapted for use in our own WEPP research.</p>
</section>
<section id="stratified-random-forest-models" class="level2">
<h2 class="anchored" data-anchor-id="stratified-random-forest-models">Stratified Random Forest Models</h2>
<p>I took the positive soil loss dataset and tested the performance of random forest models when using a local expert technique. I used the soil loss data from the years 2009 and 2010 as the training set and the data from 2011 as the test set. First, I took the training set and stratified it by ‘crop’. Then I fit a random forest model to each strata, predicting soil loss with the variables ‘rad’, ‘tmin’, ‘precip’, ‘max_prcp’, and ‘slp_wavg4’. Then I predicted soil loss on the test set and recorded the RMSE and mean absolute error of the predictions. I repeated this, stratifying on ‘till’ and then on both ‘crop’ and ‘till’.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Model(Factor)</th>
<th style="text-align: center;">RMSE</th>
<th style="text-align: center;">MAE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">RF(crop)</td>
<td style="text-align: center;">17.82</td>
<td style="text-align: center;">3.75</td>
</tr>
<tr class="even">
<td style="text-align: center;">RF(till)</td>
<td style="text-align: center;">17.47</td>
<td style="text-align: center;">3.66</td>
</tr>
<tr class="odd">
<td style="text-align: center;">RF(both)</td>
<td style="text-align: center;">17.57</td>
<td style="text-align: center;">3.75</td>
</tr>
</tbody>
</table>
<p>The stratified random forest models did not perform significantly differently from the RF models that included crop or till. The performance gap between the local expert GP models and the RF models is not due to this stratification. For stratification by ‘crop’, plots of the predicted by actual response are shown below.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot1-1-17-25.png" style="width:50.0%;height:50.0%" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot2-1-17-25.png" style="width:50.0%;height:50.0%" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The local expert random forest models have a much narrower range in predicted values than the local expert GPs.</p>
</section>
<section id="model-structures-for-factor-fusion" class="level2">
<h2 class="anchored" data-anchor-id="model-structures-for-factor-fusion">Model Structures for Factor Fusion</h2>
<p>Here, I describe the model and prior correlation structure presented by Pauger and Wagner in their 2017 paper. We have a linear model based on the dummy variable expression of a categorical covariate with <span class="math inline">\(c+1\)</span> levels. Let <span class="math inline">\(B_0(\delta,\tau^2)\)</span> be the prior correlation matrix of the regression coefficients <span class="math inline">\(\beta_1,\dots ,\beta_c\)</span> with respect to a baseline category. That is, <span class="math inline">\(\mathbf{\beta}\sim N(0,B_0(\delta, \tau^2))\)</span>. It depends on a scale parameter <span class="math inline">\(\tau^2\)</span> and a vector of binary indices <span class="math inline">\(\delta\)</span> with one element for each pair of levels that may be fused. <span class="math display">\[B_0(\delta,\tau^2)=\gamma \tau^2 Q^{-1}(\delta)= \gamma\tau^2\left(\begin{array}{cccc}
\sum_{j\neq 1} \kappa_{1j} &amp; -\kappa_{12} &amp; \dots &amp; -\kappa_{1c}\\
-\kappa_{21} &amp; \sum_{j\neq 2} \kappa_{2j} &amp; \dots &amp; -\kappa_{2c}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
-\kappa_{c1} &amp; -\kappa_{c2} &amp; \dots &amp; \sum_{j\neq c} \kappa_{cj}
\end{array}\right)\]</span></p>
<p>Here, <span class="math inline">\(\gamma=c/2\)</span> is a fixed constant. The <span class="math inline">\(\kappa_{kj}\)</span> (for <span class="math inline">\(k&gt;j\)</span>) are defined as <span class="math inline">\(\delta_{kj}+r(1-\delta_{kj})\)</span> where <span class="math inline">\(r\)</span> is a fixed large number called the precision ratio and <span class="math inline">\(\delta_{kj}\)</span> is the indicator variable for whether levels <span class="math inline">\(k\)</span> and <span class="math inline">\(j\)</span> are separate. Then <span class="math inline">\(\kappa_{jk}=\kappa_{kj}\)</span>. Note that this means two levels of a factor will be highly correlated if <span class="math inline">\(\delta_{kj}=0\)</span> and weakly correlated if <span class="math inline">\(\delta_{kj}=1\)</span>.</p>
<p>The precision ratio is so called because it is the ratio of the maximum prior precision of <span class="math inline">\(\beta_k\)</span> to the minimum prior precision. Lastly, hyperpriors are assigned to <span class="math inline">\(\tau^2\)</span> and <span class="math inline">\(\delta\)</span>, an inverse Gamma prior on <span class="math inline">\(\tau^2\)</span> and iid Bernoulli priors on the elements of <span class="math inline">\(\delta\)</span>. After fitting the model, the elements of <span class="math inline">\(\delta\)</span> tell you which pairs of factor levels were fused.</p>
<p>Cruz-Reyes, in her 2023 paper, starts with the same model structure but models <span class="math inline">\(\mathbf{\beta}\sim N(\mathbf{0},\sigma^2Q^{-1})\)</span> where <span class="math inline">\(\sigma^2\)</span> is a scale parameter and the <span class="math inline">\(c\times c\)</span> precision matrix <span class="math inline">\(Q\)</span> is defined according to a parameter vector <span class="math inline">\(\rho\)</span>. The vector <span class="math inline">\(\rho\)</span> has <span class="math inline">\(c+\frac{c(c-1)}{2}\)</span> elements, one for each factor level and one for each pair of factor levels (though some of the latter elements may be set to zero). Then <span class="math inline">\(Q\)</span> is as follows.</p>
<p><span class="math display">\[Q(\rho) = \left(\begin{array}{ccc}
1+\rho_1+\sum_{j\neq 1} |\rho_{1j}| &amp; \dots &amp; -\rho_{1c}\\
-\rho_{21} &amp; \dots &amp; -\rho_{2c}\\
\vdots &amp; \ddots &amp; \vdots\\
-\rho_{c1} &amp; \dots &amp; 1+\rho_{c}+\sum_{j\neq c} |\rho_{cj}|
\end{array}\right)\]</span></p>
<p>In <span class="math inline">\(Q\)</span>, the <span class="math inline">\(\rho_i&gt;0\)</span> for <span class="math inline">\(i=1,\dots, c\)</span> and <span class="math inline">\(\rho_{ij}=\rho_{ji}\)</span>. Lastly, Cruz-Reyes defines a custom prior for <span class="math inline">\(\rho\)</span> that accounts for these conditions. Both approaches combine variable selection with level fusion, as the merge of all levels of a categorical variable implies it has no effect on the response. Cruz-Reyes’s prior structure is arranged to allow for correlation between the levels of the variable, such as when represent distinct, possibly neighboring, geographic areas.</p>
</section>
<section id="further-research-options" class="level2">
<h2 class="anchored" data-anchor-id="further-research-options">Further Research Options</h2>
<p>The most obvious application to our soil erosion research is to use priors like the ones presented for the correlation matrix of a qualitative input in a Gaussian process model. Interpretation would be the same: a correlation of one between two levels of a factor would indicate that those levels can be clustered together. The optimal form of that prior would depend on the nature of the variable being modeled. The Cruz-Reyes prior permits spacial correlation between levels but requires more parameters than the Pauger-Wagner prior.</p>
<p>A further avenue of research is how best to adapt these methods to situations with many qualitative variables, variables of many levels, or large <span class="math inline">\(n\)</span>. The existing methods depend on large numbers of parameters to model all the possible interactions. The question of how the model could be simplified or how the model fitting process could be sped up are worth exploring.</p>
<section id="december-19-2024" class="level3">
<h3 class="anchored" data-anchor-id="december-19-2024">December 19, 2024</h3>
</section>
</section>
<section id="summary-20" class="level2">
<h2 class="anchored" data-anchor-id="summary-20">Summary</h2>
<p>This week I fit a set of random forest models on the positive soil loss dataset, as a comparison with the local expert GP models I had fit previously. The local expert GPs underperformed the random forest models except for the GP stratified on ‘crop’.</p>
<p>I also considered several approaches to identifying and consolidating significantly similar levels of categorical variables. A paper by Danna Cruz-Reyes suggested setting a Bayesian prior on the correlation matrix of the factor levels, acting as a penalty term to induce sparsity. It is designed for linear regression but may provide inspiration for Gaussian processes.</p>
<p>The paper by Cruz-Reyes is available at <a href="https://link.springer.com/chapter/10.1007/978-3-031-48415-5_11." class="uri">https://link.springer.com/chapter/10.1007/978-3-031-48415-5_11.</a> It draws heavily on a paper by Daniela Pauger and Helga Wagner, available at <a href="https://projecteuclid.org/journals/bayesian-analysis/volume-14/issue-2/Bayesian-Effect-Fusion-for-Categorical-Predictors/10.1214/18-BA1096.full." class="uri">https://projecteuclid.org/journals/bayesian-analysis/volume-14/issue-2/Bayesian-Effect-Fusion-for-Categorical-Predictors/10.1214/18-BA1096.full.</a></p>
</section>
<section id="random-forest-model-comparisons" class="level2">
<h2 class="anchored" data-anchor-id="random-forest-model-comparisons">Random Forest Model Comparisons</h2>
<p>I have previously fit a number of GP models to the positive soil loss data set, using the data from 2009 and 2010 as a training set and the data from 2011 as a test set. I now fit three random forest models to the same training set, using the same numeric variables as well as ‘crop’, ‘till’, or both. I evaluate these models on the 2011 test set and display the results in the table below.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Model</th>
<th style="text-align: center;">RMSE</th>
<th style="text-align: center;">MAE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">RF(crop)</td>
<td style="text-align: center;">17.79</td>
<td style="text-align: center;">3.73</td>
</tr>
<tr class="even">
<td style="text-align: center;">RF(till)</td>
<td style="text-align: center;">17.56</td>
<td style="text-align: center;">3.61</td>
</tr>
<tr class="odd">
<td style="text-align: center;">RF(both)</td>
<td style="text-align: center;">17.52</td>
<td style="text-align: center;">3.63</td>
</tr>
<tr class="even">
<td style="text-align: center;">GP(crop)</td>
<td style="text-align: center;">18.60</td>
<td style="text-align: center;">4.26</td>
</tr>
<tr class="odd">
<td style="text-align: center;">GP(till)</td>
<td style="text-align: center;">26.44</td>
<td style="text-align: center;">6.60</td>
</tr>
<tr class="even">
<td style="text-align: center;">GP(both)</td>
<td style="text-align: center;">21.73</td>
<td style="text-align: center;">5.70</td>
</tr>
</tbody>
</table>
<p>The GP models stratified on ‘till’ and ‘crop’+‘till’ significantly underperform their random forest equivalents, while the GP model stratified on ‘crop’ performs close to the random forest including crop.</p>
</section>
<section id="combining-factor-levels" class="level2">
<h2 class="anchored" data-anchor-id="combining-factor-levels">Combining Factor Levels</h2>
<p>Similar levels of a categorical variable may be identified at three stages: before, during, or after a model has been fit. Before, the distribution of response values within each category could be compared for statistical similarity. This could be based on mean values or other summaries, or test statistics such as KL divergence. It would be necessary to control for the other covariates for such statistics to have meaning.</p>
<p>During, or as a part of, model fit, the most obvious solution is to convert factor inputs into dummy variables and apply any existing variable selection technique to the dummy variables. The downside of this approach is that dropping a dummy variable ‘combines’ that level with the reference level but cannot combine it with any other level. A better approach is presented by Cruz-Reyes in the section below, based on defining a multivariate normal prior for the effects of the factor levels. Numeric covariates could possibly be included in this process by modeling the covariances between factor levels as a function of the numeric variables.</p>
<p>After fitting a model, there are a few possible ways of evaluating level similarity depending on the type of model that was fit. If fitting a local expert GP model, the levels could be compared on their lengthscale parameter values or on their predictive accuracy on other strata. If a GP trained on one level can accurately predict data from another level, the levels may not be significantly different. As another option, since GP regression has a multivariate normal posterior distribution, we could calculate confidence intervals for the expected value of the response conditional on each factor level and see if they overlap.</p>
</section>
<section id="spacial-shrinkage-prior-by-cruz-reyes-et-al." class="level2">
<h2 class="anchored" data-anchor-id="spacial-shrinkage-prior-by-cruz-reyes-et-al.">Spacial Shrinkage Prior by Cruz-Reyes et al.</h2>
<p>The paper “Spacial Shrinkage Prior: A Probabilistic Approach to Models for Categorical Variables with Many Levels” by Danna Cruz-Reyes describes a Bayesian method for combining levels of categorical variables within a linear regression model. Her technique is to encode the categorical variable with a parameter <span class="math inline">\(\mathbf{\theta}=(\theta_1, \dots,\theta_r)\)</span> representing the effects of its <span class="math inline">\(r\)</span> levels.</p>
<p>She imagines the levels as nodes on a map with edges connecting each pair. She then puts a multivariate normal prior on <span class="math inline">\(\mathbf{\theta}\)</span> with a covariance matrix depending on the random weights of the edges connecting nodes. Finally, she puts a hyperprior on the random edge weights that allows the weights to go to zero. If, upon fitting the model, they do, then the two categorical levels connected by that edge have been combined.</p>
<section id="december-10-2024" class="level3">
<h3 class="anchored" data-anchor-id="december-10-2024">December 10, 2024</h3>
</section>
</section>
<section id="summary-21" class="level2">
<h2 class="anchored" data-anchor-id="summary-21">Summary</h2>
<p>This week I performed a deeper exploratory analysis of the variables crop and till together. Notably, some combinations of crop and till did not occur in the stratified data set at all. Corn, for example, was never associated with zero tilling operations while tre was only associated with zero tilling operations. These imbalances may have significant effects on the performance of local expert methods.</p>
<p>I also fit a local expert GP model by crop and till together. It outperformed the local expert model stratified on till but not the model stratified on crop. As with previous LE models, there were a small number of categories that contributed the majority of error.</p>
</section>
<section id="crop-by-till" class="level2">
<h2 class="anchored" data-anchor-id="crop-by-till">Crop by Till</h2>
<p>The number of hillslopes with a particular combination of crop and tilling operation before 2012 is shown in the table below.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">season</th>
<th style="text-align: center;">crop\till</th>
<th style="text-align: center;">0</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">winter</td>
<td style="text-align: center;">alfalfa</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">winter</td>
<td style="text-align: center;">bromegr</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: center;">winter</td>
<td style="text-align: center;">none</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">258</td>
<td style="text-align: center;">5</td>
</tr>
<tr class="even">
<td style="text-align: center;">spring</td>
<td style="text-align: center;">alfalfa</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: center;">spring</td>
<td style="text-align: center;">corn</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">283</td>
<td style="text-align: center;">6</td>
</tr>
<tr class="even">
<td style="text-align: center;">spring</td>
<td style="text-align: center;">tre</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>The distribution of spring after 2012 is shown below. (The distribution for winter is unchanged.)</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">season</th>
<th style="text-align: center;">crop\till</th>
<th style="text-align: center;">0</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">spring</td>
<td style="text-align: center;">alfalfa</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">spring</td>
<td style="text-align: center;">corn</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">281</td>
<td style="text-align: center;">6</td>
</tr>
<tr class="odd">
<td style="text-align: center;">spring</td>
<td style="text-align: center;">soy</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">spring</td>
<td style="text-align: center;">tre</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>For each combination of crop and till, I recorded summary statistics of soil loss: the mean, median, and first and third quartiles. The results are shown below, from largest to smallest mean soil loss.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">crop</th>
<th style="text-align: center;">till</th>
<th style="text-align: center;">Q1</th>
<th style="text-align: center;">median</th>
<th style="text-align: center;">mean</th>
<th style="text-align: center;">Q3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">corn</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">1.30</td>
<td style="text-align: center;">15.44</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr class="even">
<td style="text-align: center;">soy</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">1.10</td>
<td style="text-align: center;">13.07</td>
<td style="text-align: center;">7.60</td>
</tr>
<tr class="odd">
<td style="text-align: center;">alfalfa</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">12.04</td>
<td style="text-align: center;">1.53</td>
</tr>
<tr class="even">
<td style="text-align: center;">corn</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">6.99</td>
<td style="text-align: center;">1.80</td>
</tr>
<tr class="odd">
<td style="text-align: center;">soy</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">6.54</td>
<td style="text-align: center;">1.90</td>
</tr>
<tr class="even">
<td style="text-align: center;">alfalfa</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">5.29</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr class="odd">
<td style="text-align: center;">none</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">3.41</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr class="even">
<td style="text-align: center;">none</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">3.34</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr class="odd">
<td style="text-align: center;">corn</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">3.10</td>
<td style="text-align: center;">1.50</td>
</tr>
<tr class="even">
<td style="text-align: center;">none</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">2.44</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr class="odd">
<td style="text-align: center;">bromegr</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">1.62</td>
<td style="text-align: center;">0.60</td>
</tr>
<tr class="even">
<td style="text-align: center;">bromegr</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">1.43</td>
<td style="text-align: center;">0.43</td>
</tr>
<tr class="odd">
<td style="text-align: center;">tre</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">1.36</td>
<td style="text-align: center;">1.45</td>
</tr>
<tr class="even">
<td style="text-align: center;">soy</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">1.36</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr class="odd">
<td style="text-align: center;">corn</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">1.24</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr class="even">
<td style="text-align: center;">alfalfa</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">1.22</td>
<td style="text-align: center;">1.20</td>
</tr>
<tr class="odd">
<td style="text-align: center;">bromegr</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">1.10</td>
</tr>
<tr class="even">
<td style="text-align: center;">alfalfa</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.43</td>
</tr>
<tr class="odd">
<td style="text-align: center;">none</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.60</td>
</tr>
<tr class="even">
<td style="text-align: center;">alfalfa</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.60</td>
</tr>
<tr class="odd">
<td style="text-align: center;">bromegr</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.50</td>
</tr>
</tbody>
</table>
</section>
<section id="nle-by-crop-and-till" class="level2">
<h2 class="anchored" data-anchor-id="nle-by-crop-and-till">NLE by Crop and Till</h2>
<p>From my stratified data sample, I extracted only the dates between 2009 and 2010 which had nonzero soil loss, a total of 5938 observations. I combined crop and till into a single variable. There were 18 combinations present in the data set but three of them had too few observations in them to fit a model. After removing these, there were 5923 observations in the training data set. I then fit a local expert GP model by crop and till using the variables ‘rad’, ‘tmax’, ‘precip’, ‘max_prcp’, and ‘slp_wavg4’. The ML estimates of the component models are shown below.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 11%">
<col style="width: 14%">
<col style="width: 15%">
<col style="width: 11%">
<col style="width: 11%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">group</th>
<th style="text-align: center;">rad</th>
<th style="text-align: center;">tmax</th>
<th style="text-align: center;">precip</th>
<th style="text-align: center;">max_prcp</th>
<th style="text-align: center;">slp_wavg4</th>
<th style="text-align: center;">nugget</th>
<th style="text-align: center;">sp.var</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">alfalfa 0</td>
<td style="text-align: center;">23378</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">3.07</td>
<td style="text-align: center;">5e4</td>
<td style="text-align: center;">3.52</td>
<td style="text-align: center;">1.014</td>
<td style="text-align: center;">1.494</td>
</tr>
<tr class="even">
<td style="text-align: center;">alfalfa 1</td>
<td style="text-align: center;">1.88</td>
<td style="text-align: center;">636.2</td>
<td style="text-align: center;">3065</td>
<td style="text-align: center;">5854</td>
<td style="text-align: center;">0.634</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">1.371</td>
</tr>
<tr class="odd">
<td style="text-align: center;">alfalfa 4</td>
<td style="text-align: center;">5e4</td>
<td style="text-align: center;">42585</td>
<td style="text-align: center;">47815</td>
<td style="text-align: center;">41995</td>
<td style="text-align: center;">41908</td>
<td style="text-align: center;">0.712</td>
<td style="text-align: center;">0.124</td>
</tr>
<tr class="even">
<td style="text-align: center;">alfalfa 5</td>
<td style="text-align: center;">34596</td>
<td style="text-align: center;">41995</td>
<td style="text-align: center;">5e4</td>
<td style="text-align: center;">34577</td>
<td style="text-align: center;">1.470</td>
<td style="text-align: center;">9.254</td>
<td style="text-align: center;">0.003</td>
</tr>
<tr class="odd">
<td style="text-align: center;">bromegr 0</td>
<td style="text-align: center;">2.45</td>
<td style="text-align: center;">4.52</td>
<td style="text-align: center;">29445</td>
<td style="text-align: center;">5e4</td>
<td style="text-align: center;">1.991</td>
<td style="text-align: center;">8.204</td>
<td style="text-align: center;">0.011</td>
</tr>
<tr class="even">
<td style="text-align: center;">bromegr 1</td>
<td style="text-align: center;">2.07</td>
<td style="text-align: center;">7.48</td>
<td style="text-align: center;">12.17</td>
<td style="text-align: center;">23.51</td>
<td style="text-align: center;">10.15</td>
<td style="text-align: center;">1e-8</td>
<td style="text-align: center;">0.954</td>
</tr>
<tr class="odd">
<td style="text-align: center;">bromegr 5</td>
<td style="text-align: center;">39731</td>
<td style="text-align: center;">5e4</td>
<td style="text-align: center;">44022</td>
<td style="text-align: center;">38292</td>
<td style="text-align: center;">1.377</td>
<td style="text-align: center;">4.407</td>
<td style="text-align: center;">0.249</td>
</tr>
<tr class="even">
<td style="text-align: center;">corn 1</td>
<td style="text-align: center;">66.15</td>
<td style="text-align: center;">50.62</td>
<td style="text-align: center;">4.32</td>
<td style="text-align: center;">50.25</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.122</td>
<td style="text-align: center;">24.91</td>
</tr>
<tr class="odd">
<td style="text-align: center;">corn 4</td>
<td style="text-align: center;">6.80</td>
<td style="text-align: center;">3394</td>
<td style="text-align: center;">4.12</td>
<td style="text-align: center;">3058</td>
<td style="text-align: center;">3.702</td>
<td style="text-align: center;">0.027</td>
<td style="text-align: center;">293.7</td>
</tr>
<tr class="even">
<td style="text-align: center;">corn 5</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">7.40</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">6.59</td>
<td style="text-align: center;">0.020</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">6283</td>
</tr>
<tr class="odd">
<td style="text-align: center;">corn 6</td>
<td style="text-align: center;">5e4</td>
<td style="text-align: center;">32206</td>
<td style="text-align: center;">18.02</td>
<td style="text-align: center;">5.25</td>
<td style="text-align: center;">1.304</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">411.1</td>
</tr>
<tr class="even">
<td style="text-align: center;">none 1</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">18351</td>
<td style="text-align: center;">2.29</td>
<td style="text-align: center;">5e4</td>
<td style="text-align: center;">0.130</td>
<td style="text-align: center;">0.323</td>
<td style="text-align: center;">0.799</td>
</tr>
<tr class="odd">
<td style="text-align: center;">none 5</td>
<td style="text-align: center;">17213</td>
<td style="text-align: center;">10259</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">0.035</td>
<td style="text-align: center;">39.80</td>
</tr>
<tr class="even">
<td style="text-align: center;">none 6</td>
<td style="text-align: center;">2.64</td>
<td style="text-align: center;">6.63</td>
<td style="text-align: center;">6.21</td>
<td style="text-align: center;">1e-8</td>
<td style="text-align: center;">5.712</td>
<td style="text-align: center;">1e-8</td>
<td style="text-align: center;">7.499</td>
</tr>
<tr class="odd">
<td style="text-align: center;">tre 0</td>
<td style="text-align: center;">44524</td>
<td style="text-align: center;">1.02</td>
<td style="text-align: center;">1.69</td>
<td style="text-align: center;">5e4</td>
<td style="text-align: center;">3.338</td>
<td style="text-align: center;">0.445</td>
<td style="text-align: center;">6.448</td>
</tr>
</tbody>
</table>
<p>The model was tested on the positive soil loss data from 2011, with 1762 observations. The overall RMSE was <span class="math inline">\(21.73\)</span> and the MAE was <span class="math inline">\(5.70\)</span>. The within group RMSEs on the test set were relatively low for all strata except for ‘corn 5’, ‘corn 6’, ‘none 5’, and ‘none 6’.</p>
<section id="december-3-2024" class="level3">
<h3 class="anchored" data-anchor-id="december-3-2024">December 3, 2024</h3>
</section>
</section>
<section id="summary-22" class="level2">
<h2 class="anchored" data-anchor-id="summary-22">Summary</h2>
<p>This week I fit a series of local expert GP models to the stratified data sample, fitting on soil loss based on ‘till’ and on average detachment based on the variables ‘crop’ and ‘till’. I found that the crop model fit to soil loss better than the till model, while the two models were about equal in performance when fit to average detachment.</p>
<p>I also fit a set of linear regression models to the positive soil loss dataset to test for interactions between ‘crop’, ‘till’, and the numeric variables. Both had particularly strong interactions with slope and precip and weaker interactions with the other variables.</p>
</section>
<section id="gp-of-soil-loss-by-till" class="level2">
<h2 class="anchored" data-anchor-id="gp-of-soil-loss-by-till">GP of Soil Loss by Till</h2>
<p>From my stratified data sample, I extracted only the dates between 2009 and 2010 which had nonzero soil loss, a total of 5938 observations. I then fit a local expert GP model by till using the variables ‘rad’, ‘tmax’, ‘precip’, ‘max_prcp’, and ‘slp_wavg4’. The ML estimates of the component models are shown below.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 12%">
<col style="width: 15%">
<col style="width: 16%">
<col style="width: 12%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">till</th>
<th style="text-align: center;">rad</th>
<th style="text-align: center;">tmax</th>
<th style="text-align: center;">precip</th>
<th style="text-align: center;">max_prcp</th>
<th style="text-align: center;">slp_wavg4</th>
<th style="text-align: center;">nugget</th>
<th style="text-align: center;">sp.var</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">2.51</td>
<td style="text-align: center;">1.01</td>
<td style="text-align: center;">3.71</td>
<td style="text-align: center;">5e4</td>
<td style="text-align: center;">3.84</td>
<td style="text-align: center;">3.258</td>
<td style="text-align: center;">0.482</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">83.05</td>
<td style="text-align: center;">59.05</td>
<td style="text-align: center;">5.71</td>
<td style="text-align: center;">85.96</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.116</td>
<td style="text-align: center;">25.67</td>
</tr>
<tr class="odd">
<td style="text-align: center;">4</td>
<td style="text-align: center;">1297</td>
<td style="text-align: center;">12483</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">11744</td>
<td style="text-align: center;">0.407</td>
<td style="text-align: center;">0.052</td>
<td style="text-align: center;">162.2</td>
</tr>
<tr class="even">
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">5.58</td>
<td style="text-align: center;">0.232</td>
<td style="text-align: center;">14.53</td>
<td style="text-align: center;">0.017</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">4822.4</td>
</tr>
<tr class="odd">
<td style="text-align: center;">6</td>
<td style="text-align: center;">131.3</td>
<td style="text-align: center;">159.3</td>
<td style="text-align: center;">6.62</td>
<td style="text-align: center;">0.012</td>
<td style="text-align: center;">5.521</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;">928.8</td>
</tr>
</tbody>
</table>
<p>The model was tested on the positive soil loss data from 2011, with 1769 observations. The overall RMSE was <span class="math inline">\(26.44\)</span> and the MAE was <span class="math inline">\(6.60\)</span>. The within group RMSEs on the test set for Till = 0, 1, 4, 5, 6 were <span class="math inline">\(3.60\)</span>, <span class="math inline">\(1.93\)</span>, <span class="math inline">\(12.30\)</span>, <span class="math inline">\(35.27\)</span>, and <span class="math inline">\(39.45\)</span>. Till levels 5 and 6 had relatively high RMSEs while the rest had low, since those had a much greater range in soil loss values than the others.</p>
</section>
<section id="gp-of-average-detachment-by-crop-and-till" class="level2">
<h2 class="anchored" data-anchor-id="gp-of-average-detachment-by-crop-and-till">GP of Average Detachment by Crop and Till</h2>
<p>Again from the stratified sample, I extracted the days with positive average detachment between 2009 and 2010, a total of <span class="math inline">\(4662\)</span> observations. I then fit a local expert GP model by crop using the variables ‘rad’, ‘tmax’, ‘precip’, ‘max_prcp’, and ‘slp_wavg4’. The ML estimates of the component models are shown below.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 11%">
<col style="width: 14%">
<col style="width: 16%">
<col style="width: 11%">
<col style="width: 11%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">till</th>
<th style="text-align: center;">rad</th>
<th style="text-align: center;">tmax</th>
<th style="text-align: center;">precip</th>
<th style="text-align: center;">max_prcp</th>
<th style="text-align: center;">slp_wavg4</th>
<th style="text-align: center;">nugget</th>
<th style="text-align: center;">sp.var</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">alfalfa</td>
<td style="text-align: center;">90.70</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">42.34</td>
<td style="text-align: center;">0.009</td>
<td style="text-align: center;">414.7</td>
<td style="text-align: center;">0.027</td>
<td style="text-align: center;">0.014</td>
</tr>
<tr class="even">
<td style="text-align: center;">bromegr</td>
<td style="text-align: center;">4.77</td>
<td style="text-align: center;">11.53</td>
<td style="text-align: center;">5e4</td>
<td style="text-align: center;">24388</td>
<td style="text-align: center;">35410</td>
<td style="text-align: center;">0.495</td>
<td style="text-align: center;">0.002</td>
</tr>
<tr class="odd">
<td style="text-align: center;">corn</td>
<td style="text-align: center;">1.62</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.181</td>
<td style="text-align: center;">1.92</td>
<td style="text-align: center;">0.017</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">0.200</td>
</tr>
<tr class="even">
<td style="text-align: center;">tre</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">4.22</td>
<td style="text-align: center;">8.50</td>
<td style="text-align: center;">8.09</td>
<td style="text-align: center;">9.228</td>
<td style="text-align: center;">0.203</td>
<td style="text-align: center;">0.0007</td>
</tr>
<tr class="odd">
<td style="text-align: center;">none</td>
<td style="text-align: center;">146.6</td>
<td style="text-align: center;">4.03</td>
<td style="text-align: center;">608.3</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.668</td>
<td style="text-align: center;">0.064</td>
<td style="text-align: center;">0.007</td>
</tr>
</tbody>
</table>
<p>The model was tested on the positive average detachment data from 2011, with <span class="math inline">\(946\)</span> observations. The overall RMSE was <span class="math inline">\(0.268\)</span> and the MAE was <span class="math inline">\(0.095\)</span>. The within group RMSEs on the test set for alfalfa, bromegrass, corn, tre, and none were <span class="math inline">\(0.044\)</span>, <span class="math inline">\(0.055\)</span>, <span class="math inline">\(0.295\)</span>, <span class="math inline">\(0.020\)</span>, and <span class="math inline">\(0.278\)</span>. I then fit a GP with the same inputs but stratified on ‘till’, with its ML estimates shown below.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 12%">
<col style="width: 15%">
<col style="width: 16%">
<col style="width: 12%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">till</th>
<th style="text-align: center;">rad</th>
<th style="text-align: center;">tmax</th>
<th style="text-align: center;">precip</th>
<th style="text-align: center;">max_prcp</th>
<th style="text-align: center;">slp_wavg4</th>
<th style="text-align: center;">nugget</th>
<th style="text-align: center;">sp.var</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">1.89</td>
<td style="text-align: center;">8.66</td>
<td style="text-align: center;">3.97</td>
<td style="text-align: center;">273.1</td>
<td style="text-align: center;">454.8</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.001</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1.78</td>
<td style="text-align: center;">7.19</td>
<td style="text-align: center;">1.08</td>
<td style="text-align: center;">12.59</td>
<td style="text-align: center;">1.574</td>
<td style="text-align: center;">0.048</td>
<td style="text-align: center;">0.003</td>
</tr>
<tr class="odd">
<td style="text-align: center;">4</td>
<td style="text-align: center;">32.45</td>
<td style="text-align: center;">2.90</td>
<td style="text-align: center;">6.12</td>
<td style="text-align: center;">4.66</td>
<td style="text-align: center;">0.748</td>
<td style="text-align: center;">0.131</td>
<td style="text-align: center;">0.010</td>
</tr>
<tr class="even">
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">1.40</td>
<td style="text-align: center;">0.347</td>
<td style="text-align: center;">13.45</td>
<td style="text-align: center;">0.028</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.209</td>
</tr>
<tr class="odd">
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">782.8</td>
<td style="text-align: center;">7.22</td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">0.147</td>
<td style="text-align: center;">0.040</td>
</tr>
</tbody>
</table>
<p>The model was tested on the positive average detachment data from 2011, with <span class="math inline">\(946\)</span> observations. The overall RMSE was <span class="math inline">\(0.249\)</span> and the MAE was <span class="math inline">\(0.084\)</span>. The within group RMSEs on the test set for till = 0, 1, 4, 5, 6 were <span class="math inline">\(0.041\)</span>, <span class="math inline">\(0.039\)</span>, <span class="math inline">\(0.125\)</span>, <span class="math inline">\(0.298\)</span>, and <span class="math inline">\(0.417\)</span>.</p>
</section>
<section id="linear-models-with-interactions" class="level2">
<h2 class="anchored" data-anchor-id="linear-models-with-interactions">Linear Models with Interactions</h2>
<p>Using the positive soil loss dataset, I fit a series of linear models to test for interactions between the categorical and numeric variables. The table below displays parameter estimates for the variables ‘crop’, ‘rad’, ‘tmin’, precip’, ‘max_prcp’, and ‘slp_wavg4’. It is filtered to only display the interaction terms that were statistically significant.</p>
<div class="cell">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">statistic</th>
<th style="text-align: right;">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: right;">-2.23</td>
<td style="text-align: right;">2.26</td>
<td style="text-align: right;">-0.99</td>
<td style="text-align: right;">0.32</td>
</tr>
<tr class="even">
<td style="text-align: left;">cropbromegr</td>
<td style="text-align: right;">-1.24</td>
<td style="text-align: right;">4.50</td>
<td style="text-align: right;">-0.27</td>
<td style="text-align: right;">0.78</td>
</tr>
<tr class="odd">
<td style="text-align: left;">cropcorn</td>
<td style="text-align: right;">-10.10</td>
<td style="text-align: right;">2.58</td>
<td style="text-align: right;">-3.92</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">cropnone</td>
<td style="text-align: right;">2.53</td>
<td style="text-align: right;">3.30</td>
<td style="text-align: right;">0.77</td>
<td style="text-align: right;">0.44</td>
</tr>
<tr class="odd">
<td style="text-align: left;">cropsoy</td>
<td style="text-align: right;">-11.63</td>
<td style="text-align: right;">2.85</td>
<td style="text-align: right;">-4.09</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">croptre</td>
<td style="text-align: right;">-0.77</td>
<td style="text-align: right;">5.83</td>
<td style="text-align: right;">-0.13</td>
<td style="text-align: right;">0.89</td>
</tr>
<tr class="odd">
<td style="text-align: left;">rad</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.49</td>
<td style="text-align: right;">0.62</td>
</tr>
<tr class="even">
<td style="text-align: left;">tmin</td>
<td style="text-align: right;">-0.16</td>
<td style="text-align: right;">0.09</td>
<td style="text-align: right;">-1.83</td>
<td style="text-align: right;">0.07</td>
</tr>
<tr class="odd">
<td style="text-align: left;">precip</td>
<td style="text-align: right;">0.09</td>
<td style="text-align: right;">0.02</td>
<td style="text-align: right;">3.80</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">max_prcp</td>
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">0.04</td>
<td style="text-align: right;">0.32</td>
<td style="text-align: right;">0.75</td>
</tr>
<tr class="odd">
<td style="text-align: left;">slp_wavg4</td>
<td style="text-align: right;">16.51</td>
<td style="text-align: right;">13.16</td>
<td style="text-align: right;">1.25</td>
<td style="text-align: right;">0.21</td>
</tr>
<tr class="even">
<td style="text-align: left;">cropsoy:rad</td>
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">2.13</td>
<td style="text-align: right;">0.03</td>
</tr>
<tr class="odd">
<td style="text-align: left;">cropcorn:tmin</td>
<td style="text-align: right;">0.19</td>
<td style="text-align: right;">0.11</td>
<td style="text-align: right;">1.81</td>
<td style="text-align: right;">0.07</td>
</tr>
<tr class="even">
<td style="text-align: left;">cropnone:tmin</td>
<td style="text-align: right;">0.39</td>
<td style="text-align: right;">0.18</td>
<td style="text-align: right;">2.17</td>
<td style="text-align: right;">0.03</td>
</tr>
<tr class="odd">
<td style="text-align: left;">cropcorn:precip</td>
<td style="text-align: right;">0.08</td>
<td style="text-align: right;">0.03</td>
<td style="text-align: right;">3.19</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">cropsoy:precip</td>
<td style="text-align: right;">0.07</td>
<td style="text-align: right;">0.03</td>
<td style="text-align: right;">2.21</td>
<td style="text-align: right;">0.03</td>
</tr>
<tr class="odd">
<td style="text-align: left;">cropcorn:max_prcp</td>
<td style="text-align: right;">0.10</td>
<td style="text-align: right;">0.04</td>
<td style="text-align: right;">2.20</td>
<td style="text-align: right;">0.03</td>
</tr>
<tr class="even">
<td style="text-align: left;">cropcorn:slp_wavg4</td>
<td style="text-align: right;">162.59</td>
<td style="text-align: right;">15.32</td>
<td style="text-align: right;">10.62</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">cropsoy:slp_wavg4</td>
<td style="text-align: right;">208.82</td>
<td style="text-align: right;">17.65</td>
<td style="text-align: right;">11.83</td>
<td style="text-align: right;">0.00</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Below are the model results for the variable ‘till’, filtered as before.</p>
<div class="cell">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">statistic</th>
<th style="text-align: right;">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: right;">-2.41</td>
<td style="text-align: right;">2.05</td>
<td style="text-align: right;">-1.18</td>
<td style="text-align: right;">0.24</td>
</tr>
<tr class="even">
<td style="text-align: left;">till1</td>
<td style="text-align: right;">1.13</td>
<td style="text-align: right;">2.50</td>
<td style="text-align: right;">0.45</td>
<td style="text-align: right;">0.65</td>
</tr>
<tr class="odd">
<td style="text-align: left;">till4</td>
<td style="text-align: right;">0.45</td>
<td style="text-align: right;">5.60</td>
<td style="text-align: right;">0.08</td>
<td style="text-align: right;">0.94</td>
</tr>
<tr class="even">
<td style="text-align: left;">till5</td>
<td style="text-align: right;">-13.61</td>
<td style="text-align: right;">2.28</td>
<td style="text-align: right;">-5.97</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">till6</td>
<td style="text-align: right;">-34.88</td>
<td style="text-align: right;">6.28</td>
<td style="text-align: right;">-5.55</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">rad</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.34</td>
<td style="text-align: right;">0.73</td>
</tr>
<tr class="odd">
<td style="text-align: left;">tmin</td>
<td style="text-align: right;">-0.04</td>
<td style="text-align: right;">0.08</td>
<td style="text-align: right;">-0.51</td>
<td style="text-align: right;">0.61</td>
</tr>
<tr class="even">
<td style="text-align: left;">precip</td>
<td style="text-align: right;">0.06</td>
<td style="text-align: right;">0.02</td>
<td style="text-align: right;">2.92</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">max_prcp</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.03</td>
<td style="text-align: right;">0.07</td>
<td style="text-align: right;">0.94</td>
</tr>
<tr class="even">
<td style="text-align: left;">slp_wavg4</td>
<td style="text-align: right;">10.73</td>
<td style="text-align: right;">12.05</td>
<td style="text-align: right;">0.89</td>
<td style="text-align: right;">0.37</td>
</tr>
<tr class="odd">
<td style="text-align: left;">till5:rad</td>
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">2.42</td>
<td style="text-align: right;">0.02</td>
</tr>
<tr class="even">
<td style="text-align: left;">till6:rad</td>
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">1.86</td>
<td style="text-align: right;">0.06</td>
</tr>
<tr class="odd">
<td style="text-align: left;">till5:tmin</td>
<td style="text-align: right;">0.15</td>
<td style="text-align: right;">0.09</td>
<td style="text-align: right;">1.70</td>
<td style="text-align: right;">0.09</td>
</tr>
<tr class="even">
<td style="text-align: left;">till5:precip</td>
<td style="text-align: right;">0.12</td>
<td style="text-align: right;">0.02</td>
<td style="text-align: right;">4.89</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">till6:precip</td>
<td style="text-align: right;">0.70</td>
<td style="text-align: right;">0.07</td>
<td style="text-align: right;">10.40</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">till5:max_prcp</td>
<td style="text-align: right;">0.12</td>
<td style="text-align: right;">0.04</td>
<td style="text-align: right;">3.12</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">till6:max_prcp</td>
<td style="text-align: right;">0.31</td>
<td style="text-align: right;">0.07</td>
<td style="text-align: right;">4.49</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">till5:slp_wavg4</td>
<td style="text-align: right;">267.98</td>
<td style="text-align: right;">14.36</td>
<td style="text-align: right;">18.66</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">till6:slp_wavg4</td>
<td style="text-align: right;">184.26</td>
<td style="text-align: right;">57.92</td>
<td style="text-align: right;">3.18</td>
<td style="text-align: right;">0.00</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Lastly, here are the model results for the variable ‘cover’.</p>
<div class="cell">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">statistic</th>
<th style="text-align: right;">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: right;">-7.91</td>
<td style="text-align: right;">0.79</td>
<td style="text-align: right;">-10.02</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">coverTRUE</td>
<td style="text-align: right;">1.87</td>
<td style="text-align: right;">3.04</td>
<td style="text-align: right;">0.61</td>
<td style="text-align: right;">0.54</td>
</tr>
<tr class="odd">
<td style="text-align: left;">rad</td>
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">4.87</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">tmin</td>
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">0.04</td>
<td style="text-align: right;">0.36</td>
<td style="text-align: right;">0.72</td>
</tr>
<tr class="odd">
<td style="text-align: left;">precip</td>
<td style="text-align: right;">0.12</td>
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">13.19</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">max_prcp</td>
<td style="text-align: right;">0.09</td>
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">7.80</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">slp_wavg4</td>
<td style="text-align: right;">106.57</td>
<td style="text-align: right;">5.18</td>
<td style="text-align: right;">20.56</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">coverTRUE:rad</td>
<td style="text-align: right;">-0.01</td>
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">-0.91</td>
<td style="text-align: right;">0.36</td>
</tr>
<tr class="odd">
<td style="text-align: left;">coverTRUE:tmin</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.16</td>
<td style="text-align: right;">0.03</td>
<td style="text-align: right;">0.98</td>
</tr>
<tr class="even">
<td style="text-align: left;">coverTRUE:precip</td>
<td style="text-align: right;">0.03</td>
<td style="text-align: right;">0.03</td>
<td style="text-align: right;">1.03</td>
<td style="text-align: right;">0.31</td>
</tr>
<tr class="odd">
<td style="text-align: left;">coverTRUE:max_prcp</td>
<td style="text-align: right;">-0.08</td>
<td style="text-align: right;">0.07</td>
<td style="text-align: right;">-1.06</td>
<td style="text-align: right;">0.29</td>
</tr>
<tr class="even">
<td style="text-align: left;">coverTRUE:slp_wavg4</td>
<td style="text-align: right;">-59.80</td>
<td style="text-align: right;">20.91</td>
<td style="text-align: right;">-2.86</td>
<td style="text-align: right;">0.00</td>
</tr>
</tbody>
</table>
</div>
</div>
<section id="november-19-2024" class="level3">
<h3 class="anchored" data-anchor-id="november-19-2024">November 19, 2024</h3>
</section>
</section>
<section id="summary-23" class="level2">
<h2 class="anchored" data-anchor-id="summary-23">Summary</h2>
<p>This week I studied an article by describing an approach to incorporating qualitative variables in Gaussian processes based on latent variables. By expressing each level of a quantitative variable as a value of a 2D latent numeric variable, they can model the underlying numeric process behind the variable directly. This method had significantly lower restricted RMSE than comparable qualitative methods.</p>
<p>I also conducted further exploratory analyses of the stratified data set. I graphed soil loss by several input variables and colored the points according to the ‘crop’ and ‘till’ variables. The results confirmed that alfalfa and bromegrass generally had lower soil loss than corn and soy, even when controlling for inputs such as precipitation, temperature, and slope.</p>
</section>
<section id="latent-variable-approach-to-gp-modeling" class="level2">
<h2 class="anchored" data-anchor-id="latent-variable-approach-to-gp-modeling">Latent Variable Approach to GP Modeling</h2>
<p>In their paper “A Latent Variable Approach to GP Modeling with Qualitative and Quantitative Variables”, the authors Zhang et al.&nbsp;argue that in any physical process qualitative variables are stand-ins for unobserved numeric variables. Therefore, they can be modeled by modeling the underlying latent variables they represent. For a GP with numeric inputs <span class="math inline">\(x_1,\dots, x_p\)</span> and a qualitative input <span class="math inline">\(t\)</span>, they suggest this modification of the squared exponential function. <span class="math display">\[K(X, X') =\exp\left(-\sum_{i=1}^p \frac{|x_i-x_i'|^2}{\theta_i} -|z(t)-z(t')|^2 \right)\]</span> Where <span class="math inline">\(\theta_i\)</span> are lengthscale parameters and <span class="math inline">\(z(t)\)</span> is a function mapping each level of <span class="math inline">\(t\)</span> to a 2D numeric quantity. The <span class="math inline">\(z(t)\)</span> values can then be fit through MLE.</p>
<p>A two dimensional latent variable is deemed superior to a one dimensional variable for the following reason. Suppose the qualitative factor has three levels with equal correlation between each pair of levels. To represent this with a one dimensional mapping is impossible, but with two or more dimensions it can be done. To prevent indeterminacy in ML estimation, they further restrict the first level of each qualitative variable to be mapped to <span class="math inline">\((0,0)\)</span> and the second to be mapped to the horizontal axis, so for a variable with <span class="math inline">\(m\)</span> levels, there are <span class="math inline">\(2m-3\)</span> scalar values to estimate.</p>
<p>The authors compared their method to two other methods of qualitative GP analysis: multiplicative covariance and hypersphere covariance, which they called unrestricted covariance. I discussed these methods on October 22 of this report. The latent variable method outperformed both in terms of restricted RMSE, which is the RMSE divided by the sum of the squared differences between the predicted response and the true mean of the responses, on multiple real-life datasets.</p>
</section>
<section id="additional-exploratory-analysis" class="level2">
<h2 class="anchored" data-anchor-id="additional-exploratory-analysis">Additional Exploratory Analysis</h2>
<p>I performed a more in-depth analysis of the relationships between the new categorical variables and the numeric variables. Shown below are plots of significant inputs by response with points colored according to crop or to tilling operations.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-46-1.png" style="width:50.0%;height:50.0%" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-46-2.png" style="width:50.0%;height:50.0%" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-46-3.png" style="width:50.0%;height:50.0%" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-46-4.png" style="width:50.0%;height:50.0%" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-46-5.png" style="width:50.0%;height:50.0%" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-46-6.png" style="width:50.0%;height:50.0%" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>I also fit a linear model to soil loss, including ‘crop’ and ‘till’ as inputs. I used ‘precip’, ‘rad’, ‘tmin’, and ‘slp_wavg4’ as my numeric inputs.</p>
<div class="cell">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">statistic</th>
<th style="text-align: right;">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: right;">-5.79</td>
<td style="text-align: right;">0.69</td>
<td style="text-align: right;">-8.41</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">precip</td>
<td style="text-align: right;">3.07</td>
<td style="text-align: right;">0.20</td>
<td style="text-align: right;">15.76</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">rad</td>
<td style="text-align: right;">0.83</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">3.75</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">tmin</td>
<td style="text-align: right;">0.12</td>
<td style="text-align: right;">0.25</td>
<td style="text-align: right;">0.49</td>
<td style="text-align: right;">0.62</td>
</tr>
<tr class="odd">
<td style="text-align: left;">slp_wavg4</td>
<td style="text-align: right;">5.84</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">26.58</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">cropbromegr</td>
<td style="text-align: right;">1.83</td>
<td style="text-align: right;">1.30</td>
<td style="text-align: right;">1.40</td>
<td style="text-align: right;">0.16</td>
</tr>
<tr class="odd">
<td style="text-align: left;">cropcorn</td>
<td style="text-align: right;">6.47</td>
<td style="text-align: right;">1.16</td>
<td style="text-align: right;">5.56</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">cropnone</td>
<td style="text-align: right;">4.77</td>
<td style="text-align: right;">1.37</td>
<td style="text-align: right;">3.48</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">cropsoy</td>
<td style="text-align: right;">6.69</td>
<td style="text-align: right;">1.20</td>
<td style="text-align: right;">5.56</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">croptre</td>
<td style="text-align: right;">1.70</td>
<td style="text-align: right;">1.31</td>
<td style="text-align: right;">1.31</td>
<td style="text-align: right;">0.19</td>
</tr>
<tr class="odd">
<td style="text-align: left;">till1</td>
<td style="text-align: right;">1.48</td>
<td style="text-align: right;">1.29</td>
<td style="text-align: right;">1.14</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="even">
<td style="text-align: left;">till4</td>
<td style="text-align: right;">1.09</td>
<td style="text-align: right;">1.81</td>
<td style="text-align: right;">0.60</td>
<td style="text-align: right;">0.55</td>
</tr>
<tr class="odd">
<td style="text-align: left;">till5</td>
<td style="text-align: right;">7.63</td>
<td style="text-align: right;">1.24</td>
<td style="text-align: right;">6.16</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">till6</td>
<td style="text-align: right;">8.90</td>
<td style="text-align: right;">1.68</td>
<td style="text-align: right;">5.30</td>
<td style="text-align: right;">0.00</td>
</tr>
</tbody>
</table>
</div>
</div>
<section id="november-12-2024" class="level3">
<h3 class="anchored" data-anchor-id="november-12-2024">November 12, 2024</h3>
</section>
</section>
<section id="summary-24" class="level2">
<h2 class="anchored" data-anchor-id="summary-24">Summary</h2>
<p>This week I reran the Dirichlet process cluster analysis on a dataset with unbalanced cluster sizes, having one large cluster, two medium, and one small. The small cluster was still fit fairly accurately, so the bias towards adding points to large clusters doesn’t seem to be very big.</p>
<p>I experimented with random forest models to predict whether a observation had positive soil loss or not. I found that ‘crop’ was a much more significant variable than ‘till’, and ‘month’ was also significant. By undersampling the zero cases to form a new training set and refitting the model, I was able to somewhat improve the predictive accuracy on a test set.</p>
<p>I also fit an NLE GP model to positive soil loss data from 2009 and 2010, separated by crop. Based on test data from 2011, Corn and No Crop were much harder to predict than the others, as there is much more variability in soil loss within those groups.</p>
</section>
<section id="soil-loss-prediction-with-random-forest" class="level2">
<h2 class="anchored" data-anchor-id="soil-loss-prediction-with-random-forest">Soil Loss Prediction with Random Forest</h2>
<p>I fit a random forest model to the stratified sample of hillslopes using the new categorical variables. I first fit a model including all the new variables and five variables that had proven significant in previous models. Second, I fit a model using only the top 5 variables. The table displays the importance of each variable in the models by the mean decrease in the Gini index (a measure of the purity of sets).</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Variable</th>
<th style="text-align: center;">Full</th>
<th style="text-align: center;">Reduced</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">precip</td>
<td style="text-align: center;">54803</td>
<td style="text-align: center;">56420</td>
</tr>
<tr class="even">
<td style="text-align: center;">slp_wavg1</td>
<td style="text-align: center;">1918</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="odd">
<td style="text-align: center;">tmin</td>
<td style="text-align: center;">7300</td>
<td style="text-align: center;">8151</td>
</tr>
<tr class="even">
<td style="text-align: center;">rad</td>
<td style="text-align: center;">3982</td>
<td style="text-align: center;">3259</td>
</tr>
<tr class="odd">
<td style="text-align: center;">max_prcp</td>
<td style="text-align: center;">1128</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="even">
<td style="text-align: center;">crop</td>
<td style="text-align: center;">2813</td>
<td style="text-align: center;">3022</td>
</tr>
<tr class="odd">
<td style="text-align: center;">till</td>
<td style="text-align: center;">338</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="even">
<td style="text-align: center;">month</td>
<td style="text-align: center;">4309</td>
<td style="text-align: center;">4120</td>
</tr>
<tr class="odd">
<td style="text-align: center;">cover</td>
<td style="text-align: center;">251</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>The confusion matrix of the second model showed that it never predicted any observation in the training set as having nonzero soil loss. This occurred even though I tried to weight the nonzero cases in the training set. Its performance was not bad on the test set, however.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Model</th>
<th style="text-align: center;">True\Pred</th>
<th style="text-align: center;">0</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">ClassError</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Train</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">149877</td>
<td style="text-align: center;">2911</td>
<td style="text-align: center;">0.0191</td>
</tr>
<tr class="even">
<td style="text-align: center;">.</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Test</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">16646</td>
<td style="text-align: center;">330</td>
<td style="text-align: center;">0.0194</td>
</tr>
<tr class="even">
<td style="text-align: center;">.</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">279</td>
<td style="text-align: center;">0.0313</td>
</tr>
</tbody>
</table>
<p>I decided to try fitting a random forest model to a more balanced training set by undersampling the zero cases. I took my training set and randomly selected <span class="math inline">\(10\%\)</span> of the zero cases and added them to all the nonzero cases in the training set. The resulting data had <span class="math inline">\(15279\)</span> zero observations and <span class="math inline">\(2593\)</span> nonzero observations. This model performed much better on predicting the nonzero observations and a bit worse on the zero observations.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Model</th>
<th style="text-align: center;">True\Pred</th>
<th style="text-align: center;">0</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">ClassError</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Train</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">14891</td>
<td style="text-align: center;">388</td>
<td style="text-align: center;">0.0254</td>
</tr>
<tr class="even">
<td style="text-align: center;">.</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2557</td>
<td style="text-align: center;">0.0139</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Test</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">16556</td>
<td style="text-align: center;">420</td>
<td style="text-align: center;">0.0247</td>
</tr>
<tr class="even">
<td style="text-align: center;">.</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">284</td>
<td style="text-align: center;">0.0139</td>
</tr>
</tbody>
</table>
</section>
<section id="gp-nle-by-crop" class="level2">
<h2 class="anchored" data-anchor-id="gp-nle-by-crop">GP NLE by Crop</h2>
<p>From my stratified data sample, I extracted only the dates between 2009 and 2010 which had nonzero soil loss, a total of 5938 observations. I then fit a local expert GP model by crop using the variables ‘rad’, ‘tmax’, ‘precip’, and ‘max_prcp’. The ML estimates of the component models are shown below.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">crop</th>
<th style="text-align: center;">rad</th>
<th style="text-align: center;">tmax</th>
<th style="text-align: center;">precip</th>
<th style="text-align: center;">max_prcp</th>
<th style="text-align: center;">nugget</th>
<th style="text-align: center;">sp.var</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">alfalfa</td>
<td style="text-align: center;">32.19</td>
<td style="text-align: center;">47.70</td>
<td style="text-align: center;">26.98</td>
<td style="text-align: center;">14.77</td>
<td style="text-align: center;">9.318</td>
<td style="text-align: center;">0.003</td>
</tr>
<tr class="even">
<td style="text-align: center;">bromegr</td>
<td style="text-align: center;">4.14</td>
<td style="text-align: center;">12.06</td>
<td style="text-align: center;">39033</td>
<td style="text-align: center;">50000</td>
<td style="text-align: center;">11.534</td>
<td style="text-align: center;">0.0003</td>
</tr>
<tr class="odd">
<td style="text-align: center;">corn</td>
<td style="text-align: center;">39052</td>
<td style="text-align: center;">2302</td>
<td style="text-align: center;">0.689</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">0.169</td>
<td style="text-align: center;">3249</td>
</tr>
<tr class="even">
<td style="text-align: center;">tre</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">3.04</td>
<td style="text-align: center;">3.22</td>
<td style="text-align: center;">4.95</td>
<td style="text-align: center;">0.494</td>
<td style="text-align: center;">6.672</td>
</tr>
<tr class="odd">
<td style="text-align: center;">none</td>
<td style="text-align: center;">50000</td>
<td style="text-align: center;">38244</td>
<td style="text-align: center;">0.053</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.272</td>
<td style="text-align: center;">15.345</td>
</tr>
</tbody>
</table>
<p>The model was tested on the positive soil loss data from 2011, with 1769 observations. The overall RMSE was <span class="math inline">\(18.60\)</span> and the MAE was <span class="math inline">\(4.26\)</span>. The within group RMSEs on the test set for Alfalfa, Bromegrass, Corn, Tre, and None were <span class="math inline">\(3.33\)</span>, <span class="math inline">\(5.17\)</span>, <span class="math inline">\(20.60\)</span>, <span class="math inline">\(2.21\)</span>, and <span class="math inline">\(24.06\)</span>. Corn and None had relatively high RMSEs while Alfalfa, Bromegrass, and Tre had low, since the first two had a much greater range in soil loss values than the others.</p>
</section>
<section id="dpmm-with-unbalanced-cluster-sizes" class="level2">
<h2 class="anchored" data-anchor-id="dpmm-with-unbalanced-cluster-sizes">DPMM with Unbalanced Cluster Sizes</h2>
<p>I repeated the Dirichlet Process Mixture Modeling example from last week, but with unbalanced cluster sizes. I drew <span class="math inline">\(20\)</span> points from Cluster 1, <span class="math inline">\(100\)</span> from cluster 2 and <span class="math inline">\(60\)</span> from the other two, rather than <span class="math inline">\(60\)</span> from each. The training data is plotted below.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot1-11-12-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The results of the DPMM algorithm with <span class="math inline">\(\alpha=0.01\)</span> are shown below, comparing the true cluster assignments to the estimated ones. The class imbalance did not seem to significantly affect the accuracy of the result, though Cluster 4 is still poorly handled. I also tried running the algorithm for 2000 iterations instead of 1000 but it didn’t meaningfully change the results.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">True\Est</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">37</td>
</tr>
</tbody>
</table>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot2-11-12-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="november-5-2024" class="level3">
<h3 class="anchored" data-anchor-id="november-5-2024">November 5, 2024</h3>
</section>
</section>
<section id="summary-25" class="level2">
<h2 class="anchored" data-anchor-id="summary-25">Summary</h2>
<p>This week I prepared an example of Dirichlet Process Mixture Modeling, which sorts data values into clusters by modeling each cluster as a multivariate Gaussian distribution. The advantage of this method is that you do not need to specify the number of clusters beforehand; it is estimated from the data.</p>
<p>I also merged the management data with the weather data for the stratified sample of HUC12s. I tested the categorical variables for correlation with the response variables and prepared some boxplots displaying their relationships. These plots showed that soil loss was highest for main crops like corn and soy and for tilling with row cultivators. This seems reasonable since row cultivators can be used multiple times per season.</p>
</section>
<section id="dirichlet-process-mixture-modeling" class="level2">
<h2 class="anchored" data-anchor-id="dirichlet-process-mixture-modeling">Dirichlet Process Mixture Modeling</h2>
<p>I borrowed the following example from . Dirichlet Process Mixture Modeling is a method of cluster analysis based on the Gaussian mixture models. The data is divided into clusters where the responses in each cluster are drawn from a different multivariate Gaussian distribution. The special characteristic of DPMM is that the number of clusters is estimated organically from the data and doesn’t need to be specified beforehand.</p>
<p>The cluster assignment process is based on an MCMC algorithm that generates a sequence of cluster assignments. For each iteration, the data points are assigned to clusters with a Chinese Restaurant Process. Imagine a Chinese restaurant with an infinite number of tables. Diners enter one by one to be seated. The first diner always sits at the first table. The second diner sits at the first table with probability <span class="math inline">\(1/(1+\alpha)\)</span> and at the second table with probability <span class="math inline">\(\alpha/(1+\alpha)\)</span> where <span class="math inline">\(\alpha\)</span> is a positive real number. The <span class="math inline">\(k^{th}\)</span> diner sits at an occupied table with probability proportional to the number of people already sitting there and at the next unoccupied table with probability proportional to <span class="math inline">\(\alpha\)</span>.</p>
<p>In our case, those probabilities are adjusted by the posterior probability of the new point belonging to each cluster given prior assumptions about the distribution of points within a cluster. Once you have a sequence of such assignments, you note for each point the cluster to which it was most often assigned, which is its final assignment.</p>
<p>Suppose we have a data vector <span class="math inline">\(y\)</span> normally distributed according to a mean <span class="math inline">\(\mu\)</span> and a known measurement error <span class="math inline">\(\sigma_y^2\)</span>, and that <span class="math inline">\(\mu\)</span> is distributed with a prior mean <span class="math inline">\(\mu_0\)</span> and precision <span class="math inline">\(\tau_0^2\)</span>. Then, to be precise, the probability of a new point <span class="math inline">\(y_i\)</span> being assigned to cluster <span class="math inline">\(k\)</span> is <span class="math display">\[p(y_i\in c_k)\propto n_{-i,k}\times \text{pdf}(y_i|\mu_k,\tau_k)\]</span> <span class="math display">\[\propto \frac{n_{-i,k}}{n-1+\alpha}\Phi\left(y_i| \frac{\bar y_kn_k \tau_k+\mu_0\tau_0}{n_k\tau_k+\tau_0}, (n_k\tau_k+\tau_0)^{-1}+\sigma_y^2 \right)\]</span> where <span class="math inline">\(n_{-i,k}\)</span> is the number of points in cluster <span class="math inline">\(k\)</span> not counting <span class="math inline">\(y_i\)</span>, <span class="math inline">\(n\)</span> is the total number of points assigned to clusters, and <span class="math inline">\(\bar y_k\)</span> and <span class="math inline">\(\tau_k\)</span> are the estimated mean and precision of cluster <span class="math inline">\(k\)</span>. Finally, <span class="math inline">\(\Phi()\)</span> is the pdf of the Normal distribution.</p>
<p>For our example we sixty points each from four bivariate normal distributions, which are our four clusters. The data set is plotted below.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot1-11-4-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The results of the DPMM algorithm with <span class="math inline">\(\alpha=0.01\)</span> are shown below, comparing the true cluster assignments to the estimated ones.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">True\Est</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">59</td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">9</td>
</tr>
</tbody>
</table>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot2-11-4-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Repeating this process on the same data but with an <span class="math inline">\(\alpha\)</span> value of <span class="math inline">\(0.1\)</span> are shown here. The cluster assignments are very similar,</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">True\Est</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">60</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot3-11-4-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="tilling-operations-description" class="level2">
<h2 class="anchored" data-anchor-id="tilling-operations-description">Tilling Operations Description</h2>
<p>After studying, I can better define some of the equipment referenced in the tilling operations in the WEPP data set. A planter is used to cut open the soil and drop the seeds in. Chisel plows, field cultivators, and row cultivators are all used to break up the soil before or after the growing season. Chisel plows dig deepest, while cultivators are shallower. Row cultivators are used to break up the soil between planted rows, possibly for weed control. Tandem disks and double disk openers can be added to cultivators to further disturb the soil. Double disk openers are designed for handling stubble, field debris, or other trashy conditions.</p>
</section>
<section id="categorical-and-numeric-data-merge" class="level2">
<h2 class="anchored" data-anchor-id="categorical-and-numeric-data-merge">Categorical and Numeric Data Merge</h2>
<p>I added three variables to the stratified data set this week: Crop, Cover, Till, and Month. Crop records the kind of crop we think was growing on a hillslope at a particular time, Cover records whether the crop was a cover crop or not, Till records the set of tilling operations performed on that hillslope, and Month is the month the observation was recorded.</p>
<p>I performed Kruskal-Wallis rank sum tests between each categorical variable and response to test for correlation between them. Crop was significantly correlated to soil loss, runoff, and average detachment with chi-squared statistics of <span class="math inline">\(16916\)</span>, <span class="math inline">\(4409\)</span>, and <span class="math inline">\(16196\)</span> respectively. Cover was as well, with test statistics of <span class="math inline">\(1391\)</span>, <span class="math inline">\(491\)</span>, and <span class="math inline">\(1852\)</span> respectively. Till also was, with test statistics of <span class="math inline">\(805\)</span>, <span class="math inline">\(1010\)</span>, and <span class="math inline">\(472\)</span> respectively. The p-values were less than <span class="math inline">\(2.2\text{e-}16\)</span> for each case.</p>
<p>Here, I have plotted boxplots of the response variables with respect to the categorical inputs ‘crop’ and ‘till’. It is easy to see that main crops (corn and soy) have higher soil loss than cover crops.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-53-1.png" style="width:50.0%;height:50.0%" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-53-2.png" style="width:50.0%;height:50.0%" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-53-3.png" style="width:50.0%;height:50.0%" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-53-4.png" style="width:50.0%;height:50.0%" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-53-5.png" style="width:50.0%;height:50.0%" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-53-6.png" style="width:50.0%;height:50.0%" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="october-29-2024" class="level3">
<h3 class="anchored" data-anchor-id="october-29-2024">October 29, 2024</h3>
</section>
</section>
<section id="summary-26" class="level2">
<h2 class="anchored" data-anchor-id="summary-26">Summary</h2>
<p>This week, I performed a deep analysis into the qualitative variables available in the WEPP stratified sample, specifically the crops present at each location and the tilling operations performed at that location. As a reminder, the stratified sample consists of three HUC12 watersheds, spread across Iowa, and all the hillslopes recorded within each watershed.</p>
<p>Each crop listed in a hillslope’s management file is associated with a comment that may include a date, either in 1998 or 2013. There is a stark division between the crops labeled 1998 and those labeled 2013. There are also distinctions between the different watersheds, which are displayed through separate graphs. Several questions of how to interpret the crop data remain before it can be merged with the main dataset.</p>
<p>I also performed analysis of the tilling operations data. There are six tilling operations in the sample, with four unique patterns of co-occurence. Notable differences between watersheds are explored through graphs of the operation frequency. This data could be merged with the main dataset now, possibly alongside a date or month variable, since the effect of tilling is time-dependent.</p>
</section>
<section id="wepp-crops-deep-dive" class="level2">
<h2 class="anchored" data-anchor-id="wepp-crops-deep-dive">WEPP Crops Deep Dive</h2>
<p>The original analysis of the crop variables in WEPP considered the full stratified sample drawn in June which contained all hillslopes in three HUC12 watersheds. This sample contained the watershed <span class="math inline">\(071000030704\)</span> with <span class="math inline">\(227\)</span> hillslopes, <span class="math inline">\(070801051103\)</span> with <span class="math inline">\(113\)</span> hillslopes, and <span class="math inline">\(102802010405\)</span> with <span class="math inline">\(133\)</span> hillslopes. Separating the watersheds, we see the distribution of crop variants differs by location.</p>
<p>For location <span class="math inline">\(071000030704\)</span>, we see the following.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-54-1.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-54-2.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
</div>
<p>For location <span class="math inline">\(070801051103\)</span>, we see:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-55-1.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-55-2.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
</div>
<p>For location <span class="math inline">\(102802010405\)</span>, we see:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-56-1.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-56-2.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
</div>
<p>A detailed analysis of the crop data for the stratified sample of HUC12s found a sharp delineation between crops labeled in 1998 and crops labeled in 2013. For crops labeled February 1998, only corn, alfalfa, brome grass, and Tre_2932 are present. The table below displays the frequency of each combination of these crops, where a 1 in the first four columns means that that crop is present and a 0 means it is absent.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Corn</th>
<th>Alfalfa</th>
<th>Brome</th>
<th>Tre2932</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>13</td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>65</td>
</tr>
<tr class="even">
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>356</td>
</tr>
<tr class="odd">
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>31</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>7</td>
</tr>
</tbody>
</table>
<p>For crops labeled April 2013, if they exist they are always a corn variant and a soy variant. The WEPP User Summary adds some clarity to this situation, explaining that “A different type of residue on a field besides the current crop growth being simulated also needs to be assigned a crop number.” In other words, crops marked as present may not actually be growing on the field if the remains from past growth are present.</p>
<p>The logical next step is to merge the crop data with the daily weather and soil loss data, but three major complications exist to implementing this at this point. First, we must decide when each crop type is on the field. A reasonable estimation is that main Crops run from May 1 to October 31 while cover crops run from November 1 to April 30. Second, many hillslopes list two main crops or two cover crops present at the same time. One solution might be to say that if two main crops or two cover crops coexist, they alternate years (e.g.&nbsp;the farmer plants corn one year, soy the next, corn the next, and so on). Lastly, we must decide whether Thre_2932 is a main crop or a cover crop. It never coexists with corn or soy so a main crop may be more likely, but a definitive answer is needed. Alternatively, we can just omit the <span class="math inline">\(14\)</span> hillslopes containing Tre_2932 from our analysis.</p>
</section>
<section id="tilling-operations-analysis" class="level2">
<h2 class="anchored" data-anchor-id="tilling-operations-analysis">Tilling Operations Analysis</h2>
<p>There are six tilling operations found within the stratified WEPP data set. They are listed below, alongside their descriptions as taken from the management files.</p>
<ul>
<li>PLNTFC: Planter, no-till with fluted coulter.</li>
<li>FCSTACDP: Field cultivator, secondary tillage, after duckfoot points. Maximum depth of 10 cm (4 inches).</li>
<li>TAND0002: Tandem disk.</li>
<li>PLDDO: Planter, double disk openers. Tillage depth of 2 inches.</li>
<li>CULTMUSW: Cultivator, row, multiple sweeps per row.</li>
<li>CHISSTSP: A chisel plow, straight with spike points.</li>
</ul>
<p>Some distinct patters emerge in how these tilling operations co-occur. PLNTFC occurs alone in 98 hillslopes. The other five operations occur together in 294 hillslopes. The other five minus CULTMUSW occur in 9 hillslopes. All six occur together in 6 hillslopes. No tilling operations are listed for 66 hillslopes</p>
<p>The results for <span class="math inline">\(071000030704\)</span> are below.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-57-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The results for <span class="math inline">\(070801051103\)</span> are below.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-58-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The results for <span class="math inline">\(102802010405\)</span> are below.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-59-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<section id="october-22-2024" class="level3">
<h3 class="anchored" data-anchor-id="october-22-2024">October 22, 2024</h3>
</section>
</section>
<section id="summary-27" class="level2">
<h2 class="anchored" data-anchor-id="summary-27">Summary</h2>
<p>This week I reviewed a number of papers discussing how to incorporate qualitative variables into GPs. Existing solutions focus on defining a correlation function for qualitative variables, separate from that for quantitative variables. These correlation functions often include a large number of estimated parameters to capture the correlations between factor levels. For this reason, most methods increase the computational complexity of model fitting. How best to fit qualitative variables while minimizing computation time is a less explored question.</p>
<p>I also summarized the crop qualitative variables in the WEPP dataset, and described some of the other qualitative variables in that dataset.</p>
</section>
<section id="qualitative-input-lit-review" class="level2">
<h2 class="anchored" data-anchor-id="qualitative-input-lit-review">Qualitative Input Lit Review</h2>
<p>A literature review of existing work on incorporating qualitative variables into Gaussian process models shows that the existing methods generally approach the issue by defining a kernel (covariance) function for qualitative variables that differs from the kernel function for quantitative variables. This is a contrast to my previous approach, which focused on defining a distance function for qualitative variables that could be fed into the same covariance function as the quantitative variables. The covariance-based approach allows for much greater flexibility in how correlations between levels of a factor are defined, but has some weaknesses of its own.</p>
<p>The methods discussed below derive from three main articles: by X. Deng, C. Devon Lin, et al.&nbsp; by Peter Quian, C. F. Jeff Wu, and Huaiqing Wu. by Peter Quian, Qiang Shu, and Shiyu Zhu.</p>
<p>If your distance function on qualitative variables doesn’t depend on any parameters, then the distance matrix between your observed data points can be calculated once and reused in the GP estimation process, saving much time. This is lost when you have parameters to estimate in your distance matrix as you have to calculate it again every time. However, calculating distance between the levels of a nominal variable is hardly useful without prior knowledge of how the levels differ. The only sensible choice is to let <span class="math inline">\(d(x_j,x_{j'})=0\)</span> when <span class="math inline">\(j=j'\)</span> and <span class="math inline">\(d(x_j,x_{j'})=c\)</span> when <span class="math inline">\(j\neq j'\)</span> for some constant <span class="math inline">\(c\)</span>. This is equivalent to using the Gower dissimilarity as the distance metric.</p>
<p>Alternative distance metric, cosine dissimilarity, is based on encoding qualitative inputs with dummy variables and taking the angle between two normalized vectors representing the qualitative inputs values of two points. This returns a single distance representing the collective difference between two observations across all qualitative variables.</p>
<p>The methods I’ve studied are exchangeable correlation (EC) functions, multiplicative correlation (MC) functions, and hypersphere-based correlation (HC) functions. In EC, MC, and HC, one kernel function is used for quantitative variables and another kernel function is used for qualitative variables. The qualitative kernel functions for EC and MC are <span class="math inline">\(EC: k_Z(z_j,z_{j'})= c\)</span> if <span class="math inline">\(j\neq j'\)</span> where <span class="math inline">\(0&lt;c&lt;1\)</span> and <span class="math inline">\(MC: k_Z(z_j,z_{j'})= \exp(-(\theta_j+\theta_{j'}))\)</span> where <span class="math inline">\(\theta_j,\theta_{j'} &gt;0\)</span> respectively. The HC method projects the correlation matrix for each qualitative variable onto a hypersphere and expresses the corresponding elements as the sum of sines and cosines of estimated parameters.</p>
<p>A major difference between these three methods is the number of parameters they need to estimate. Supposing we have <span class="math inline">\(k\)</span> qualitative inputs with <span class="math inline">\(m_k\)</span> levels each, the EC method has <span class="math inline">\(k\)</span> parameters, the MC method has <span class="math inline">\(\sum_k m_k\)</span> parameters and the HC method has <span class="math inline">\(\sum_k m_k(m_k-1)\)</span> parameters. Also noteworthy is that the HC method can accommodate negative correlations between different levels of a factor, which is possible if, for example, <span class="math inline">\(y=\cos(x)\)</span> given factor level <span class="math inline">\(1\)</span> and <span class="math inline">\(y=-cos(x)\)</span> given factor level <span class="math inline">\(2\)</span>. Given that we are working with a very large dataset, it is of interest to minimize the complexity of the approach we take.</p>
</section>
<section id="wepp-qualitative-variables" class="level2">
<h2 class="anchored" data-anchor-id="wepp-qualitative-variables">WEPP Qualitative Variables</h2>
<p>The WEPP dataset contains a large number of qualitative variables, mostly representing the crops present and the tilling operations performed on each hillslope. There are ten crops present, or five if you combine different variants of corn and soy. The five are alfalfa, bromegrass, corn, soy, and something called ‘Tre_2932’. Multiple crops can be present on a hillslope at the same time and the slopes seem to have been measured twice: once in 1998 and once in 2013. Graphs of the number of different crops found on the same hillslope and of the prevalence of each crop are shown below.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-60-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-60-2.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Most qualitative variables in this dataset have only two levels, but there are a large number of them. It is reasonable to expect interactions between factors and between factors and numeric variables. There may also be negative correlations between some factor levels, though it is hard to predict this one way or another.</p>
</section>
<section id="dirichlet-process-example" class="level2">
<h2 class="anchored" data-anchor-id="dirichlet-process-example">Dirichlet Process Example</h2>
<p>The Dirichlet process is a stochastic process of probability distributions specified by a base distribution <span class="math inline">\(F_0\)</span> and a concentration parameter <span class="math inline">\(\alpha\)</span>. When used as a Bayesian model to estimate the distribution generating data, <span class="math inline">\(F_0\)</span> is our prior guess at the distribution and <span class="math inline">\(\alpha\)</span> controls the relative weight of the prior versus the data. When the number of data points is much greater than <span class="math inline">\(\alpha\)</span>, the posterior approaches the empirical CDF of the data. We do not directly calculate the posterior though, but sample from it by generating a Dirichlet process of posterior CDFs and averaging them together.</p>
<p>Given <span class="math inline">\(30\)</span> draws from a <span class="math inline">\(N(3,1)\)</span> distribution and a <span class="math inline">\(N(1,1)\)</span> prior, I estimated the data distribution with a Dirichlet process with <span class="math inline">\(50\)</span> runs, obtaining the results shown below.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot1-10-22-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Since our prior distribution is light-tailed, there is not enough data at the extremes to overrule the prior. This can be mitigated by choosing a heavy-tailed prior, such as a t-distribution with <span class="math inline">\(df=2\)</span> and <span class="math inline">\(ncp=1\)</span>. The estimation result is shown below.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot2-10-22-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="october-15-2024" class="level3">
<h3 class="anchored" data-anchor-id="october-15-2024">October 15, 2024</h3>
</section>
</section>
<section id="summary-28" class="level2">
<h2 class="anchored" data-anchor-id="summary-28">Summary</h2>
<p>This week, I explored several papers discussing methods of handling mixed data in GPs. These methods centered around defining covariance functions for the qualitative variables, rather than using the same covariance function for qualitative and quantitative variables and defining separate distance functions.</p>
<p>I tried clustering data with the PAM (Partitioning Around Medoids) method, specifying more or fewer clusters than optimal. Then I used a local expert method, fitting a GP to the data in each cluster, to train a model and make inference on an out-of-sample set.</p>
<p>I also updated the simulation study of a GP with an interaction term, using the same input data each time and comparing models with interaction terms to models without. There was not a consistent relationship between the relevance of the interaction term and the size of the interaction component of the response.</p>
</section>
<section id="additive-gp-with-mixed-data" class="level2">
<h2 class="anchored" data-anchor-id="additive-gp-with-mixed-data">Additive GP with Mixed Data</h2>
<p>I studied the additive GP model with mixed data proposed by . Starting with a response variable <span class="math inline">\(Y\)</span> based on quantitative variables <span class="math inline">\(X_j\)</span>, <span class="math inline">\(j=1,\dots p,\)</span>, and qualitative variables <span class="math inline">\(Z_k\)</span>, <span class="math inline">\(k=1,\dots,q\)</span>, they define the model as a sum of Gaussian processes. <span class="math display">\[Y(X,Z)=\mu+ GP_1(Z_1,X) +\dots +GP_q(Z_q,X)\]</span> so <span class="math inline">\(q\)</span> GPs are fit to <span class="math inline">\(Y\)</span>, each including one qualitative variable and all the quantitative variables. Then each GP follows a MVN distribution <span class="math inline">\(N(0,\Sigma)\)</span>, where the kernel function of <span class="math inline">\(\Sigma\)</span> given two inputs <span class="math inline">\(\textbf{w}_1= (\textbf{x}_1, \textbf{z}_1)\)</span> and <span class="math inline">\(\textbf{w}_2=(\textbf{x}_2, \textbf{z}_2)\)</span> is <span class="math display">\[K(\textbf{w}_1, \textbf{w}_2)= \tau^2 k_X(\textbf{x}_1, \textbf{x}_2|\theta) \prod_{k=1}^q  k_{Z_q}(\textbf{z}_{1q}, \textbf{z}_{2q}|\theta)\]</span> where <span class="math inline">\(k_X(\cdot)\)</span> represents the covariance function between the quantitative variables and <span class="math inline">\(k_{Z_q}(\cdot)\)</span> represents the covariance function for the qth qualitative variable.</p>
<p>The authors choose to use separate covariance or kernel functions, not just separate distance functions like I had. Since the overall kernel function <span class="math inline">\(K\)</span> is the product of the individual covariance functions, a value of zero for any of them could cause the entire covariance to become zero. Thus, they opted for an additive model that splits the qualitative variables among an equal number of GPs.</p>
<p>For the covariance function of a qualitative variable, the authors proposed three options. First, let <span class="math inline">\(k_{Z_q}(\textbf{z}_{iq}, \textbf{z}_{jq}|\theta_z)=\theta_z\)</span>, <span class="math inline">\(0&lt;\theta_z&lt;1\)</span> for <span class="math inline">\(i\neq j\)</span>, called the exchangeable method. Second <span class="math inline">\(k_{Z_q}(\textbf{z}_{iq}, \textbf{z}_{jq}|\theta)=\exp(-(\theta_{iq}+ \theta_{jq}))\)</span>, <span class="math inline">\(\theta_{iq}&gt;0, \theta_{jq}&gt;0\)</span> for <span class="math inline">\(i\neq j\)</span>, called the multiplicative method. Third, a covariance function based on a hypersphere decomposition of the covariance matrix for each factor.</p>
<p>The multiplicative method is most similar to my previous approach of using the same kernel but different distance functions for qualitative and quantitative variables. However, the third method is preferred by the authors as the show that it has a higher RMSE on average. Nevertheless, I think the second method or a variation of it will be most suitable for the WEPP project. The hypersphere method favored by the authors has many parameters and needs a lot of computation time, which runs against our interests, while the exchangeable method assumes different levels of a factor always have the same ‘distance’ between them, which may not be justified.</p>
</section>
<section id="gp-with-gowerpam-clustering" class="level2">
<h2 class="anchored" data-anchor-id="gp-with-gowerpam-clustering">GP with Gower/PAM Clustering</h2>
<p>I tried clustering our previous training dataset with a 3-level factor using the Partitioning around Medoids method. When I specified <span class="math inline">\(3\)</span> clusters, it perfectly captured the three levels of the factor input, so I also clustered the data into <span class="math inline">\(2\)</span> and <span class="math inline">\(4\)</span> clusters. For each set of clusters, I fit a GP to each cluster and predicted on the corresponding cluster in the training data.</p>
<p>For two clusters, the estimated lengthscales for the continuous input were <span class="math inline">\(1.33\)</span> and <span class="math inline">\(1.72\)</span> respectively for clusters 1 and 2. The cluster structure relative to the factor input is shown in this table.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Factor</th>
<th style="text-align: center;">C1</th>
<th style="text-align: center;">C2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">300</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;">167</td>
<td style="text-align: center;">133</td>
</tr>
</tbody>
</table>
<p>The RMSE of this model on the test set was <span class="math inline">\(9.91\)</span> and its estimated functions with <span class="math inline">\(95\%\)</span> confidence bands are plotted below. As indicated in the table, factor level 3 is treated the same as 1 for values below zero and the same as 2 for values roughly above zero.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot1-10-15-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>For four clusters, the estimated lengthscales for the continuous input were <span class="math inline">\(1.60\)</span>, <span class="math inline">\(1.38\)</span>, <span class="math inline">\(1.04\)</span>, and <span class="math inline">\(2.54\)</span> respectively for clusters 1 to 4. The cluster structure relative to the factor input is shown in this table.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Factor</th>
<th style="text-align: center;">C1</th>
<th style="text-align: center;">C2</th>
<th style="text-align: center;">C3</th>
<th style="text-align: center;">C4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">180</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">300</td>
</tr>
</tbody>
</table>
<p>The RMSE of this model on the test set was <span class="math inline">\(9.06\)</span> and its estimated functions with <span class="math inline">\(95\%\)</span> confidence bands are plotted below. This method succeeded in estimating the functions fairly well, though the uncertainty on Factor 2 varies according to what cluster it is in.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot2-10-15-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="october-8-2024" class="level3">
<h3 class="anchored" data-anchor-id="october-8-2024">October 8, 2024</h3>
</section>
</section>
<section id="summary-29" class="level2">
<h2 class="anchored" data-anchor-id="summary-29">Summary</h2>
<p>This week, I tried clustering our previous datasets with the PAM (Partitioning Around Medoids) method. I was able to perfectly retrieve the original cluster structure of the factor input for both the one-factor and the two-factor datasets.</p>
<p>I also tried generating a dataset with two continuous inputs and fitting a GP using just those two inputs and again with a third input that was the product of the other two. I repeated this with several datasets with different amounts of interaction between the inputs. My hope was that the relevance of the interaction parameter (represented by the lengthscale) would increase as the amount of interaction increased. This occurred, but the relative relevance of the interaction term was not consistent, hampering interpretability. A new approach may be needed.</p>
<p>Lastly, I finished coding a GP model with a custom distance function. I fit a GP with the old single-factor dataset from September 17 using the Gower distance (Euclidean distance for numeric variables and 0-1 distance for categorical variables). I then predicted on the test set of evenly spaced numeric values. The model performed very well, even better than the NLE model with a GP fit to each factor level independently.</p>
<section id="gp-with-interaction-variable" class="level3">
<h3 class="anchored" data-anchor-id="gp-with-interaction-variable">GP with Interaction Variable</h3>
<p>To test how a Gaussian process handles interactions between inputs, I generated a dataset based on two continuous variables. I fit a GP to the data using the following kernel function.</p>
<p><span class="math display">\[K(x,x')= \exp\left(-\left[\frac{d(x_1,x_1')}{\theta_1} + \frac{d(x_2,x_2')}{\theta_2} +\frac{d(x_1x_2,x_1'x_2')}{\theta_3}\right] \right)\]</span> In practice, all I needed to do was add a third variable that was the product of the first two. I then scaled all three inputs, to make their lengthscales comparable. The response depended on the inputs by the equation <span class="math display">\[y_i = 1.3x_{1i}+1.8x_{2i}+\alpha\exp(x_{1i}+x_{2i})+\epsilon_i\]</span> where <span class="math inline">\(\epsilon_i~N(0,0.1)\)</span>. I generated five such datasets with <span class="math inline">\(\alpha\)</span> equal to <span class="math inline">\(0.25\)</span>, <span class="math inline">\(0.5\)</span>, <span class="math inline">\(1\)</span>, <span class="math inline">\(1.5\)</span> and <span class="math inline">\(2\)</span>. The MLE parameter estimates of these models were as follows:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Interact</th>
<th style="text-align: center;">Alpha</th>
<th style="text-align: center;">X1</th>
<th style="text-align: center;">X2</th>
<th style="text-align: center;">X1X2</th>
<th style="text-align: center;">Nugget</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Y</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">0.0001</td>
</tr>
<tr class="even">
<td style="text-align: center;">N</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">–</td>
<td style="text-align: center;">0.0001</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Y</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">0.0001</td>
</tr>
<tr class="even">
<td style="text-align: center;">N</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">7.36</td>
<td style="text-align: center;">–</td>
<td style="text-align: center;">0.0001</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Y</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">0.0001</td>
</tr>
<tr class="even">
<td style="text-align: center;">N</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">6.72</td>
<td style="text-align: center;">–</td>
<td style="text-align: center;">0.0001</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Y</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">0.0001</td>
</tr>
<tr class="even">
<td style="text-align: center;">N</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">6.60</td>
<td style="text-align: center;">–</td>
<td style="text-align: center;">0.0001</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Y</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">0.0001</td>
</tr>
<tr class="even">
<td style="text-align: center;">N</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">6.56</td>
<td style="text-align: center;">–</td>
<td style="text-align: center;">0.0001</td>
</tr>
</tbody>
</table>
<p>The lengthscale of the interaction term does decrease as <span class="math inline">\(\alpha\)</span> increases, suggesting increasing relevance for that input. However, the lengthscales of the other inputs do not follow a consistent pattern, so it is difficult to judge the relevance of the interaction term by comparing it to the other terms. An interaction term in the kernel function thus may not be an effective way of measuring variable interaction within a GP.</p>
</section>
<section id="gp-with-gower-dist" class="level3">
<h3 class="anchored" data-anchor-id="gp-with-gower-dist">GP with Gower Dist</h3>
<p>I fit a custom Gaussian process model to a subset of the single-factor dataset with two levels of the factor similar, composed of <span class="math inline">\(100\)</span> observations drawn from each factor level for a total of <span class="math inline">\(300\)</span> observations. This GP used the Gower dissimilarity to quantify the difference between observations on the categorical input. If two observations matched on the factor input the distance was zero, if they differed the distance was one.</p>
<p>The lengthscales are <span class="math inline">\(37.39\)</span> and <span class="math inline">\(21.00\)</span> for the numeric and factor inputs respectively, and the nugget was <span class="math inline">\(0.001\)</span>. The RMSE for this model was <span class="math inline">\(2.20\)</span>, far better than any other method. The <span class="math inline">\(95\%\)</span> confidence bands for the predicted functions are plotted below.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot1-10-8-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="october-1-2024" class="level3">
<h3 class="anchored" data-anchor-id="october-1-2024">October 1, 2024</h3>
</section>
</section>
<section id="summary-30" class="level2">
<h2 class="anchored" data-anchor-id="summary-30">Summary</h2>
<p>This week I considered how we might design interpretable interactions between covariates, based on a remark by Dr.&nbsp;Niemi. This could be done either as part of the clustering process in a local experts method or as a modification of the Gaussian process model. I show that the Gaussian process allows for interactions between terms, but does not have any clear interpretation of them.</p>
<p>I also tested my best performing methods for GPs with categorical inputs on a new dataset with two categorical inputs instead of one. The linear mean function method suffered, as the data did not follow a consistent pattern even with the factor effects removed, but the NLE method stayed effective.</p>
<p>I worked on developing a GP function that calls a customizable distance function, but was not able to get it to run consistently in time for this meeting.</p>
</section>
<section id="covariate-interactions-in-a-gp" class="level2">
<h2 class="anchored" data-anchor-id="covariate-interactions-in-a-gp">Covariate Interactions in a GP</h2>
<p>Dr.&nbsp;Niemi mentioned last week that he was interested in whether there was a way to make use of the fact that the similarity between levels of the factor input varied according to the value of the numeric input in last week’s dataset. I considered two ways to approach that question, first as a question of clustering/partitioning and second as a question of model fitting.</p>
<p>Currently I treat each level of the categorical input as a cluster, with no involvement from the numeric input, but this need not be the case. Clustering algorithms such as those described by Costa, Papatsouma, and Markos (see Sep.&nbsp;17) partition the data based on all inputs, allowing for interactions between them. If we want to study or visualize the interactions, we could cluster in two steps: first partition the data based on the numeric inputs, then partition each cluster based on the categorical inputs. Alternatively, we could cluster the data by the numeric inputs, then cluster it again by the categorical inputs and see how the two sets of clusters overlap.</p>
<p>Another approach is to fit a Gaussian process model and study how its inputs interact within the fitted model. This is hard to do in practice because while a GP accommodates interaction between inputs with a squared exponential kernel, there is no clean way to visualize interactions between them. The expected value of a new observation is based on the correlations between it and each other observation. Changing one input will change these correlations, and the expected value, to a degree that depends on the value of the other inputs.</p>
<p>To demonstrate this, consider a squared exponential Gaussian process fit to a dataset with inputs <span class="math inline">\(X_i=(x_{i,con},x_{i,cat}), i=1,\dots,n\)</span> and response <span class="math inline">\(Y\)</span>, where <span class="math inline">\(x_{con}\)</span> is a continuous variable and <span class="math inline">\(x_{cat}\)</span> is a categorical variable. The covariance between two points <span class="math display">\[K(X_1, X_2)=\exp\left(-\left[\frac{||x_{1,con}-x_{2,con}||^2}{\theta_{con}} + \frac{||x_{1,cat}-x_{2,cat}||^2}{\theta_{cat}} \right] \right)\]</span> <span class="math display">\[=\exp\left(-\frac{||x_{1,con}-x_{2,con}||^2}{\theta_{con}} \right) \exp\left( -\frac{||x_{1,cat}-x_{2,cat}||^2}{\theta_{cat}} \right) =k_{con}(X_1,X_2)k_{cat}(X_1,X_2)\]</span></p>
<p>The covariance is then a product of functions of the squared distances between inputs. The size of the change in covariance resulting from a change in one input will thus depend on the size of the contributions of the other inputs.</p>
<p>As is, the GP provides no insight on the general nature interactions between its inputs. However, it may be possible to modify the model to draw out those interactions. Adding an interaction term inside the kernel function (or distance function) could be a way to achieve this.</p>
</section>
<section id="gp-with-two-factor-inputs" class="level2">
<h2 class="anchored" data-anchor-id="gp-with-two-factor-inputs">GP with Two Factor Inputs</h2>
<p>To extend my previous research into categorical methods, I constructed a new dataset with two factor inputs each with two levels and one numeric input and generated a training set of 800 observations and a test set of 200. I applied three methods of addressing categorical inputs with GPs: to ignore them (Method 1), to partition the data based on the factors and fit a different GP to each combination of levels (Method 2), and to fit a linear model using the factor inputs and then fit a GP using the residuals of the linear model (Method 3). The training data is plotted below.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot1-10-1-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>For Method 1, the prediction RMSE was <span class="math inline">\(13.7\)</span>. The maximum likelihood estimates of the parameters were <span class="math inline">\(107.66\)</span> for the lengthscale of x1 and <span class="math inline">\(0.223\)</span> for the nugget. For Method 2, the prediction RMSE was <span class="math inline">\(0.72\)</span>. The maximum likelihood estimates of the parameters were <span class="math inline">\(2.80\)</span>, <span class="math inline">\(3.40\)</span>, <span class="math inline">\(1.32\)</span>, and <span class="math inline">\(1.94\)</span> for the lengthscales of x1 given <span class="math inline">\((x2,x3) = \{(1,1), (1,2), (2,1), (2,2)\}\)</span>. For Method 3, the prediction RMSE was <span class="math inline">\(11.01\)</span>. Its lengthscale MSE was <span class="math inline">\(110.80\)</span> and its nugget was <span class="math inline">\(0.203\)</span>. The predicted vs residual plots for each method looked normal, and the estimated functions with <span class="math inline">\(95\%\)</span> confidence bands are plotted below.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot2-10-1-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot3-10-1-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot4-10-1-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="september-24-2024" class="level3">
<h3 class="anchored" data-anchor-id="september-24-2024">September 24, 2024</h3>
</section>
</section>
<section id="summary-31" class="level2">
<h2 class="anchored" data-anchor-id="summary-31">Summary</h2>
<p>This week I researched current ways of using functional inputs for clustering. There are two basic strategies I’ve found: to use a distance metric between two functions, or to convert the functions into linear combinations of basis functions and use the covariates for clustering.</p>
<p>I also prepared a new method of fitting a GP with a categorical input by first fitting a linear model to the categorical input to simulate a mean function in the GP. I then compared all these methods on a new dataset that had two factor levels similar and the third different. The ranking of the different methods didn’t change, though the new mean function method performed best. I also plotted <span class="math inline">\(95\%\)</span> confidence bands of the estimated functions.</p>
</section>
<section id="distances-for-functional-data" class="level2">
<h2 class="anchored" data-anchor-id="distances-for-functional-data">Distances for Functional Data</h2>
<p>There are a number of ways of handling functional inputs for clustering or regression purposes. One is to use a distance metric between two curves such as the Frechet distance as a basis for clustering. Another is to convert the functional input into a linear combination of basis functions or principal components and use the linear coefficients as inputs.</p>
<p> suggest a clustering method using Frechet distance and show that it is competitive for functional inputs that are irregularly measured. propose a variation of basis function-based dimension reduction for cases when the response is categorical where the basis functions are chosen adaptively to maximize classification accuracy and are restricted to piecewise linear functions to improve their interpretability.</p>
</section>
<section id="gp-with-categorical-mean-fn" class="level2">
<h2 class="anchored" data-anchor-id="gp-with-categorical-mean-fn">GP with Categorical Mean Fn</h2>
<p>As a fourth method to handle categorical inputs, I tried fitting a GP with a categorical mean function by first fitting a linear model with the factor variable as the predictor. Then I fit a GP using the residuals of the linear model as my response. The RMSE of this method was <span class="math inline">\(3.15\)</span>. The first plot below shows the predicted values versus the residuals while the second shows the <span class="math inline">\(95\%\)</span> confidence bands for the estimated function for each level.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot1-9-24-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot2-9-24-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="categorical-data-with-similar-levels" class="level2">
<h2 class="anchored" data-anchor-id="categorical-data-with-similar-levels">Categorical Data with Similar Levels</h2>
<p>I repeated the four methods described before on a new dataset where levels 1 and 2 of the factor were much closer together while level 3 was different. The new training data is plotted below.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot3-9-24-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>To fit a model and perform inference, I compared the four methods used on the last dataset. First, simply ignoring the categorical variable (Method 1). Second, partitioning the data set by the categorical input and fitting a different GP to each section (Method 2). Third, encoding the categorical input as dummy variables and including those in the GP (Method 3). Fourth, fitting a linear model with the factor input followed by a GP with the continuous input (Method 4).</p>
<p>For Method 1, the prediction RMSE was <span class="math inline">\(14.49\)</span>. The maximum likelihood estimates of the parameters were <span class="math inline">\(3.74\)</span> for the lengthscale of x1 and <span class="math inline">\(0.288\)</span> for the nugget. For Method 2, the prediction RMSE was <span class="math inline">\(9.09\)</span>. The maximum likelihood estimates of the parameters were <span class="math inline">\(1.60\)</span>, <span class="math inline">\(1.28\)</span>, and <span class="math inline">\(2.54\)</span> for the lengthscales of x1 given <span class="math inline">\(x2 = {1,2,3}\)</span>. For Method 3, the prediction RMSE was <span class="math inline">\(16.98\)</span>. I was not able to get MLE estimates for its parameters. For Method 4, the prediction RMSE was <span class="math inline">\(6.67\)</span> and the MLE of its lengthscale was <span class="math inline">\(1.29\)</span>.</p>
<p>For each method, the predicted versus residual plot on the test data is shown alongside the <span class="math inline">\(95\%\)</span> confidence band of the predicted function. Note that though the residuals for Methods 2 and 3 look to be strictly positive, the median residual for Method 2 is <span class="math inline">\(0.04\)</span> and the median residual for Method 3 is <span class="math inline">\(1.01\)</span>. They simply have large positive errors that obscure the distribution of errors.</p>
<div style="page-break-after: always;"></div>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot4-9-24-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot5-9-24-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot6-9-24-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot7-9-24-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot8-9-24-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot9-9-24-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot10-9-24-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot11-9-24-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div style="page-break-after: always;"></div>
</section>
<section id="september-17-2024" class="level2">
<h2 class="anchored" data-anchor-id="september-17-2024">September 17, 2024</h2>
<section id="summary-32" class="level3">
<h3 class="anchored" data-anchor-id="summary-32">Summary</h3>
<p>This week I explored possible methods of incorporating categorical variables into a Gaussian process. I discovered several existing dissimilarity metrics for categorical variables that are in use for cluster analysis, including Gower’s, HL, and cosine dissimilarity. Gower’s dissimilarity returns the range-normalized Manhattan distance for continuous inputs and a 0 for a match and 1 otherwise for categorical inputs. HL (Hennig-Laou) dissimilarity is a modification of Gower’s that scales dummy-encoded categorical inputs to have roughly unit variance. Cosine dissimilarity is the dot product between two normalized vectors. In the paper, it is only applied to dummy-encoded categorical inputs, while continuous inputs are still treated with the Euclidean distance.</p>
<p>I also compared three possible methods for handling categorical variables when fitting a GP: first, to simply ignore categorical inputs, second to partition the data before fitting models, and third to encode the categorical data as numeric dummy variables. The second method performed best, though I am still working on incorporating custom distance metrics and mean functions into a Gaussian process, which must be coded by hand since I couldn’t find an R package with that much flexibility.</p>
</section>
<section id="clustering-mixed-data" class="level3">
<h3 class="anchored" data-anchor-id="clustering-mixed-data">Clustering Mixed Data</h3>
<p>A useful paper by described several clustering methods for data including both numeric and categorical variables (mixed data). Eight methods are described, all of which rely on distance or dissimilarity measures. Gower’s dissimilarity is one such measure, that encodes categorical distances as 0 if the levels are the same and 1 if they’re different. Another is proposed by Hennig and Laou, that modifies Gower’s method with a weighting scheme that controls variable importance.</p>
<p>The first two methods are Gower/PAM and HL/PAM, since they start with one of the dissimilarity measures described and pair it with a “Partitioning around Medoids” method that finds a set of points that minimize the average dissimilarity within clusters.</p>
<p>K-Prototypes seeks to minimize the trace of the within cluster dispersion matrix cost function. It relies on the mode of each categorical input and does not weight categorical distances, which inspired Ahmad and Dey to propose a Mixed K-Means method that calculates categorical distances based on the co-occurrence of categorical values. Modha-Spengler K-Means is related, but uses the cosine dissimilarity for categorical variables.</p>
<p>Other approaches to cluster analysis seek to reduce dimensionality before searching for clusters. One example is Factor Analysis for Mixed Data (FAMD), which is a compromise between principal component analysis and multiple correspondence analysis. When FAMD is followed by a K-Means clustering approach, the method is called FAMD/K-Means. Other authors suggest performing dimensionality reduction and K-means clustering simultaneously, called Mixed Reduced K-Means, where mixed refers to the presence of mixed data.</p>
</section>
<section id="gp-with-categorical-data" class="level3">
<h3 class="anchored" data-anchor-id="gp-with-categorical-data">GP with Categorical Data</h3>
<p>First, I simulated a dataset consisting of one continuous variable x1 and one categorical variable x2. The categorical variable had three levels: <span class="math inline">\({1,2,3}\)</span>. The response was generated using a function proposed by Han, et al, where a different quadratic function of the continuous variable is used for each level of the categorical variable. I generated a training dataset with <span class="math inline">\(900\)</span> observations, and a test dataset with 93. The training data is plotted below.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot4-9-17-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>To fit a model and perform inference, I tried three methods. First, simply ignoring the categorical variable (Method 1). Second, partitioning the data set by the categorical input and fitting a different GP to each section (Method 2). Third, encoding the categorical input as dummy variables and including those in the GP (Method 3).</p>
<p>For Method 1, the prediction RMSE was <span class="math inline">\(8.45\)</span>. The maximum likelihood estimates of the parameters were <span class="math inline">\(1.82\)</span> for the lengthscale of x1 and <span class="math inline">\(0.061\)</span> for the nugget. For Method 2, the prediction RMSE was <span class="math inline">\(8.60\)</span>. The maximum likelihood estimates of the parameters were <span class="math inline">\(0.86\)</span>, <span class="math inline">\(2.52\)</span>, and <span class="math inline">\(3.11\)</span> for the lengthscales of x1 given <span class="math inline">\(x2 = {1,2,3}\)</span>. For Method 3, the prediction RMSE was <span class="math inline">\(16.51\)</span>. I was not able to get MLE estimates for its parameters.</p>
<p>Plots of the predicted values versus residuals are shown below for all three methods.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot1-9-17-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot2-9-17-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="../WEPP/Reports/plot3-9-17-24.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>