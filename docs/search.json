[
  {
    "objectID": "ref.html",
    "href": "ref.html",
    "title": "References",
    "section": "",
    "text": "M. Blei, D., & I. Frazier, P. (2010). Distance Dependent Chinese Restaurant Processes. Journal of Machine Learning Research. https://jmlr.org/papers/volume12/blei11a/blei11a.pdf\nRasmussen, C. E., & Ghahramani, Z. (2001). Infinite mixtures of Gaussian process experts. Neural Information Processing Systems, 14, 881–888. http://learning.eng.cam.ac.uk/zoubin/papers/iMGPE.pdf\nRuth, W. (2024). A Review of Monte Carlo-based Versions of the EM Algorithm. https://arxiv.org/pdf/2401.00945\nLawrence, N. D. (2021). Deep Gaussian Processes: A Motivation and Introduction. https://inverseprobability.com/talks/notes/deep-gaussian-processes-a-motivation-and-introduction.html\nFoti, N., & Williamson, S. (2015). A Survey of Non-exchangeable Priors for Bayesian Nonparametric Models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 37, no. 2, 359-371 https://www.researchgate.net/publication/233720862_A_Survey_of_Non-Exchangeable_Priors_for_Bayesian_Nonparametric_Models\nSun, S., & Xu, X. (2011). Variational Inference for Infinite Mixtures of Gaussian Processes With Applications to Traffic Flow Prediction. IEEE Transactions on Intelligent Transportation Systems, 12, no. 2, 466-475, doi: 10.1109/TITS.2010.2093575.\nGorur, D., Rasmussen, C. E., (2009). Dirichlet Process Gaussian Mixture Models: Choice of the Base Distribution. Journal of Computer Science and Technology, 25(4), 615-626. https://mlg.eng.cam.ac.uk/pub/pdf/GoeRas10.pdf\nGelfand, A. E., Kottas, A., & MacEachern, S. N. (2005). Bayesian Nonparametric Spatial Modeling With Dirichlet Process Mixing. Journal of the American Statistical Association, 100(471), 1021–1035. https://doi.org/10.1198/016214504000002078"
  },
  {
    "objectID": "imgpe.html",
    "href": "imgpe.html",
    "title": "The iMGPE Algorithm",
    "section": "",
    "text": "The basic data model of the Infinite Mixture of Gaussian Process Experts model is as follows. We have an \\(n\\times 1\\) continuous response vector \\(y\\) and an \\(n\\times d\\) data matrix \\(X\\). The estimated value of a data point \\(y_i\\) under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing \\(y_i\\) and weighted by a Dirichlet process. Let \\(z\\) represent a possible vector of cluster assignments and let \\(z^{(k)}\\) be the \\(k^{th}\\) element of some ordered list of all possible \\(z\\). Then \\(j=1,\\dots,J_k\\) index the clusters within \\(z^{(k)}\\). Let \\(C_j^{(k)}\\) be the number of observations in cluster \\(j\\) given assignment \\(z^{(k)}\\). Then we have as follows.\n\\[y\\sim \\sum_{k=1}^{p(n)} \\left[\\prod_{j=1}^{J_k} N_{C_j^{(k)}}(0,\\Sigma_{\\theta_j}) \\right] w_k\\] \\[w_k=P(z=z^{(k)}|\\alpha,\\phi)\\] where the \\(w_z\\) are the marginal probabilities of a modified Chinese Restaurant Process that generates cluster assignments \\(z\\) and \\(p(n)\\) is the partition function. The CRP used here has been modified to depend on the input data \\(X\\). It is controlled by two parameters, \\(\\alpha\\) and \\(\\phi\\), the first being the usual concentration parameter and the second controlling the cluster occupancy estimates. A more in-depth explanation of this CRP is provided in the iMGPE Algorithm section from last week. It should also be noted that there is no known closed form for \\(w_k\\). The closest I could find was the multivariate Ewen’s distribution, which describes the distribution on the set of \\(p(n)\\) that arises from a regular Chinese Restaurant Process. However, it only accounts for the number of clusters of different sizes, and does not serve as a marginal distribution of \\(z\\).\nThe full joint distribution, including the priors for all parameters, is as follows. Here, \\(\\Phi\\) is the pdf of a normal distribution.\n\\[p(y,\\theta,\\phi,\\alpha)=\\left[ \\sum_{k=1}^{p(n)} \\left[\\prod_{j=1}^{J_k} \\Phi_{C_j^{(k)}}(0,\\Sigma_{\\theta_j}) \\right] p(z(k)|\\alpha,\\phi)\\right] p(\\theta)p(\\phi)p(\\alpha)\\]\nAlternatively, the model can be expressed in hierarchical terms where \\(J_z\\) is the number of clusters in \\(z\\) and \\(C_{j,z}\\) is the number of observations in the \\(j^{th}\\) cluster in \\(z\\).\n\\[y|z\\sim \\prod_{j=1}^{J_z} N_{C_{j,z}}(0,\\Sigma_{\\theta_{j}})\\] \\[z|\\alpha,\\phi \\sim MCRP(\\alpha,\\phi)\\]\nHere, \\(N_{C_j^{(z)}}(0,\\Sigma_{\\theta_j})\\) is the \\(C_j^{(z)}\\)-dimensional multivariate normal distribution with covariance matrix \\(\\Sigma_{\\theta_j}\\) defined by a Gaussian kernel function with parameters \\(\\theta_j\\).\nThen \\(\\theta_j\\) is the parameter vector for the GP expert assigned to cluster \\(j\\), while \\(\\alpha\\) is the CRP concentration parameter and \\(\\phi\\) is the parameter vector for the CRP’s occupation number estimate. Note that \\(\\phi\\) is purely a vector of lengthscales for a Gaussian kernel. The priors on \\(\\theta\\), \\(\\alpha\\), and \\(\\phi\\) are described below.\n\\[\\theta_{j,k}\\stackrel{ind}{\\sim} Gamma(a_k,b_k) \\text{ for } k=1,\\dots,d\\] \\[\\alpha\\sim Inv.Gam(1,1),\\text{   } \\phi_k\\stackrel{iid}{\\sim} LogN(0,1) \\text{ for } k=1,\\dots,d\\] That is, each element \\(k\\) of \\(\\theta_j\\) ( where \\(d=\\dim(X)+1\\) counting the lengthscales and a noise parameter) is assigned a independent Gamma prior with fixed parameters \\(a_k\\) and \\(b_k\\). Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of \\(\\phi\\) receives an independent log-normal prior.\nA third visualization of the model structure is a directed acyclic graph, shown below. Starting with the priors for \\(\\alpha\\), \\(\\phi\\), and \\(\\theta\\), we can draw their values and generate our latent variables \\(z\\) and our true variables \\(y\\). Note that \\(\\theta\\) depends on \\(z\\) as well as its prior, as \\(z\\) defines the number of clusters and thus the number of GP parameters to be drawn. The red and blue boxes indicate the quantities which are drawn multiple times for each of the \\(n\\) data points or each of the \\(J\\) clusters."
  },
  {
    "objectID": "imgpe.html#data-model",
    "href": "imgpe.html#data-model",
    "title": "The iMGPE Algorithm",
    "section": "",
    "text": "The basic data model of the Infinite Mixture of Gaussian Process Experts model is as follows. We have an \\(n\\times 1\\) continuous response vector \\(y\\) and an \\(n\\times d\\) data matrix \\(X\\). The estimated value of a data point \\(y_i\\) under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing \\(y_i\\) and weighted by a Dirichlet process. Let \\(z\\) represent a possible vector of cluster assignments and let \\(z^{(k)}\\) be the \\(k^{th}\\) element of some ordered list of all possible \\(z\\). Then \\(j=1,\\dots,J_k\\) index the clusters within \\(z^{(k)}\\). Let \\(C_j^{(k)}\\) be the number of observations in cluster \\(j\\) given assignment \\(z^{(k)}\\). Then we have as follows.\n\\[y\\sim \\sum_{k=1}^{p(n)} \\left[\\prod_{j=1}^{J_k} N_{C_j^{(k)}}(0,\\Sigma_{\\theta_j}) \\right] w_k\\] \\[w_k=P(z=z^{(k)}|\\alpha,\\phi)\\] where the \\(w_z\\) are the marginal probabilities of a modified Chinese Restaurant Process that generates cluster assignments \\(z\\) and \\(p(n)\\) is the partition function. The CRP used here has been modified to depend on the input data \\(X\\). It is controlled by two parameters, \\(\\alpha\\) and \\(\\phi\\), the first being the usual concentration parameter and the second controlling the cluster occupancy estimates. A more in-depth explanation of this CRP is provided in the iMGPE Algorithm section from last week. It should also be noted that there is no known closed form for \\(w_k\\). The closest I could find was the multivariate Ewen’s distribution, which describes the distribution on the set of \\(p(n)\\) that arises from a regular Chinese Restaurant Process. However, it only accounts for the number of clusters of different sizes, and does not serve as a marginal distribution of \\(z\\).\nThe full joint distribution, including the priors for all parameters, is as follows. Here, \\(\\Phi\\) is the pdf of a normal distribution.\n\\[p(y,\\theta,\\phi,\\alpha)=\\left[ \\sum_{k=1}^{p(n)} \\left[\\prod_{j=1}^{J_k} \\Phi_{C_j^{(k)}}(0,\\Sigma_{\\theta_j}) \\right] p(z(k)|\\alpha,\\phi)\\right] p(\\theta)p(\\phi)p(\\alpha)\\]\nAlternatively, the model can be expressed in hierarchical terms where \\(J_z\\) is the number of clusters in \\(z\\) and \\(C_{j,z}\\) is the number of observations in the \\(j^{th}\\) cluster in \\(z\\).\n\\[y|z\\sim \\prod_{j=1}^{J_z} N_{C_{j,z}}(0,\\Sigma_{\\theta_{j}})\\] \\[z|\\alpha,\\phi \\sim MCRP(\\alpha,\\phi)\\]\nHere, \\(N_{C_j^{(z)}}(0,\\Sigma_{\\theta_j})\\) is the \\(C_j^{(z)}\\)-dimensional multivariate normal distribution with covariance matrix \\(\\Sigma_{\\theta_j}\\) defined by a Gaussian kernel function with parameters \\(\\theta_j\\).\nThen \\(\\theta_j\\) is the parameter vector for the GP expert assigned to cluster \\(j\\), while \\(\\alpha\\) is the CRP concentration parameter and \\(\\phi\\) is the parameter vector for the CRP’s occupation number estimate. Note that \\(\\phi\\) is purely a vector of lengthscales for a Gaussian kernel. The priors on \\(\\theta\\), \\(\\alpha\\), and \\(\\phi\\) are described below.\n\\[\\theta_{j,k}\\stackrel{ind}{\\sim} Gamma(a_k,b_k) \\text{ for } k=1,\\dots,d\\] \\[\\alpha\\sim Inv.Gam(1,1),\\text{   } \\phi_k\\stackrel{iid}{\\sim} LogN(0,1) \\text{ for } k=1,\\dots,d\\] That is, each element \\(k\\) of \\(\\theta_j\\) ( where \\(d=\\dim(X)+1\\) counting the lengthscales and a noise parameter) is assigned a independent Gamma prior with fixed parameters \\(a_k\\) and \\(b_k\\). Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of \\(\\phi\\) receives an independent log-normal prior.\nA third visualization of the model structure is a directed acyclic graph, shown below. Starting with the priors for \\(\\alpha\\), \\(\\phi\\), and \\(\\theta\\), we can draw their values and generate our latent variables \\(z\\) and our true variables \\(y\\). Note that \\(\\theta\\) depends on \\(z\\) as well as its prior, as \\(z\\) defines the number of clusters and thus the number of GP parameters to be drawn. The red and blue boxes indicate the quantities which are drawn multiple times for each of the \\(n\\) data points or each of the \\(J\\) clusters."
  },
  {
    "objectID": "imgpe.html#gibbs-sampling-algorithm",
    "href": "imgpe.html#gibbs-sampling-algorithm",
    "title": "The iMGPE Algorithm",
    "section": "Gibbs Sampling Algorithm",
    "text": "Gibbs Sampling Algorithm\nThe modified CRP used in this algorithm is defined by “R. M. Neal” (Algorithm 8 in that paper with \\(m=1\\)) and works as follows:\nWe represent the current cluster state with assignment labels \\(z=(z_1,\\dots,z_n)\\) and GP parameter vectors \\(\\theta_1,\\dots,\\theta_J\\) where \\(J\\) is the number of clusters in the current state. For \\(i=1,\\dots,n\\), repeat the following. Let \\(J^{-i}\\) be the number of clusters in \\(z\\) with point \\(i\\) removed. Let \\(\\theta_{J^{-i}+1}\\) be a parameter vector drawn from its prior distribution, in this case \\(Gamma^d(a,b)\\). Draw a new value for \\(z_i\\) with the following conditional probabilities:\n\\[P(z_i=j|z_{-i},y_i,\\dots)\\propto \\begin{cases}\n\\frac{n-1}{n+\\alpha-1}\\frac{\\sum_{i'\\neq i,z_{i'}=j} K_{\\phi}(X_i,X_{i'})}{\\sum_{i'\\neq i} K_{\\phi}(X_i,X_{i'})} f(y_i|\\theta_j) \\text{ for } j=1,\\dots,J^{-i}\\\\\n\\frac{\\alpha}{n+\\alpha-1}f(y_i|\\theta_{J^{-i}+1}) \\text{ for } j=J^{-i}+1\n\\end{cases}\\]\nwhere \\(f(y_i|\\theta_j)\\) is the normal density of \\(y_i\\) given the kriging equations with parameter vector \\(\\theta_j\\) defining the kernel function.\nI have implemented the Infinite Mixture of Gaussian Process Experts algorithm mostly as described by the authors Rasmussen and Ghahramani, though with a few alterations of my own which are noted below. First, I initialize indicator variables \\(z\\) to a set of values. I generally start by assigning all points to a single cluster. I set gamma prior distributions on the lengthscale and nugget parameters of the GP experts, using the ‘darg’ and ‘garg’ functions of the package ‘laGP’ and set initial values for \\(\\alpha\\) and \\(\\phi\\). This approach then iterates through the following MCMC algorithm.\n\nPerform a Gibbs sampling sweep over the cluster assignment indicators, using the modified Chinese Restaurant Process described in the model explanation, to generate a new cluster assignment vector \\(z\\).\nFit a Gaussian process expert to each cluster in \\(z\\) and get ML estimates of each expert’s parameters \\(\\theta_j\\). Note that this is not a sampling step but maximization.\nSample the Dirichlet process concentration parameter, \\(\\alpha\\), using quantile slice sampling with a \\(Gamma(1,1)\\) proposal distribution. The posterior distribution of \\(\\alpha\\) we sample from is \\[p(\\alpha|n, J)\\propto \\alpha^{J-3/2}\\exp(-1/2\\alpha)\\Gamma(\\alpha)/\\Gamma(n+\\alpha)\\]\nSample the other CRP parameter \\(\\phi\\) via random walk Monte Carlo. The random walk step uses a normal proposal distribution centered at the current value and with variance equal to \\((2.38^2/d)H^{-1}\\), \\(d\\) being the number of inputs and \\(H\\) the Hessian matrix of the distribution of \\(\\phi\\). The posterior distribution of \\(\\phi\\) we are sampling from is \\[p(\\phi|z,\\alpha,\\dots)\\propto p(z|y,\\phi,\\alpha)p(\\phi)\\approx \\left[\\prod_{i=1}^n p(z_i|y,\\phi,\\alpha) \\right] p(\\phi)\\]\nRepeat from step 1 until the MCMC output has converged."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dictionary.html",
    "href": "dictionary.html",
    "title": "Data Dictionary",
    "section": "",
    "text": "This data set contains the acceleration of a crash test dummy at a sequence of points in time as it rides a motorcycle into an obstacle."
  },
  {
    "objectID": "dictionary.html#motorcycle-data-set",
    "href": "dictionary.html#motorcycle-data-set",
    "title": "Data Dictionary",
    "section": "",
    "text": "This data set contains the acceleration of a crash test dummy at a sequence of points in time as it rides a motorcycle into an obstacle."
  },
  {
    "objectID": "dictionary.html#simulated-data-set-1",
    "href": "dictionary.html#simulated-data-set-1",
    "title": "Data Dictionary",
    "section": "Simulated Data Set 1",
    "text": "Simulated Data Set 1\nColumn \\(x\\) is a random draw from a uniform distribution on the interval \\((0,5)\\). Column \\(ytrue\\) is generated from the \\(x\\) values with the following function: \\[f(x)= \\begin{cases}\n\\frac{7}{3}-\\frac{2}{3}x\\text{ if } x\\leq 2\\\\\n\\cos(\\pi x)\\text{ if } x&gt;2\n\\end{cases}\\] Column \\(y\\) is the \\(ytrue\\) column plus \\(N(\\mu=0,\\sigma^2=0.04)\\) random noise."
  },
  {
    "objectID": "index.html#summary",
    "href": "index.html#summary",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I built a Quarto website to store my thesis work and weekly reports going forward. I also investigated possible factors for a factorial study of the existing variations of the iMGPE algorithm. Lastly, I began coding an iMGPE algorithm that uses the distance dependent Chinese Restaurant Process."
  },
  {
    "objectID": "index.html#prep-for-factorial-study",
    "href": "index.html#prep-for-factorial-study",
    "title": "Weekly Reports",
    "section": "Prep for Factorial Study",
    "text": "Prep for Factorial Study\nWhile I was not able to arrange a factorial study of the iMGPE algorithm, I did put some thought into what levels should be included in one. The first part of the algorithm I can vary is how the GP experts are fit, the options being ML optimization, STAN, and my homemade slice sampler. The next is the priors placed on \\(\\alpha\\), \\(\\phi\\), and \\(\\theta\\). A third, discussed by this method’s authors, is to cap the number of points that can be included in a single cluster with the parameter “maxSize”.\nI considered a couple of other factors such as the initial cluster assignment and the proposal distributions for the samplers for \\(\\alpha\\), \\(\\phi\\), and the GPs. I decided not to include the first since testing indicated that the initial cluster assignment has no real effect on future clusters. Likewise, I decided against the latter since a good proposal distribution shouldn’t affect the output of the sampler.\nI settled on testing two plausible priors for \\(\\theta\\): an InverseGamma\\((1,1)\\) distribution and a LogNormal\\((\\mu=0,\\sigma^2=1)\\) distribution. The lognormal distribution should have heavier tails than the inverse gamma. Both priors were tested with each GP fitting method. Additionally, each GP method was tested with unlimited cluster sizes and with cluster size capped at \\(50\\).\nFor the slice sampler version, I completed three runs: one with the default inverse gamma prior on \\(\\theta\\), one with the lognormal prior on \\(\\theta\\), and one with the inverse gamma prior and “maxSize” set to \\(50\\). The estimated mean functions of each run are plotted below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the STAN version, I completed three runs: one with the default inverse gamma prior on \\(\\theta\\), one with the lognormal prior on \\(\\theta\\), and one with “maxSize” set to \\(50\\). The estimated mean functions of each run are plotted below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe clearest takeaway was that my jury-rigged slice sampling method to fit a GP expert is not very effective. The slice sampler version displays much more oversmoothing, to the point of seeming to ignore the data entirely. Interestingly, the slice sampler version seems to break entirely when the \\(\\theta\\) prior or the maximum cluster size is changed, while the STAN version handles both just fine."
  },
  {
    "objectID": "index.html#ddcrp",
    "href": "index.html#ddcrp",
    "title": "Weekly Reports",
    "section": "DDCRP",
    "text": "DDCRP\nBased on R code provided by Blei and Frazier, I was able to begin coding a version of the iMGPE algorithm that uses the Distance Dependent Chinese Restaurant Process to cluster the data. This required rewriting the cluster update step, the \\(\\phi\\) sampling step, and the posterior prediction step.\nThe code is not quite functional yet, but it will be soon.\n\nJune 26, 2025"
  },
  {
    "objectID": "index.html#summary-1",
    "href": "index.html#summary-1",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I researched variations of the Chinese Restaurant Process similar to the modified CRP presented by Rasmussen and Ghahramani. The MCRP does not appear to be well documented in literature, but I found a similar algorithm called the distance dependent CRP. These variations appear to be a subset of the field of random measures over data partitions.\nI also studied the MCRP prior on a new data set with three distinct clusters, allowing me to examine how well the true cluster structure is captured for varying parameter values. As expected, \\(\\phi\\) controls the degree to which similar points prefer to cluster together, seen in how two measures of cluster quality improve as \\(\\phi\\) decreases.\nI continue to test the STAN and slice sampler iMGPE algorithms on strictly positive data sets. The STAN version performs very well, while the slice sampler version struggled."
  },
  {
    "objectID": "index.html#distance-dependent-crp",
    "href": "index.html#distance-dependent-crp",
    "title": "Weekly Reports",
    "section": "Distance Dependent CRP",
    "text": "Distance Dependent CRP\nThere does not appear to be any research about the specific modified Chinese Restaurant Process used by Rasmussen and Ghahramani. However, the authors Blei and Frazier discuss a similar algorithm in their 2010 paper Distance Dependent Chinese Restaurant Processes. The DD-CRP differs from the regular and modified CRP in that rather than assigning customers to tables (that is, points to clusters) each customer is assigned to sit with another customer or with themselves. The table assignments are derived from these customer assignments by grouping together all customers who can be linked by a sequence of customer pairings into a table.\nThe conditional probability of customer \\(i\\) being assigned to sit with customer \\(j\\) is dependent on the distance between them. Given \\(D\\), the pairwise distances between all points, and \\(\\alpha\\), the concentration parameter, the probability of customer assignment \\(c_i\\) is as follows.\n\\[p(c_i=j|D,\\alpha)\\propto \\begin{cases}\nf(d_{ij}) \\text{ if } j\\neq i\\\\\n\\alpha \\text{ if } i=j\n\\end{cases}\\]\nNote that this differs from our MCRP (and the regular CRP) in that the assignment of point \\(i\\) only depends on the distances between customers, and not on the customer or cluster assignments of other points.\nThe authors demonstrate that the regular CRP can be characterized as a special case of the distance dependent CRP. Furthermore, the regular CRP is the only marginally invariate distance dependent CRP, meaning that marginalizing over a particular customer yields the same probability distribution as if the customer was not included at all. They go on to describe a Gibbs sampling formula for the DD-CRP.\nThe modified CRP used by Rasmussen and Ghahramani does not appear to have been theoretically explored or justified. While certain properties of the DD-CRP are described, it is not considered in the context of a mixture of experts model. I came across a decent sized array of papers on the more general topic of using random measures to define probability distributions over partitions, of which the CRP, DD-CRP, and probably the MCRP are all examples, but did not have time to synthesize them here.\nIn summary, I think it may be worthwhile to consider reworking the iMGPE algorithm to use a clustering process that is more clearly defined or easier to work with in terms of theoretical properties. It may be possible to prove, or disprove, qualities such as consistency for the MCRP as well."
  },
  {
    "objectID": "index.html#prior-analysis-of-mcrp",
    "href": "index.html#prior-analysis-of-mcrp",
    "title": "Weekly Reports",
    "section": "Prior Analysis of MCRP",
    "text": "Prior Analysis of MCRP\nI tested the prior distribution of the Modified Chinese Restaurant Process on a fully segregated data set with three clusters and 100 points in total. I drew \\(40\\) points from the interval \\([0,1]\\), \\(30\\) from \\([3,4]\\), and \\(30\\) from \\([7,8]\\). Since this data set has a true cluster structure, we can evaluate how well the MCRP captures it through metrics such as cluster purity and the Adjusted Rand Index.\nCluster purity is calculated by assigning each cluster to the class most frequent in that cluster and then finding the percent of the total number of data points that were classified correctly given those assignments. It ranges from \\(0\\) to \\(1\\) with \\(1\\) being perfect purity, though this is also affected by the number of points and clusters. The Adjusted Rand Index is a measure of the similarity between the generated partition and the true partition of the data set. It ranges from roughly \\(0\\) to \\(1\\), with \\(1\\) indicating a perfect match to the true cluster structure and \\(0\\) indicating a fully random cluster assignment.\nAfter correcting a typo in my MCRP code, my initial intuition was borne out. I started with all points assigned to the same cluster and performed \\(1000\\) MCMC updates according to the MCRP algorithm. I repeated this process with a few different initial states, but they did not have a significant effect on the final state. Considering the results, we see both metrics of cluster accuracy improve as \\(\\phi\\) gets smaller, while the number of clusters varied only with \\(\\alpha\\) staying at about \\(5\\) clusters for \\(\\alpha=1\\), \\(17\\) clusters for \\(\\alpha=3\\), and \\(34\\) clusters for \\(\\alpha=9\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that cluster purity tends to increase slightly as the number of clusters increases (most visible on the third column) since smaller clusters are more likely to be pure even when generated randomly."
  },
  {
    "objectID": "index.html#monte-carlo-methods-on-new-data",
    "href": "index.html#monte-carlo-methods-on-new-data",
    "title": "Weekly Reports",
    "section": "Monte Carlo Methods on New Data",
    "text": "Monte Carlo Methods on New Data\nThis week I tested the STAN and slice sampler versions of the iMGPE on strictly positive data sets. The slice sampler version was tested on the data set from last week, where \\(f(x)=1+0.3x+\\frac{\\sin(\\pi x)}{5x}\\). The STAN version was tested on a similar data set where \\(f(x)=1+0.5x+\\frac{\\sin(\\pi x)}{2x}\\). The posterior estimate of the function is plotted below for each, with STAN on the left and the slice sampler on the right.\nThe blue line is the true function path and the red line is the median of the fitted values. The upper grey band shows the \\(95\\%\\) credible band over the test set while the lower grey band is a visual aid displaying the width of the credible band with its lower bound fixed at a level below the graph. The mean MC standard error for the \\(2.5th\\), \\(50th\\) and \\(97.5th\\) quantiles is \\(0.03\\), \\(0.02\\), and \\(0.01\\) for the STAN version and \\(0.04\\), \\(0.02\\) and \\(0.01\\) for the slice sampler version.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnexpectedly, the slice sampler displays high uncertainty towards the right half of the plot and severely underestimates not just the true function path but the observed data as well. I suspect the algorithm’s estimating function, which takes a weighted mean of every cluster’s estimate of a new data point, is still putting too much weight on the estimates of distant clusters. I am not sure why it would perform so much worse than the STAN version though.\n\nJune 19, 2025"
  },
  {
    "objectID": "index.html#summary-2",
    "href": "index.html#summary-2",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I investigated the Modified Chinese Restaurant Process and attempted to reason out the role of the parameter \\(\\phi\\) with some success. I also studied the new versions of iMGPE with STAN and slice sampling based GP experts and analyzed the Monte Carlo standard error of their quantile estimates.\nI also tested the iMGPE algorithm on a new simulated data set with a strictly positive response. The predictive posterior distribution appears to be over-smoothed; it estimates the function as closer to a straight line than it really is."
  },
  {
    "objectID": "index.html#mcrp-prior",
    "href": "index.html#mcrp-prior",
    "title": "Weekly Reports",
    "section": "MCRP Prior",
    "text": "MCRP Prior\nThis week, I continued to study the Modified Chinese Restaurant Process used in the iMGPE algorithm with particular attention to the role played by the parameter \\(\\phi\\). Previous experiments had indicated that \\(\\phi\\) had little influence on the average number of clusters and new experiments over a broader range of \\(\\phi\\) values simply bore this out.\nUpon reflection, I realized that \\(\\phi\\) shouldn’t have a major impact on the number of clusters anyways. The number of clusters would be controlled primarily by the probability of assigning a point to a new cluster, which is \\(\\alpha/(n+\\alpha)\\); that is, it depends only on \\(\\alpha\\) and the number of data points. Experimentation with the MCRP conditional assignment probabilities seemed to indicate that \\(\\phi\\) controls how strongly a point prefers to join a cluster that is close to it in \\(X\\). As \\(\\phi\\) approaches infinity, the probability of a point joining a specific cluster approaches proportionality with the number of points in that cluster. As \\(\\phi\\) approaches zero, the probability of a point joining a specific cluster approaches one if that cluster is the closest to the point and zero otherwise.\nTherefore, we should not expect \\(\\phi\\) to influence the number of clusters generated but rather how points are distributed among the clusters, with smaller \\(\\phi\\) values tending towards more separation between clusters. Unfortunately, I struggled to verify this empirically, as it is difficult to express separation between clusters in a compact form. I found one single number summary, the Dunn Index, but when tested on a range of values of \\(\\alpha\\) and \\(\\phi\\) (\\((0.1,1,10)\\) and \\((1,10,100)\\) respectively) it did not vary significantly.\nA clearer pattern was visible when considering the spread of cluster sizes, that is, the difference between the largest cluster size and the smallest. As \\(\\phi\\) increases, the spread shrinks slightly. Admittedly, I am not certain why this would be the case. Further investigation is warranted.\n\n\n\n\n\n\n\n\n\nAnother plausible use of the \\(\\phi\\) parameter would be if we had multiple input variables. Then \\(\\phi\\) could determine the importance of a given \\(X\\) input for clustering."
  },
  {
    "objectID": "index.html#updated-experimental-results",
    "href": "index.html#updated-experimental-results",
    "title": "Weekly Reports",
    "section": "Updated Experimental Results",
    "text": "Updated Experimental Results\nI tested the STAN version of the iMGPE algorithm with \\(2000\\) iterations per expert instead of \\(500\\). For now, I am still limited to \\(500\\) MCMC iterations in total due to memory restraints. The results did not appear to be significantly different.\nThe predicted posterior of the STAN version is plotted below on the left. The blue line is the true function path and the red line is the median of the fitted values. The upper grey band shows the \\(95\\%\\) credible band over the test set while the lower grey band is a visual aid displaying the width of the credible band with its lower bound fixed at \\(-2\\).\nOn the right is a plot of the \\(95\\%\\) credible intervals for the median and \\(95\\%\\) credible interval bounds of the estimated function given the Monte Carlo standard error. The blue line is the true function path. The solid red line is the median of the fitted vales and the two dashed red lines are the \\(2.5th\\) and \\(97.5th\\) quantiles of the fitted values. The grey ribbons are the MCSE credible intervals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe average MC standard error for the median was \\(0.012\\) and for the \\(2.5th\\) and \\(97.5th\\) quantiles it was \\(0.019\\) and \\(0.022\\) respectively.\nFor comparison, here are the same plots for the slice sampling version. On the left is the predicted posterior with the true and median fitted functions plotted in blue and red and the \\(95\\%\\) credible band as the grey band. On the right is the plot of the \\(95\\%\\) credible bands for the \\(2.5th\\), \\(50th\\), and \\(97.5th\\) percentiles given the Monte Carlo standard error of their estimates.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe average MC standard error for the median was \\(0.015\\) and for the \\(2.5th\\) and \\(97.5th\\) quantiles it was \\(0.022\\) and \\(0.027\\) respectively."
  },
  {
    "objectID": "index.html#new-simulated-data-set",
    "href": "index.html#new-simulated-data-set",
    "title": "Weekly Reports",
    "section": "New Simulated Data Set",
    "text": "New Simulated Data Set\nOur previous experiments on both the motorcycle data and the simulated data set displayed an unexpected shrinkage effect on the fitted function line, which we theorized to be towards the grand mean or perhaps \\(y=0\\). To distinguish between these possibilities, I fit a new instance to a new data set with a strictly positive response. A new data set of \\(100\\) points was generated by the following function.\n\\[f(x)=1+0.3x+\\frac{\\sin(\\pi x)}{5x}\\]\nI tried fitting this data set with the optimization method and obtained the plot of the fitted function below. The fitted line undershoots the true line (in blue) in some places but overshoots in others. Overall, it seems to under-fit the function line, preferring a straighter path than what the function actually follows.\n\n\n\n\n\n\n\n\n\n\nJune 12, 2025"
  },
  {
    "objectID": "index.html#summary-3",
    "href": "index.html#summary-3",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I refined my data model description and DAG chart. I also evaluated the prior distribution of the number of clusters generated by the modified Chinese Restaurant Process used in iMGPE, finding that it is often much higher than the observed number of clusters.\nI developed two new versions of the iMGPE algorithm that replaced the optimization-based GP evaluation step with two sampling-based methods: STAN and slice sampling. These methods take much longer to run than the optimization version but produce estimates of similar quality."
  },
  {
    "objectID": "index.html#data-model",
    "href": "index.html#data-model",
    "title": "Weekly Reports",
    "section": "Data Model",
    "text": "Data Model\nWe have an \\(n\\times 1\\) continuous response vector \\(y\\) and an \\(n\\times d\\) data matrix \\(X\\). The estimated value of a data point \\(y_i\\) under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing \\(y_i\\) and weighted by a Dirichlet process. Let \\(z\\) represent a possible vector of cluster assignments and let \\(z^{(k)}\\) be the \\(k^{th}\\) element of some ordered list of all possible \\(z\\). Then \\(j=1,\\dots,J_k\\) index the clusters within \\(z^{(k)}\\). Let \\(C_j^{(k)}\\) be the number of observations in cluster \\(j\\) given assignment \\(z^{(k)}\\). Then we have as follows.\n\\[y\\sim \\sum_{k=1}^{p(n)} \\left[\\prod_{j=1}^{J_k} N_{C_j^{(k)}}(0,\\Sigma_{\\theta_j}) \\right] w_k\\] \\[w_k=P(z=z^{(k)}|\\alpha,\\phi)\\] where the \\(w_z\\) are the marginal probabilities of a modified Chinese Restaurant Process that generates cluster assignments \\(z\\) and \\(p(n)\\) is the partition function. The CRP used here has been modified to depend on the input data \\(X\\). It is controlled by two parameters, \\(\\alpha\\) and \\(\\phi\\), the first being the usual concentration parameter and the second controlling the cluster occupancy estimates. A more in-depth explanation of this CRP is provided in the iMGPE Algorithm section from last week. It should also be noted that there is no known closed form for \\(w_k\\). The closest I could find was the multivariate Ewen’s distribution, which describes the distribution on the set of \\(p(n)\\) that arises from a regular Chinese Restaurant Process. However, it only accounts for the number of clusters of different sizes, and does not serve as a marginal distribution of \\(z\\).\nThe full joint distribution, including the priors for all parameters, is as follows. Here, \\(\\Phi\\) is the pdf of a normal distribution.\n\\[p(y,\\theta,\\phi,\\alpha)=\\left[ \\sum_{k=1}^{p(n)} \\left[\\prod_{j=1}^{J_k} \\Phi_{C_j^{(k)}}(0,\\Sigma_{\\theta_j}) \\right] p(z(k)|\\alpha,\\phi)\\right] p(\\theta)p(\\phi)p(\\alpha)\\]\nAlternatively, the model can be expressed in hierarchical terms where \\(J_z\\) is the number of clusters in \\(z\\) and \\(C_{j,z}\\) is the number of observations in the \\(j^{th}\\) cluster in \\(z\\).\n\\[y|z\\sim \\prod_{j=1}^{J_z} N_{C_{j,z}}(0,\\Sigma_{\\theta_{j}})\\] \\[z|\\alpha,\\phi \\sim MCRP(\\alpha,\\phi)\\]\nHere, \\(N_{C_j^{(z)}}(0,\\Sigma_{\\theta_j})\\) is the \\(C_j^{(z)}\\)-dimensional multivariate normal distribution with covariance matrix \\(\\Sigma_{\\theta_j}\\) defined by a Gaussian kernel function with parameters \\(\\theta_j\\).\nThen \\(\\theta_j\\) is the parameter vector for the GP expert assigned to cluster \\(j\\), while \\(\\alpha\\) is the CRP concentration parameter and \\(\\phi\\) is the parameter vector for the CRP’s occupation number estimate. Note that \\(\\phi\\) is purely a vector of lengthscales for a Gaussian kernel. The priors on \\(\\theta\\), \\(\\alpha\\), and \\(\\phi\\) are described below.\n\\[\\theta_{j,k}\\stackrel{ind}{\\sim} Gamma(a_k,b_k) \\text{ for } k=1,\\dots,d\\] \\[\\alpha\\sim Inv.Gam(1,1),\\text{   } \\phi_k\\stackrel{iid}{\\sim} LogN(0,1) \\text{ for } k=1,\\dots,d\\] That is, each element \\(k\\) of \\(\\theta_j\\) (the dimension of \\(X\\) plus a noise parameter) is assigned a independent Gamma prior with fixed parameters \\(a_k\\) and \\(b_k\\). Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of \\(\\phi\\) receives an independent log-normal prior.\nA third visualization of the model structure is a directed acyclic graph, shown below. Starting with the priors for \\(\\alpha\\), \\(\\phi\\), and \\(\\theta\\), we can draw their values and generate our latent variables \\(z\\) and our true variables \\(y\\). Note that \\(\\theta\\) depends on \\(z\\) as well as its prior, as \\(z\\) defines the number of clusters and thus the number of GP parameters to be drawn. The red and blue boxes indicate the quantities which are drawn multiple times for each of the \\(n\\) data points or each of the \\(J\\) clusters."
  },
  {
    "objectID": "index.html#prior-of-mcrp",
    "href": "index.html#prior-of-mcrp",
    "title": "Weekly Reports",
    "section": "Prior of MCRP",
    "text": "Prior of MCRP\nThe modified Chinese Restaurant process is dependent on the input data \\(X\\) through the parameter \\(\\phi\\) in addition to the parameter \\(\\alpha\\) used by a regular CRP. It is worth considering how this affects the prior distribution of our cluster assignment vector \\(z\\). While there is no closed form for the marginal distribution of \\(z\\), we can study it empirically by generating samples of \\(z\\) using the algorithm described in the iMGPE Algorithm section last week.\nUsing the simulated dataset, described in the April 24 section, I studied the average number of clusters generated by the MCRP. I chose the values \\(\\alpha=1,3,5\\) and \\(\\phi=2.5,5,7.5\\) to cover the most common range of values observed in our data and generated a set of \\(1000\\) \\(z\\) vectors for each combination of \\(\\alpha\\) and \\(\\phi\\). The mean and standard deviation of the number of clusters in those sets are shown below.\n\n\n\n\\(\\alpha\\)\n\\(\\phi\\)\nmean\nsd\n\n\n\n\n1\n2.5\n4.42\n1.86\n\n\n1\n5\n4.86\n1.84\n\n\n1\n7.5\n5.04\n1.99\n\n\n3\n2.5\n17.6\n3.26\n\n\n3\n5\n17.8\n3.55\n\n\n3\n7.5\n17.9\n3.27\n\n\n5\n2.5\n27.1\n3.66\n\n\n5\n5\n27.1\n3.87\n\n\n5\n7.5\n26.8\n3.75\n\n\n\nThese results can be compared to the expected numbers of clusters from a standard CRP with the same \\(\\alpha\\) value, which is \\(\\alpha(\\psi(n+\\alpha)-\\psi(\\alpha))\\) where \\(n\\) is the number of data points and \\(\\psi\\) is the digamma function. For \\(\\alpha\\) values of \\(1\\), \\(3\\), and \\(5\\), the expected numbers of clusters are \\(5.2\\), \\(11.1\\), and \\(15.7\\). We can see that the data dependency in the MCRP has increased the expected number of clusters and the inflation gets bigger as \\(\\alpha\\) increases. However, \\(\\phi\\) does not appear to have a strong influence on the number of clusters, at least within the range I examined."
  },
  {
    "objectID": "index.html#imgpe-with-stan-slice-sampling",
    "href": "index.html#imgpe-with-stan-slice-sampling",
    "title": "Weekly Reports",
    "section": "iMGPE with STAN & Slice Sampling",
    "text": "iMGPE with STAN & Slice Sampling\nI coded two new versions of the iMGPE algorithm: one that fit GP experts using Hybrid Monte Carlo using STAN and one that fit GP experts by sampling their parameters with elliptical slice sampling. The STAN experts ran for \\(500\\) iterations each while the slice sampler drew \\(100\\) sample parameter values from the GP posterior and averaged them. Both took much longer to run than the optimization version and had to be run for fewer iterations. Particularly, running the STAN version for more than \\(600\\) or so iterations caused a “Maximum number of DLLs reached” error that I haven’t yet determined how to fix. Thus, I am presenting a \\(500\\) iteration run of the STAN version and a \\(1000\\) iteration run of the slice sampling version.\nBoth methods were trained on the simulated data set and compared on a test set of \\(500\\) evenly spaced points between \\(0\\) and \\(5\\). The predicted posterior of the STAN version is plotted below. The blue line is the true function path and the red line is the median of the fitted values (the mean proved to be much less coherent). The upper grey band shows the \\(95\\%\\) credible band over the test set while the lower grey band is a visual aid displaying the width of the credible band with its lower bound fixed at \\(-2\\).\n\n\n\n\n\n\n\n\n\nThe predictive posterior of the slice sampling version is plotted here. Unlike the STAN version, the red line here does portray the mean rather than the median.\n\n\n\n\n\n\n\n\n\nThe STAN and slice sampling versions are a bit less accurate than the optimization version in terms of their estimated function. That is, the red line strays further from the blue line in these versions. The STAN version, however, has equal or less uncertainty in its estimate than the optimization version, while the slice sampling version has more.\n\nMay 29, 2025"
  },
  {
    "objectID": "index.html#summary-4",
    "href": "index.html#summary-4",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nSince our last meeting, I have improved the data model and algorithm descriptions and added a directed acyclic graph to portray the relationships between the model components. I also investigated the cause of high uncertainty in the estimated function occurring in areas with many data points. This does not seem to be caused by interplay between the estimated nugget and the function, as the nugget estimates are both small and highly consistent across the test data. More likely, it is an artifact of the clustering process, where points are occasionally put in the ‘wrong’ cluster or a cluster is stacked with disparate points, whose GP subsequently has greater uncertainty in its estimates.\nI also coded a functioning iMGPE algorithm that fits GP experts with an MCMC process via STAN. I have not yet completed a full run of the new model as it is much slower than the original."
  },
  {
    "objectID": "index.html#data-model-description",
    "href": "index.html#data-model-description",
    "title": "Weekly Reports",
    "section": "Data Model Description",
    "text": "Data Model Description\nWe have an \\(n\\times 1\\) continuous response vector \\(y\\) and an \\(n\\times d\\) data matrix \\(X\\). The estimated value of a data point \\(y_i\\) under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing \\(y_i\\) and weighted by a Dirichlet process. Let \\(z\\) represent a possible vector of cluster assignments and let \\(z^{(k)}\\) be the \\(k^{th}\\) element of some ordered list of all possible \\(z\\). Then \\(j=1,\\dots,J_k\\) index the clusters within \\(z^{(k)}\\). Let \\(C_j^{(k)}\\) be the number of observations in cluster \\(j\\) given assignment \\(z^{(k)}\\). Then we have as follows.\n\\[y\\sim \\sum_{k=1}^{p(n)} \\left[\\prod_{j=1}^{J_k} N_{C_j^{(k)}}(0,\\Sigma_{\\theta_j}) \\right] w_k\\] \\[w_k=P(z=z^{(k)}|\\alpha,\\phi)\\] where the \\(w_z\\) are the marginal probabilities of a modified Chinese Restaurant Process that generates cluster assignments \\(z\\) and \\(p(n)\\) is the partition function. The CRP used here has been modified to depend on two parameters, \\(\\alpha\\) and \\(\\phi\\), the first being the usual concentration parameter and the second controlling the cluster occupancy estimates. A more in-depth explanation of this CRP is provided in the next section. It should also be noted that there is no known closed form for \\(w_k\\). The closest I could find was the multivariate Ewen’s distribution, which describes the distribution on the set of \\(p(n)\\) that arises from a regular Chinese Restaurant Process. However, it only accounts for the number of clusters of different sizes, and does not serve as a marginal distribution of \\(z\\).\nThe full joint distribution, including the priors for all parameters, is as follows. Here, \\(\\Phi\\) is the pdf of a normal distribution.\n\\[p(y)=\\left[ \\sum_{k=1}^{p(n)} \\left[\\prod_{j=1}^{J_k} \\Phi_{C_j^{(k)}}(0,\\Sigma_{\\theta_j}) \\right] P(z(k)|\\alpha,\\phi)\\right] p(\\theta)p(\\phi)p(\\alpha)\\]\nAlternatively, the model can be expressed in hierarchical terms where \\(J_z\\) is the number of clusters in \\(z\\) and \\(C_{j,z}\\) is the number of observations in the \\(j^{th}\\) cluster in \\(z\\).\n\\[y|z\\sim \\prod_{j=1}^{J_z} N_{C_{j,z}}(0,\\Sigma_{\\theta_{j}})\\] \\[z|\\alpha,\\phi \\sim MCRP(\\alpha,\\phi)\\]\nHere, \\(N_{C_j^{(z)}}(0,\\Sigma_{\\theta_j})\\) is the \\(C_j^{(z)}\\)-dimensional multivariate normal distribution with covariance matrix \\(\\Sigma_{\\theta_j}\\) defined by a Gaussian kernel function with parameters \\(\\theta_j\\).\nThen \\(\\theta_j\\) is the parameter vector for the GP expert assigned to cluster \\(j\\), while \\(\\alpha\\) is the CRP concentration parameter and \\(\\phi\\) is the parameter vector for the CRP’s occupation number estimate. Note that \\(\\phi\\) is purely a vector of lengthscales for a Gaussian kernel. The priors on \\(\\theta\\), \\(\\alpha\\), and \\(\\phi\\) are described below.\n\\[\\theta_{j,k}\\stackrel{ind}{\\sim} Gamma(a_k,b_k) \\text{ for } k=1,\\dots,d\\] \\[\\alpha\\sim Inv.Gam(1,1),\\text{   } \\phi_k\\stackrel{iid}{\\sim} LogN(0,1) \\text{ for } k=1,\\dots,d\\] That is, each element \\(k\\) of \\(\\theta_j\\) (the dimension of \\(X\\) plus a noise parameter) is assigned a independent Gamma prior with fixed parameters \\(a_k\\) and \\(b_k\\). Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of \\(\\phi\\) receives an independent log-normal prior.\nA third visualization of the model structure is a directed acyclic graph, shown below. Starting with the priors for \\(\\alpha\\), \\(\\phi\\), and \\(\\theta\\), we can draw their values and generate our latent variables \\(z\\) and our true variables \\(y\\). Note that \\(\\theta\\) depends on \\(z\\) as well as its prior, as \\(z\\) defines the number of clusters and thus the number of GP parameters to be drawn."
  },
  {
    "objectID": "index.html#the-imgpe-algorithm",
    "href": "index.html#the-imgpe-algorithm",
    "title": "Weekly Reports",
    "section": "The iMGPE Algorithm",
    "text": "The iMGPE Algorithm\nThe modified CRP used in this algorithm is defined by “R. M. Neal” (Algorithm 8 in that paper with \\(m=1\\)) and works as follows:\nWe represent the current cluster state with assignment labels \\(z=(z_1,\\dots,z_n)\\) and GP parameter vectors \\(\\theta_1,\\dots,\\theta_J\\) where \\(J\\) is the number of clusters in the current state. For \\(i=1,\\dots,n\\), repeat the following. Let \\(J^{-i}\\) be the number of clusters in \\(z\\) with point \\(i\\) removed. Let \\(\\theta_{J^{-i}+1}\\) be a parameter vector drawn from its prior distribution, in this case \\(Gamma^d(a,b)\\). Draw a new value for \\(z_i\\) with the following conditional probabilities:\n\\[P(z_i=j|z_{-i},y_i,\\dots)\\propto \\begin{cases}\n\\frac{n-1}{n+\\alpha-1}\\frac{\\sum_{i'\\neq i,z_{i'}=j} K_{\\phi}(X_i,X_{i'})}{\\sum_{i'\\neq i} K_{\\phi}(X_i,X_{i'})} f(y_i|\\theta_j) \\text{ for } j=1,\\dots,J^{-i}\\\\\n\\frac{\\alpha}{n+\\alpha-1}f(y_i|\\theta_{J^{-i}+1}) \\text{ for } j=J^{-i}+1\n\\end{cases}\\]\nwhere \\(f(y_i|\\theta_j)\\) is the normal density of \\(y_i\\) given the kriging equations with parameter vector \\(\\theta_j\\) defining the kernel function.\nI have implemented the Infinite Mixture of Gaussian Process Experts algorithm mostly as described by the authors Rasmussen and Ghahramani, though with a few alterations of my own which are noted below. First, I initialize indicator variables \\(z\\) to a set of values. I generally start by assigning all points to a single cluster. I set gamma prior distributions on the lengthscale and nugget parameters of the GP experts, using the ‘darg’ and ‘garg’ functions of the package ‘laGP’ and set initial values for \\(\\alpha\\) and \\(\\phi\\). This approach then iterates through the following MCMC algorithm.\n\nPerform a Gibbs sampling sweep over the cluster assignment indicators, using the modified Chinese Restaurant Process described in the model explanation, to generate a new cluster assignment vector \\(z\\).\nFit a Gaussian process expert to each cluster in \\(z\\) and get ML estimates of each expert’s parameters \\(\\theta_j\\). Note that this is not a sampling step but maximization.\nSample the Dirichlet process concentration parameter, \\(\\alpha\\), using quantile slice sampling with a \\(Gamma(1,1)\\) proposal distribution. The posterior distribution of \\(\\alpha\\) we sample from is \\[p(\\alpha|n, J)\\propto \\alpha^{J-3/2}\\exp(-1/2\\alpha)\\Gamma(\\alpha)/\\Gamma(n+\\alpha)\\]\nSample the other CRP parameter \\(\\phi\\) via random walk Monte Carlo. The random walk step uses a normal proposal distribution centered at the current value and with variance equal to \\((2.38^2/d)H^{-1}\\), \\(d\\) being the number of inputs and \\(H\\) the Hessian matrix of the distribution of \\(\\phi\\). The posterior distribution of \\(\\phi\\) we are sampling from is \\[p(\\phi|z,\\alpha,\\dots)\\propto p(z|y,\\phi,\\alpha)p(\\phi)\\approx \\left[\\prod_{i=1}^n p(z_i|y,\\phi,\\alpha) \\right] p(\\phi)\\]\nRepeat from step 1 until the MCMC output has converged."
  },
  {
    "objectID": "index.html#analysis-of-current-results",
    "href": "index.html#analysis-of-current-results",
    "title": "Weekly Reports",
    "section": "Analysis of Current Results",
    "text": "Analysis of Current Results\nIn the current algorithm, we see variation in the uncertainty of the fitted mean on the test data that is hard to explain given our data, with patches of relatively high uncertainty in areas with many data points (such as around \\(x=3\\)). This does not seem to be due to variation in the nugget parameter estimates, as a plot of the average and \\(95\\%\\) credible band for the estimated nugget for each observation is strongly uniform and centered around the relatively small value of \\(0.01\\). On a positive note, this is quite close to the true constant nugget value of \\(0.02\\).\nAs before, we drew an estimated value for each point on the test set every fifth iteration, using the formula \\(\\hat\\mu_i = \\sum_{j=1}^J p(z_i=j|\\alpha,\\phi)\\mu_{i,j}\\) where \\(\\hat\\mu_i\\) is the estimate for observation \\(i\\) in the test set, \\(\\mu_{i,j}\\) is the fitted value of point \\(i\\) under the GP for cluster \\(j\\), and \\(p(z_i=j|\\alpha,\\phi)\\) is the conditional probability of point \\(i\\) being assigned to cluster \\(j\\) under our modified CRP.\nThe resultant credible band for the simulated function is graphed below, based on \\(1000\\) draws from the predictive posterior. The blue line is the true function path and the red line is the mean of the fitted values. The upper grey band shows the \\(95\\%\\) credible band over the test set while the lower grey band is a visual aid displaying the width of the credible band with its lower bound fixed at \\(-2\\).\n\n\n\n\n\n\n\n\n\nI also recorded the nugget parameter estimates for each point in each iteration. I had hoped that this could explain some of the variation we see in fitted function uncertainty. The solid line and the grey band represent the mean of the nugget and the \\(95\\%\\) credible band on the nugget across all values in the test set respectively.\n\n\n\n\n\n\n\n\n\nThe nugget likely has little effect on fitted function uncertainty as it is both small and uniform. A more likely candidate is unlucky cluster assignments creating clusters that cannot be fit with a high degree of certainty. For example, the plot below displays our data set colored by the cluster assignments of the 200th iteration. Note that the eighth cluster, the purple dots, contains several points in the upper left of the plot and one in the dip around \\(x=3\\) and another in the peak around \\(x=4\\). These points likely cannot be estimated with confidence given the other points in the cluster and their estimates could be significantly inaccurate and distort the tails of the overall function estimate."
  },
  {
    "objectID": "index.html#single-gp-comparison",
    "href": "index.html#single-gp-comparison",
    "title": "Weekly Reports",
    "section": "Single GP Comparison",
    "text": "Single GP Comparison\nFor comparison, I fit a single Gaussian process model to the same data set and plotted its predictive mean and uncertainty on the test data. I found that a single GP model arguably does a better job of matching the estimated function (in red) to the true function (in blue). However, its uncertainty in that estimate is much wider than in the iMGPE model almost everywhere. As before, the red and blue lines represent the estimated and true functions, the grey band around them represents the \\(95\\%\\) credible interval in the estimated function, and the grey band at the bottom displays the width of the credible band across \\(X\\).\n\n\n\n\n\n\n\n\n\n\nMay 15, 2025"
  },
  {
    "objectID": "index.html#summary-5",
    "href": "index.html#summary-5",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I refined my explanation of the data model to be more coherent and readable. I then wrote out the full update step for the MCMC algorithm, detailing how each component of the model is updated.\nLastly, I developed a method to estimate values on a test set to replace my old practice of fitting a credible interval around each data point. The new method produces a much smoother and narrower credible band for the estimated function on both the simulated data set and the motorcycle data set."
  },
  {
    "objectID": "index.html#explanation-of-data-model",
    "href": "index.html#explanation-of-data-model",
    "title": "Weekly Reports",
    "section": "Explanation of Data Model",
    "text": "Explanation of Data Model\nWe have an \\(n\\times 1\\) continuous response vector \\(y\\) and an \\(n\\times d\\) data matrix \\(X\\). The estimated value of a data point \\(y_i\\) under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing \\(y_i\\) and weighted by a Dirichlet process. Let \\(z\\) represent a possible vector of cluster assignments and let \\(z^{(k)}\\) be the \\(k^{th}\\) element of some ordered list of all possible \\(z\\). Then \\(j=1,\\dots,J_k\\) index the clusters within \\(z^{(k)}\\). Let \\(C_j^{(k)}\\) be the number of observations in cluster \\(j\\) given assignment \\(z^{(k)}\\). Then we have as follows.\n\\[y\\sim \\sum_{k=1}^{p(n)} \\left[\\prod_{j=1}^{J_k} N_{C_j^{(k)}}(0,\\Sigma_{\\theta_j}) \\right] w_k\\] \\[w_k=P(z=z^{(k)}|\\alpha,\\phi,\\dots)\\] where the \\(w_z\\) are the marginal probabilities of a Chinese Restaurant Process that generates cluster assignments \\(z\\) and \\(p(n)\\) is the partition function. The CRP used here has been modified to depend on two parameters, \\(\\alpha\\) and \\(\\phi\\), the first being the usual concentration parameter and the second controlling the cluster occupancy estimates. A more in-depth explanation of this CRP will be provided shortly.\nAlternatively, the model can be expressed in hierarchical terms where \\(J_z\\) is the number of clusters in \\(z\\) and \\(C_{j,z}\\) is the number of observations in the \\(j^{th}\\) cluster in \\(z\\).\n\\[y|z\\sim \\prod_{j=1}^{J_z} N_{C_{j,z}}(0,\\Sigma_{\\theta_{j}})\\] \\[z|\\alpha,\\phi \\sim CRP(\\alpha,\\phi)\\]\nHere, \\(N_{C_j^{(z)}}(0,\\Sigma_{\\theta_j})\\) is the \\(C_j^{(z)}\\)-dimensional multivariate normal distribution with covariance matrix \\(\\Sigma_{\\theta_j}\\) defined by a Gaussian kernel function with parameters \\(\\theta_j\\). Similarly, \\(Gamma^d(a,b)\\) is the joint prior over the GP parameters and is the product of \\(d\\) Gamma distributions.\nThen \\(\\theta_j\\) is the parameter vector for the GP expert assigned to cluster \\(j\\), while \\(\\alpha\\) is the CRP concentration parameter and \\(\\phi\\) is the parameter vector for the CRP’s occupation number estimate. Note that \\(\\phi\\) is purely a vector of lengthscales for a Gaussian kernel. The priors on \\(\\theta\\), \\(\\alpha\\), and \\(\\phi\\) are described below.\n\\[\\theta_{j,k}\\stackrel{ind}{\\sim} Gamma(a_k,b_k) \\text{ for } k=1,\\dots,d\\] \\[\\alpha\\sim Inv.Gam(1,1),\\text{   } \\phi_k\\stackrel{iid}{\\sim} LogN(0,1) \\text{ for } k=1,\\dots,d\\] That is, each element \\(k\\) of \\(\\theta_j\\) (the dimension of \\(X\\) plus a noise parameter) is assigned a independent Gamma prior with fixed parameters \\(a_k\\) and \\(b_k\\). Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of \\(\\phi\\) receives an independent log-normal prior.\nThe modified CRP used in this algorithm is defined by “R. M. Neal” (Algorithm 8 in that paper with \\(m=1\\)) and works as follows:\nWe represent the current cluster state with assignment labels \\(z=(z_1,\\dots,z_n)\\) and GP parameter vectors \\(\\theta_1,\\dots,\\theta_J\\) where \\(J\\) is the number of clusters in the current state. For \\(i=1,\\dots,n\\), repeat the following. Let \\(J^{-i}\\) be the number of clusters in \\(z\\) with point \\(i\\) removed. Let \\(\\theta_{J^{-i}+1}\\) be a parameter vector drawn from its prior distribution, in this case \\(Gamma^d(a,b)\\). Draw a new value for \\(z_i\\) with the following conditional probabilities:\n\\[P(z_i=j|z_{-i},y_i,\\dots)\\propto \\begin{cases}\n\\frac{n-1}{n+\\alpha-1}\\frac{\\sum_{i'\\neq i,z_{i'}=j} K_{\\phi}(X_i,X_{i'})}{\\sum_{i'\\neq i} K_{\\phi}(X_i,X_{i'})} f(y_i|\\theta_j) \\text{ for } j=1,\\dots,J^{-i}\\\\\n\\frac{\\alpha}{n+\\alpha-1}f(y_i|\\theta_{J^{-i}+1}) \\text{ for } j=J^{-i}+1\n\\end{cases}\\]\nwhere \\(f(y_i|\\theta_j)\\) is the normal density of \\(y_i\\) given the kriging equations with parameter vector \\(\\theta_j\\) defining the kernel function."
  },
  {
    "objectID": "index.html#the-imgpe-algorithm-1",
    "href": "index.html#the-imgpe-algorithm-1",
    "title": "Weekly Reports",
    "section": "The iMGPE Algorithm",
    "text": "The iMGPE Algorithm\nI have implemented the Infinite Mixture of Gaussian Process Experts algorithm mostly as described by the authors Rasmussen and Ghahramani, though with a few alterations of my own which are noted below. This approach then iterates through the following MCMC algorithm.\n\nInitialize indicator variables \\(z\\) to a set of values. I generally start by assigning all points to a single cluster. Set gamma prior distributions on the lengthscale and nugget parameters of the GP experts, using the ‘darg’ and ‘garg’ functions of the package ‘laGP’. Set initial values for \\(\\alpha\\) and \\(\\phi\\).\nPerform a Gibbs sampling sweep over the cluster assignment indicators, using the Chinese Restaurant Process described in the model explanation, to generate a new cluster assignment vector \\(z\\).\nFit a Gaussian process expert to each cluster in \\(z\\) and get ML estimates of each expert’s parameters \\(\\theta_j\\).\nUse these GP experts to generate estimates on the test set. More detail on this step is included in the next section.\nSample the Dirichlet process concentration parameter, \\(\\alpha\\), using quantile slice sampling with a \\(Gamma(1,1)\\) proposal distribution. The posterior distribution of \\(\\alpha\\) we sample from is \\[p(\\alpha|n, J)\\propto \\alpha^{J-3/2}\\exp(-1/2\\alpha)\\Gamma(\\alpha)/\\Gamma(n+\\alpha)\\]\nSample the other CRP parameter \\(\\phi\\) via random walk Monte Carlo. The random walk step uses a normal proposal distribution centered at the current value and with variance equal to \\((2.38^2/d)H^{-1}\\), \\(d\\) being the number of inputs and \\(H\\) the Hessian matrix of the distribution of \\(\\phi\\). The posterior distribution of \\(\\phi\\) we are sampling from is \\[p(\\phi|z,\\alpha,\\dots)\\propto p(z|y,\\phi,\\alpha)p(\\phi)\\approx \\left[\\prod_{i=1}^n p(z_i|y,\\phi,\\alpha) \\right] p(\\phi)\\]\nRepeat from step 2 until the MCMC output has converged."
  },
  {
    "objectID": "index.html#experiments-on-simulated-data",
    "href": "index.html#experiments-on-simulated-data",
    "title": "Weekly Reports",
    "section": "Experiments on Simulated Data",
    "text": "Experiments on Simulated Data\nI set up a new method of sampling posterior predictive values on a test set and applied it to the simulated data set, using a test set of \\(500\\) evenly spaced points between \\(0\\) and \\(5\\). Every fifth iteration, I would estimate fitted values on the test set for each GP expert currently in use. Then, for each point in the test set, I would calculate the conditional probabilities of it belonging to each of the current clusters. Lastly, I found the sum of the GP estimates for each point weighted by the probabilities of their respective clusters and record the resulting set of values for that iteration.\nThe resultant credible band for the simulated function is graphed below, based on \\(1000\\) draws from the predictive posterior. The blue line is the true function path and the red line is the mean of the fitted values. The upper grey band shows the \\(95\\%\\) credible band over the test set while the lower grey band is a visual aid displaying the width of the credible band with its lower bound fixed at \\(-2\\). Uncertainty is greatest at the edges of the training data and around the rightmost peak, where there are few points. This is as we would expect."
  },
  {
    "objectID": "index.html#experiments-on-motorcycle-data",
    "href": "index.html#experiments-on-motorcycle-data",
    "title": "Weekly Reports",
    "section": "Experiments on Motorcycle Data",
    "text": "Experiments on Motorcycle Data\nI tested this posterior prediction method on the motorcycle data set, using a test set of \\(561\\) evenly spaced points between \\(2\\) and \\(58\\). I have made some changes to the body of the algorithm since I last tested on the motorcycle data, so the trace plots for \\(\\phi\\) and \\(\\alpha\\) are also included. Both parameters have converged, and the algorithm heavily favors dividing the training data into two clusters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI obtained \\(1000\\) draws from the predictive posterior distribution as described in the previous section and have plotted their mean and \\(95\\%\\) credible band below. As before, the grey band at the bottom of the graph is a visual aid displaying the width of the credible interval across time.\n\n\n\n\n\n\n\n\n\n\nMay 8, 2025"
  },
  {
    "objectID": "index.html#summary-6",
    "href": "index.html#summary-6",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I updated my marginal model notation to be more readable and included a brief description of the hierarchical model. I also found and fixed the bug in my code that was causing unusually wide credible intervals.\nI am working on converting the algorithm fully to Rcpp and implementing support for multivariate and categorical \\(X\\) inputs. Now that the code structure of the algorithm has been developed, this should proceed quickly."
  },
  {
    "objectID": "index.html#explanation-of-data-model-1",
    "href": "index.html#explanation-of-data-model-1",
    "title": "Weekly Reports",
    "section": "Explanation of Data Model",
    "text": "Explanation of Data Model\nWe have an \\(n\\times 1\\) continuous response vector \\(y\\) and an \\(n\\times d\\) data matrix \\(X\\). The estimated value of a data point \\(y_i\\) under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing \\(y_i\\) and weighted by a Dirichlet process. Let \\(z\\) represent a possible vector of cluster assignments and let \\(z^{(k)}\\) be the \\(k^{th}\\) element of some ordered list of all possible \\(z\\). Then \\(j=1,\\dots,J_k\\) index the clusters within \\(z^{(k)}\\). Let \\(C_j^{(k)}\\) be the number of observations in cluster \\(j\\) given assignment \\(z(k)\\). Then we have as follows.\n\\[y\\sim \\sum_{k=1}^{p(n)} \\left[\\prod_{j=1}^{J_k} N_{C_j^{(k)}}(0,\\Sigma_{\\theta_j}) \\right] w_k\\] \\[w_k=P(z=z^{(k)}|\\alpha,\\phi,\\dots)\\] where the \\(w_z\\) are the marginal probabilities of a Chinese Restaurant Process that generates cluster assignments \\(z\\) and \\(p(n)\\) is the partition function. Alternatively, the model can be expressed in hierarchical terms as\n\\[y|z\\sim \\prod_{j=1}^J N_{C_j}(0,\\Sigma_{\\theta_j})\\] \\[z|\\alpha,\\dots \\sim CRP(\\alpha,\\phi)\\]\nThe specific CRP is defined by “R. M. Neal” (Algorithm 8 in that paper with \\(m=1\\)) and works as follows:\nWe represent the current cluster state with assignment labels \\(z=(z_1,\\dots,z_n)\\) and GP parameter vectors \\(\\theta_1,\\dots,\\theta_J\\) where \\(J\\) is the number of clusters in the current state. For \\(i=1,\\dots,n\\), repeat the following. Let \\(J^{-i}\\) be the number of clusters in \\(z\\) with point \\(i\\) removed. Let \\(\\theta_{J^{-i}+1}\\) be a parameter vector drawn from its prior distribution, in this case \\(Gamma^d(a,b)\\). Draw a new value for \\(z_i\\) with the following conditional probabilities:\n\\[P(z_i=j|z_{-i},y_i,\\dots)\\propto \\begin{cases}\n\\frac{n-1}{n+\\alpha-1}\\frac{\\sum_{i'\\neq i,z_{i'}=j} K_{\\phi}(X_i,X_{i'})}{\\sum_{i'\\neq i} K_{\\phi}(X_i,X_{i'})} f(y_i|\\theta_j) \\text{ for } j=1,\\dots,J^{-i}\\\\\n\\frac{\\alpha}{n+\\alpha-1}f(y_i|\\theta_{J^{-i}+1}) \\text{ for } j=J^{-i}+1\n\\end{cases}\\]\nwhere \\(f(y_i|\\theta_j)\\) is the normal density of \\(y_i\\) given the kriging equations with parameter vector \\(\\theta_j\\) defining the kernel function. The kriging equations are explained in the posterior predictive distribution section.\nMeanwhile, \\(N_{C_j^{(z)}}(0,\\Sigma_{\\theta_j})\\) is the \\(C_j^{(z)}\\)-dimensional multivariate normal distribution with covariance matrix \\(\\Sigma_{\\theta_j}\\) defined by a Gaussian kernel function with parameters \\(\\theta_j\\). Similarly, \\(Gamma^d(a,b)\\) is the joint prior over the GP parameters and is the product of \\(d\\) Gamma distributions.\nAbove, \\(\\theta_j\\) is the parameter vector for the GP expert assigned to cluster \\(j\\), while \\(\\alpha\\) is the DP concentration parameter and \\(\\phi\\) is the parameter vector for the DP’s occupation number estimate. Note that \\(\\phi\\) is purely a vector of lengthscales for a Gaussian kernel. The priors on \\(\\theta\\), \\(\\alpha\\), and \\(\\phi\\) are described below.\n\\[\\theta_{j,k}\\stackrel{ind}{\\sim} Gamma(a_k,b_k) \\text{ for } k=1,\\dots,d\\] \\[\\alpha\\sim Inv.Gam(1,1),\\text{   } \\phi_k\\stackrel{iid}{\\sim} LogN(0,1) \\text{ for } k=1,\\dots,d\\] That is, each element \\(k\\) of \\(\\theta_j\\) (the dimension of \\(X\\) plus a noise parameter) is assigned a independent Gamma prior with fixed parameters \\(a_k\\) and \\(b_k\\). Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of \\(\\phi\\) receives an independent log-normal prior."
  },
  {
    "objectID": "index.html#credible-interval-investigation",
    "href": "index.html#credible-interval-investigation",
    "title": "Weekly Reports",
    "section": "Credible interval investigation",
    "text": "Credible interval investigation\nUp to now, the posterior distributions of the fitted values have been highly skewed by outliers, leading to unusually large credible intervals. After investigation, I determined that this was due to an error in the posterior sampling code.\nEvery so often, the algorithm will generate a cluster with only one data point in it and attempt to fit a GP to that cluster. I learned early on to wrap the model code in a try-catch loop in case the GP function failed. My sampler from the \\(y\\) posterior assumed that any cluster of size 1 could not fit a GP, but this was not always true! In those cases, the sampler used the wrong GP, resulting in inaccurate estimates that defaulted towards the GP mean of zero.\nAfter correcting this, I tested my code on the simulated dataset and found that the credible band was dramatically narrower.\nFor every fifth iteration, I took the fitted values of the response at each point in time, resulting in a sample of 1000 fitted values for each point. I took the 2.5th and 97.5th quantiles of this sample to get a credible interval for the fitted value at each point. Below, I have plotted the true data as black dots, the mean fitted values as a red line, and the \\(95\\%\\) credible intervals as the gray ribbon.\n\n\n\n\n\n\n\n\n\n\nMay 1, 2025"
  },
  {
    "objectID": "index.html#summary-7",
    "href": "index.html#summary-7",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I updated my explanation of the data model and added a description of the posterior predictive distribution. I also revised my choice of standard deviation for the proposal distribution of \\(\\phi\\)’s random walk sampler, bringing its acceptance rate back up to acceptable levels."
  },
  {
    "objectID": "index.html#marginal-data-model",
    "href": "index.html#marginal-data-model",
    "title": "Weekly Reports",
    "section": "Marginal Data Model",
    "text": "Marginal Data Model\nWe have an \\(n\\times 1\\) continuous response vector \\(y\\) and an \\(n\\times d\\) data matrix \\(X\\). The estimated value of a data point \\(y_i\\) under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing \\(y_i\\) and weighted by a Dirichlet process. Let \\(z\\) represent a possible vector of cluster assignments and let \\(z(k)\\) be the \\(k^{th}\\) element of some ordered list of all possible \\(z\\). Then \\(j=1,\\dots,J(k)\\) index the clusters within \\(z(k)\\). Let \\(C_j^{(z(k))}\\) be the number of observations in cluster \\(j\\) given assignment \\(z(k)\\). Then we have as follows.\n\\[y\\sim \\sum_{k=1}^{p(n)} \\left[\\prod_{j=1}^{J(k)} N_{C_j^{(z(k))}}(0,\\Sigma_{\\theta_j}) \\right] w_{z(k)}\\] \\[w_Z=P(z=Z|\\alpha,\\phi)\\] where the \\(w_z\\) are the marginal probabilities of a Chinese Restaurant Process that generates cluster assignments \\(z\\) and \\(p(n)\\) is the partition function. The specific CRP is defined by “R. M. Neal” (Algorithm 8 in that paper with \\(m=1\\)) and works as follows:\nWe represent the current cluster state with assignment labels \\(z=(z_1,\\dots,z_n)\\) and GP parameter vectors \\(\\theta_1,\\dots,\\theta_J\\) where \\(J\\) is the number of clusters in the current state. For \\(i=1,\\dots,n\\), repeat the following. Let \\(J^{-}\\) be the number of clusters in \\(z\\) with point \\(i\\) removed. Let \\(\\theta_{J^{-}+1}\\) be a parameter vector drawn from its prior distribution, in this case \\(Gamma^d(a,b)\\). Draw a new value for \\(z_i\\) with the following conditional probabilities:\n\\[P(z_i=j|z_{-i},y_i,\\theta)=\\begin{cases}\nb\\frac{n-1}{n+\\alpha-1}\\frac{\\sum_{i'\\neq i,z_{i'}=j} K_{\\phi}(X_i,X_{i'})}{\\sum_{i'\\neq i} K_{\\phi}(X_i,X_{i'})} F(y_i|\\theta_j) \\text{ for } j=1,\\dots,J^{-}\\\\\nb\\frac{\\alpha}{n+\\alpha-1}F(y_i|\\theta_{J^{-}+1}) \\text{ for } j=J^{-}+1\n\\end{cases}\\] where \\(b\\) is a normalizing constant. Note that \\(b\\) need not be calculated; we can just calculate the un-normalized probability vector and then normalize it.\nMeanwhile, \\(N_{C_j^{(z)}}(0,\\Sigma_{\\theta_j})\\) is the \\(C_j^{(z)}\\)-dimensional multivariate normal distribution with covariance matrix \\(\\Sigma_{\\theta_j}\\) defined by a Gaussian kernel function with parameters \\(\\theta_j\\). Similarly, \\(Gamma^d(a,b)\\) is the joint prior over the GP parameters and is the product of \\(d\\) Gamma distributions.\nAbove, \\(\\theta_j\\) is the parameter vector for the GP expert assigned to cluster \\(j\\), while \\(\\alpha\\) is the DP concentration parameter and \\(\\phi\\) is the parameter vector for the DP’s occupation number estimate. Note that \\(\\phi\\) is purely a vector of lengthscales for a Gaussian kernel. The priors on \\(\\theta\\), \\(\\alpha\\), and \\(\\phi\\) are described below.\n\\[\\theta_{j_k}\\stackrel{ind}{\\sim} Gamma(a_k,b_k) \\text{ for } k=1,\\dots,d\\] \\[\\alpha\\sim Inv.Gam(1,1),\\text{   } \\phi_k\\stackrel{iid}{\\sim} LogN(0,1) \\text{ for } k=1,\\dots,d\\] That is, each element \\(k\\) of \\(\\theta_j\\) (the dimension of \\(X\\) plus a noise parameter) is assigned a independent Gamma prior with fixed parameters \\(a_k\\) and \\(b_k\\). Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of \\(\\phi\\) receives an independent log-normal prior."
  },
  {
    "objectID": "index.html#posterior-predictive-distribution",
    "href": "index.html#posterior-predictive-distribution",
    "title": "Weekly Reports",
    "section": "Posterior Predictive Distribution",
    "text": "Posterior Predictive Distribution\nThe paper by Rasmussen and Ghahramani does not discuss what a posterior predictive distribution would look like. However, it can be deduced based on the posterior predictive distribution of a lone Gaussian process. With \\(p(n)\\) and \\(w_{z(k)}\\) defined the same as in the original data model, we determine that the distribution of a new point \\(y^*\\) with data \\(x^*\\) is as follows.\n\\[y^*\\sim \\sum_{k=1}^{p(n)} \\left[\\prod_{j=1}^{J_k} N(\\mu_j^*,\\sigma_j^*) \\right] w_k\\] where \\(\\mu_j^*\\) and \\(\\Sigma_j^*\\) are found according to the kriging equations of a Gaussian process. That is,\n\\[\\mu_j^*=K(x^*,X^{(j)})^TK^{-1}y^{(j)}\\text{ and } \\sigma_j^*=K(x^*,x^*)-K(x^*,X^{(j)})^T K^{-1}K(x^*,X^{(j)})\\] where \\(K\\) is the covariance matrix based on parameters \\(\\theta_j\\) and \\(X^{(j)}\\subset X\\) and \\(y^{(j)} \\subset y\\) are the data and response values associated with cluster \\(j\\)."
  },
  {
    "objectID": "index.html#simulated-example",
    "href": "index.html#simulated-example",
    "title": "Weekly Reports",
    "section": "Simulated Example",
    "text": "Simulated Example\nWe previously had a very small acceptance rate in the random walk sampler for \\(\\phi\\), which seems to have been due to tiny Hessian values that result in a giant standard deviation in the (Normal) proposal distribution. Consequently, most proposals were far too large to be accepted. My current solution was to put a cap on the proposal standard deviation equal to the range of \\(x\\). As \\(\\phi\\) represents the rate of decay in correlation between neighboring points, values greater than the range of the data would be largely indistinguishable anyways. In result, \\(\\phi\\) seems well mixed and has an acceptance rate of \\(69.7\\%\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor every fifth iteration, I took the fitted values of the response at each point in time, resulting in a sample of 1000 fitted values for each point. I took the 2.5th and 97.5th quantiles of this sample to get a credible interval for the fitted value at each point. Below, I have plotted the true data as black dots, the mean fitted values as a red line, and the \\(95\\%\\) credible intervals as the gray ribbon. As the generating function was known, that too is plotted as a blue line.\n\n\n\n\n\n\n\n\n\nThe fitted mean line still does a good job of approximating the true function.\n\nApril 24, 2025"
  },
  {
    "objectID": "index.html#summary-8",
    "href": "index.html#summary-8",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week, I further refined my explanation of the data model, particularly the base distribution of the Dirichlet process. I studied the cluster assignment update algorithm described by Rasmussen and “R. M. Neal”, which clarified that the base distribution is used to generate the parameters controlling \\(y_i\\) given a new cluster. Based on this, I can identify the base distribution to be the prior on the Gaussian process parameters: a product of independent Gamma priors.\nWhile reviewing Neal’s explanation, I identified an error in my implementation of the cluster update step, stemming from how the ‘garg’ function in laGP may select \\((0,0)\\) for the parameters of the Gamma prior on the GP nugget. While laGP considers this to be “no prior” on the nugget, other parts of my algorithm that drew on this prior did not. Correcting this had a noticeable impact on the number of clusters generated.\nLastly, I simulated a new data set of \\(100\\) points using a custom mean function and tested the iMGPE algorithm on it. This allows me to examine how well my algorithm approximates the true mean function."
  },
  {
    "objectID": "index.html#data-model-1",
    "href": "index.html#data-model-1",
    "title": "Weekly Reports",
    "section": "Data Model",
    "text": "Data Model\nWe have an \\(n\\times 1\\) continuous response vector \\(y\\) and an \\(n\\times d\\) data matrix \\(X\\). The estimated value of a data point \\(y_i\\) under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing \\(y_i\\) and weighted by a Dirichlet process. Let \\(z\\) represent a possible vector of cluster assignments and \\(j=1,\\dots,|z|\\) index the clusters within \\(z\\), where \\(|z|\\) is the number of clusters. Let \\(C_j^{(z)}\\) be the number of observations in cluster \\(j\\) given assignment \\(z\\). Then we have as follows.\n\\[y|X\\sim \\sum_z \\left[\\prod_{j=1}^{|z|} N_{C_j^{(z)}}(0,\\Sigma_{\\theta_j}) \\right] w_z\\] \\[w\\sim DP_{\\phi}(\\alpha,  Gamma^d(a,b))\\] where \\(w\\) is a vector of probabilities defining a multinomial distribution drawn from a Dirichlet process and \\(w_z\\) is the probability of observing cluster assignment \\(z\\). That is, \\(P(z=Z|\\alpha,\\phi)=w_Z\\) and so \\(z|\\alpha,\\phi\\) is generated by a Chinese Restaurant process based on a \\(DP_{\\phi}(\\alpha, Gamma^d(a,b))\\) distribution.\nMeanwhile, \\(N_{C_j^{(z)}}(0,\\Sigma_{\\theta_j})\\) is the \\(C_j^{(z)}\\)-dimensional multivariate normal distribution with covariance matrix \\(\\Sigma_{\\theta_j}\\) defined by a Gaussian kernel function with parameters \\(\\theta_j\\). Similarly, \\(Gamma^d(a,b)\\) is the joint prior over the GP parameters and is the product of \\(d\\) Gamma distributions.\nAbove, \\(\\theta_j\\) is the parameter vector for the GP expert assigned to cluster \\(j\\), while \\(\\alpha\\) is the DP concentration parameter and \\(\\phi\\) is the parameter vector for the DP’s occupation number estimate. Note that \\(\\phi\\) is purely a vector of lengthscales for a Gaussian kernel. The priors on \\(\\theta\\), \\(\\alpha\\), and \\(\\phi\\) are described below.\n\\[\\theta_{j_k}\\stackrel{ind}{\\sim} Gamma(a_k,b_k) \\text{ for } k=1,\\dots,d\\] \\[\\alpha\\sim Inv.Gam(1,1),\\text{   } \\phi_k\\stackrel{iid}{\\sim} LogN(0,1) \\text{ for } k=1,\\dots,d\\] That is, each element \\(k\\) of \\(\\theta_j\\) (the dimension of \\(X\\) plus a noise parameter) is assigned a independent Gamma prior with fixed parameters \\(a_k\\) and \\(b_k\\). Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of \\(\\phi\\) receives an independent log-normal prior."
  },
  {
    "objectID": "index.html#description-of-simulated-data",
    "href": "index.html#description-of-simulated-data",
    "title": "Weekly Reports",
    "section": "Description of Simulated Data",
    "text": "Description of Simulated Data\nThis week, I simulated a sample data set for use with the iMGPE algorithm. Whereas with the motorcycle data set we lack knowledge of the true generating function, the simulated data set is generated by a known custom function. Assuming that \\(y=f(X)+\\epsilon(X)\\), for the simulated data set we can compare the fitted estimate of \\(f(X)\\) with the truth. The generating function is \\[f(x)= \\begin{cases}\n\\frac{7}{3}-\\frac{2}{3}x\\text{ if } x\\leq 2\\\\\n\\cos(\\pi x)\\text{ if } x&gt;2\n\\end{cases}\\]\nI drew a random uniform sample of \\(x\\) of size \\(100\\) from the interval \\((0,5)\\) and added iid random noise \\(\\epsilon\\) drawn from a \\(N(0,0.04)\\) distribution. With \\(y=f(X)+\\epsilon\\), the simulated data set and the function \\(f\\) are plotted below."
  },
  {
    "objectID": "index.html#application-of-imgpe",
    "href": "index.html#application-of-imgpe",
    "title": "Weekly Reports",
    "section": "Application of iMGPE",
    "text": "Application of iMGPE\nI was using the ‘darg’ and ‘garg’ functions from the ‘laGP’ R package to select priors for the GP lengthscale and nugget parameters. However, the ‘garg’ function was setting a \\(Gamma(0,0)\\) prior on the nugget. For the purposes of fitting a GP, this is treated as no prior on the nugget, but I was drawing values from these priors elsewhere in the algorithm and ‘rgamma(n,0,0)’ always returns \\(0\\). Once I corrected this by specifying a vague prior for the nugget should ‘garg’ fail, the algorithm’s behavior changed significantly with respect to the value of \\(\\alpha\\) and the number of clusters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor every fifth iteration, I took the fitted values of the response at each point in time, resulting in a sample of 1000 fitted values for each point. I took the 2.5th and 97.5th quantiles of this sample to get a credible interval for the fitted value at each point. Below, I have plotted the true data as black dots, the mean fitted values as a red line, and the \\(95\\%\\) credible intervals as the gray ribbon. As the generating function was known, that too is plotted as a blue line.\n\n\n\n\n\n\n\n\n\nThe fitted mean line is following the true function fairly closely, though the credible bounds are still wider than seems reasonable.\n\nApril 17, 2025"
  },
  {
    "objectID": "index.html#summary-9",
    "href": "index.html#summary-9",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I improved my breakdown of the data model and identified the base distribution of the Dirichlet process, which I believe to be a multivariate normal distribution. I detailed the derivation of the pseudo-posterior density for \\(\\phi\\) and explain that the full conditional density is never directly defined and thus not trivial to derive.\nI also determined why my trace plot for \\(\\alpha\\) seemed to hit a ‘ceiling’, which was that the proposal distribution for my quantile slice sampler was misspecified. I substituted a proposal with heavier tails and the apparent ceiling disappeared. However, this had significant repercussions on my parameter values. Now, \\(\\alpha\\) took on much larger average values, and the number of clusters increased dramatically. This resulted in many more experts fit to only a handful of data points, which had negative repercussions for the accuracy of the fitted model."
  },
  {
    "objectID": "index.html#data-model-2",
    "href": "index.html#data-model-2",
    "title": "Weekly Reports",
    "section": "Data Model",
    "text": "Data Model\nWe have an \\(n\\times 1\\) continuous response vector \\(y\\) and an \\(n\\times d\\) data matrix \\(X\\). The estimated value of a data point \\(y_i\\) under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing \\(y_i\\) and weighted by a Dirichlet process. Let \\(z\\) represent a possible vector of cluster assignments and \\(j\\) index the clusters within \\(z\\). Let \\(C_j\\) be the number of observations in cluster \\(j\\). Then we have as follows.\n\\[y\\sim \\sum_z \\left[\\prod_j N_{C_j}(0,\\Sigma_{\\theta_j}) \\right] w_z\\] \\[w\\sim DP_{\\phi}(\\alpha, N_n(0,I_n))\\] where \\(w\\) is a vector of probabilities defining a multinomial distribution drawn from a Dirichlet process and \\(w_z\\) is the probability of observing cluster assignment \\(z\\). That is, \\(P(z=Z|\\alpha,\\phi)=w_Z\\) and so \\(z|\\alpha,\\phi\\) is generated by a \\(DP_{\\phi}(\\alpha, N_n(0,I_n))\\) clustering process. Meanwhile, \\(N_{C_j}(0,\\Sigma_{\\theta_j})\\) is the \\(C_j\\)-dimensional multivariate normal distribution with covariance matrix \\(\\Sigma_{\\theta_j}\\) defined by a Gaussian kernel function with parameters \\(\\theta_j\\). Similarly, \\(N_n(0,I_n)\\) is the \\(n\\)-dimensional normal distribution where \\(I_n\\) is the \\(n\\)-dimensional identity matrix.\nAbove, \\(\\theta_j\\) is the parameter vector for the GP expert assigned to cluster \\(j\\), while \\(\\alpha\\) is the DP concentration parameter and \\(\\phi\\) is the parameter vector for the DP’s occupation number estimate. Note that \\(\\phi\\) is purely a vector of lengthscales for a Gaussian kernel. The priors on \\(\\theta\\), \\(\\alpha\\), and \\(\\phi\\) are described below.\n\\[\\theta_{j_k}\\stackrel{ind}{\\sim} Gamma(a_k,b_k)\\] \\[\\alpha\\sim Inv.Gam(1,1),\\text{   } \\phi_k\\stackrel{iid}{\\sim} LogN(0,1)\\] That is, each element \\(k\\) of \\(\\theta_j\\) (the dimension of \\(X\\) plus a noise parameter) is assigned a independent Gamma prior with fixed parameters \\(a_k\\) and \\(b_k\\). Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of \\(\\phi\\) receives an independent log-normal prior."
  },
  {
    "objectID": "index.html#handling-alpha-and-phi",
    "href": "index.html#handling-alpha-and-phi",
    "title": "Weekly Reports",
    "section": "Handling Alpha and Phi",
    "text": "Handling Alpha and Phi\nLast week, the trace plot of \\(\\alpha\\) appeared to hit a ‘ceiling’ at around \\(3.67\\). After some experimentation, I determined that my quantile slice sampling function was numerically unstable above that ceiling. The reason was that the quantile slice sampler relies on a proposal distribution and my initial choice of \\(Gamma(1,10)\\) was too light-tailed. I chose a more reasonable proposal distribution of \\(Gamma(1,1)\\) and the instability disappeared.\nWith regard to \\(\\phi\\), we wanted to know its true full conditional distribution. The conditional pseudo-posterior distribution for \\(\\phi\\) that we draw from in its Gibbs sampling step is shown on the right side of the equation below. We use it for now because \\(p(z|y,\\phi,\\alpha)\\) is not defined directly; only the conditionals \\(p(z_i|y,\\phi,\\alpha)\\) are defined. As before, the prior \\(p(\\phi)\\) is a log-normal density with parameters \\(\\mu=0\\) and \\(\\sigma^2=1\\).\n\\[p(\\phi|z,\\alpha,\\dots)\\propto p(z|y,\\phi,\\alpha)p(\\phi)\\approx \\left[\\prod_{i=1}^n p(z_i|y,\\phi,\\alpha) \\right] p(\\phi)\\] The individual conditionals are as follows, taking \\(j\\) to be the current value of \\(z_i\\) and \\(y^{(j)}\\) to be the subset of \\(y\\) belonging to cluster \\(j\\).\n\\[p(z_i=j|y,z_{-i},\\dots) \\propto p(y^{(j)}|z_i=j,z_{-i},\\theta_j)p(z_i=j|\\phi,\\alpha)\\] \\[\\text{where } p(y^{(j)}|z_i=j,z_{-i},\\theta_j)=N_{C_j}(0,\\Sigma_{\\theta_j}) \\text{ and}\\] \\[p(z_i=j|z_{-i},\\phi,\\alpha)=\\frac{n-1}{n-1+\\alpha}\\frac{\\sum_{i'\\neq i,z_{i'}=j} K_{\\phi}(X_i,X_{i'})}{\\sum_{i'\\neq i} K_{\\phi}(X_i,X_{i'})}\\]\nSince \\(p(y^{(j)}|\\cdot)\\) does not depend on \\(\\phi\\), it factors out of the posterior density and we are left with \\(p(\\phi|z,\\alpha,\\dots)\\approx \\left[\\prod_{i=1}^n p(z_i|y,\\phi,\\alpha) \\right] p(\\phi)\\)."
  },
  {
    "objectID": "index.html#practical-experiment",
    "href": "index.html#practical-experiment",
    "title": "Weekly Reports",
    "section": "Practical Experiment",
    "text": "Practical Experiment\nI set the proposal distribution for \\(\\alpha\\) to \\(Gamma(1,1)\\) and ran the algorithm on the motorcycle dataset again. The concentration parameter \\(\\alpha\\) is now centered around \\(15\\) rather than \\(3\\), with a commensurate effect on the number of clusters, which now averages around \\(30\\). Though \\(\\alpha\\)’s trace plot now appears properly mixed, I am not sure I like the effect on the cluster output. With so many clusters and only \\(94\\) data points, most clusters now contain only \\(2\\) or \\(3\\) data points.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor every fifth iteration, I took the fitted values of the response at each point in time, resulting in a sample of 1000 fitted values for each point. I took the 2.5th and 97.5th quantiles of this sample to get a credible interval for the fitted value at each point. Below, I have plotted the true data as black dots, the mean fitted values as a red line, and the \\(95\\%\\) credible intervals as the gray ribbon.\n\n\n\n\n\n\n\n\n\nThe fitted mean line is not following the data very well, and the credible band is much wider than before. I suspect this loss of precision is due to the excessive number of experts fit to little data.\n\nApril 10, 2025"
  },
  {
    "objectID": "index.html#summary-10",
    "href": "index.html#summary-10",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week, I expanded upon my description of the data model. I also corrected some numerical stability issues in the \\(\\phi\\) update step that resulted in significant improvements in its acceptance rate. Lastly, I plotted a heatmap of cluster memberships that displayed which points were most often clustered together."
  },
  {
    "objectID": "index.html#full-data-model",
    "href": "index.html#full-data-model",
    "title": "Weekly Reports",
    "section": "Full Data Model",
    "text": "Full Data Model\nThe estimated value of a data point \\(y_i\\) under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing \\(y_i\\) and weighted by a Dirichlet process. Letting \\(\\mathbf{z}\\) represent a possible vector of cluster assignments and \\(j\\) index the clusters within \\(\\mathbf{z}\\), we have as follows.\n\\[\\mathbf{y}\\sim \\sum_{\\mathbf{z}} \\left[\\prod_j GP_j(\\theta_j) \\right] w_{\\mathbf{z}}\\] \\[\\mathbf{w}\\sim DP(\\alpha, GP(\\phi))\\] where \\(\\mathbf{w}\\) is a vector of probabilities defining a multinomial distribution drawn from a Dirichlet process and \\(w_{\\mathbf{z}}\\) is the probability of observing cluster assignment \\(\\mathbf{z}\\). That is, \\(P(\\mathbf{z}=Z|\\alpha,\\phi)=w_Z\\) and so \\(\\mathbf{z}|\\alpha,\\phi \\sim DP(\\alpha, GP(\\phi))\\).\nAbove, \\(\\theta_j\\) is the parameter vector for the GP expert assigned to cluster \\(j\\), while \\(\\alpha\\) is the DP concentration parameter and \\(\\phi\\) is the parameter vector for the DP’s base distribution. Note that \\(\\phi\\) is purely a vector of lengthscales; the nugget is assumed to be zero. The priors on \\(\\theta\\), \\(\\alpha\\), and \\(\\phi\\) are described below.\n\\[\\theta_{j_k}\\stackrel{ind}{\\sim} Gamma(a_k,b_k)\\] \\[\\alpha\\sim Inv.Gam(1,1),\\text{   } \\phi_k\\stackrel{iid}{\\sim} LogN(0,1)\\] That is, each element \\(k\\) of \\(\\theta_j\\) (the dimension of \\(X\\) plus a noise parameter) is assigned a independent Gamma prior with fixed parameters \\(a_k\\) and \\(b_k\\). Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of \\(\\phi\\) receives an independent log-normal prior."
  },
  {
    "objectID": "index.html#distribution-functions",
    "href": "index.html#distribution-functions",
    "title": "Weekly Reports",
    "section": "Distribution Functions",
    "text": "Distribution Functions\nThe iMGPE algorithm is a Gibbs-sampling method, which means we must draw a new value of \\(\\phi\\) and \\(\\alpha\\) from their conditional distributions each iteration. The conditional distribution of \\(\\alpha\\) only depends on the number of data points \\(N\\) and the number of clusters \\(J\\), as seen here.\n\\[p(\\alpha|n, J,\\dots)\\propto \\alpha^{J-3/2}\\exp(-1/2\\alpha)\\Gamma(\\alpha)/\\Gamma(n+\\alpha)\\]\nThe conditional distribution for \\(\\phi\\) is more difficult to obtain, as the Dirichlet process is only defined through the following conditional probabilities.\n\\[p(z_i=j|z_{-i},\\phi,\\alpha)=\\frac{n-1}{n-1+\\alpha}\\frac{\\sum_{i'\\neq i,z_{i'}=j} K_{\\phi}(X_i,X_{i'})}{\\sum_{i'\\neq i} K_{\\phi}(X_i,X_{i'})}\\] \\[p(z_i\\neq z_{i'}\\text{ for all }i'\\neq i|z_{-i},\\alpha) =\\frac{\\alpha}{n-1+\\alpha} \\]\nTherefore, Rasmussen et al chose to sample from the pseudo-posterior of \\(\\phi\\), defined as the leave-one-out pseudo-likelihood (the product of the conditional probabilities of the \\(z_i\\)) and the prior on \\(\\phi\\). That is,\n\\[p^*(\\phi|\\mathbf{z},\\alpha)=\\prod_{i=1}^n p(z_i=j|\\phi,\\alpha)\\times p(\\phi)\\]"
  },
  {
    "objectID": "index.html#practical-experiments",
    "href": "index.html#practical-experiments",
    "title": "Weekly Reports",
    "section": "Practical Experiments",
    "text": "Practical Experiments\nLast week, I had difficulties with a low acceptance rate in \\(\\phi\\)’s random walk sampler. After examining my code, I determined that the cause was numerical instability in my R function for calculating \\(\\log(p^*(\\phi|\\cdot))\\). Once I revised my posterior density function to replace values of ‘-Inf’ with \\(-1000\\), the sampler achieved an acceptance rate of \\(34\\%\\).\nWith these revisions, I reran the motorcycle example from before.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe trace plots for both \\(\\alpha\\) and \\(\\phi\\) now appear to be well mixed, with \\(\\phi\\) achieving an acceptance rate of \\(34\\%\\). As before, for every fifth iteration, I took the fitted values of the response at each point in time, resulting in a sample of 1000 fitted values for each point. I took the 2.5th and 97.5th quantiles of this sample to get a credible interval for the fitted value at each point. Below, I have plotted the true data as black dots, the mean predicted values as a red line, and the \\(95\\%\\) credible intervals as the gray ribbon.\n\n\n\n\n\n\n\n\n\nThe credible interval is still unusually wide, especially around the ‘dip’ at time 20.\nThis week I also prepared a visual to display ‘standard cluster memberships’. Below is shown a heatmap displaying the frequency of each pair of data points belonging to the same cluster. Points are ordered by time, so they appear in the same order from left to right as in the plot above. There are two distinct clusters near the beginning.\n\n\n\n\n\n\n\n\n\n\nApril 3, 2025"
  },
  {
    "objectID": "index.html#summary-11",
    "href": "index.html#summary-11",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I elaborated upon the iMGPE model and further improved the algorithm. The trace plot for \\(\\phi\\) now appears to be converging. I should be able to convert the algorithm to Rcpp this week."
  },
  {
    "objectID": "index.html#data-model-3",
    "href": "index.html#data-model-3",
    "title": "Weekly Reports",
    "section": "Data Model",
    "text": "Data Model\nWe have an explanatory data matrix \\(\\mathbf{x}\\) and a scalar response \\(\\mathbf{y}\\), indicators \\(z_i\\), \\(i=1,\\dots,n\\), representing the cluster assignments for each point. Clusters are determined by a Dirichlet process with concentration parameter \\(\\alpha\\) and a Gaussian kernel with lengthscale parameter vector \\(\\phi\\). For each Gaussian process expert indexed by \\(j\\), we have a parameter vector \\(\\theta_j\\). Given all this, the likelihood of the data is a sum over exponentially many possible cluster assignments.\n\\[p(\\mathbf{y}|\\mathbf{x},\\theta)= \\sum_{\\mathbf{z}}\\left[ \\prod_j p(\\{y_i:z_i=j\\}| \\{x_i:z_i=j\\},\\theta_j) \\right]p(\\mathbf{z}|\\mathbf{x},\\alpha,\\phi)\\]\nIn the above likelihood, \\(p(\\{y_i:z_i=j\\}| \\{x_i:z_i=j\\},\\theta_j)\\) is the likelihood of the Gaussian process fit to cluster \\(j\\). The iMGPE algorithm performs Gibbs sampling of this likelihood. Lastly, \\(\\alpha\\), \\(\\phi\\), and all \\(\\theta_j\\) are assigned priors. We give \\(\\alpha\\) a vague inverse gamma prior, while each component of \\(\\phi\\) is assigned a vague gamma prior. The components of the \\(\\theta_j\\) each receive a gamma prior as well, so that, for example, the nugget parameters of every expert share the same prior."
  },
  {
    "objectID": "index.html#updated-results",
    "href": "index.html#updated-results",
    "title": "Weekly Reports",
    "section": "Updated Results",
    "text": "Updated Results\nI adjusted the update step for \\(\\phi\\). It still uses a Random Walk Metropolis-Hastings sampler with with a normal proposal, but the variance of the proposal is set to the optimal value of \\(\\frac{2.38^2}{d}\\mathbf{H}^{-1}\\) where \\(d\\) is the number of dimensions and \\(\\mathbf{H}\\) is the Hessian matrix of the posterior density for \\(\\phi\\). I realized that I had forgotten to invert the Hessian in prior tests, which is why I was getting very small proposal variances.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe posterior distribution of \\(\\phi\\) has changed significantly, and now has a mean of about \\(88\\). Its acceptance probability is still very low, around \\(3\\%\\).\nFor every fifth iteration, I took the fitted values of the response at each point in time, resulting in a sample of 1000 fitted values for each point. I took the 2.5th and 97.5th quantiles of this sample to get a credible interval for the fitted value at each point. Below, I have plotted the true data as black dots, the mean predicted values as a red line, and the \\(95\\%\\) credible intervals as the gray ribbon.\n\n\n\n\n\n\n\n\n\n\nMarch 27, 2025"
  },
  {
    "objectID": "index.html#summary-12",
    "href": "index.html#summary-12",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I elaborated upon the iMGPE model and further improved the algorithm. The trace plot for \\(\\phi\\) now appears to be converging. I should be able to convert the algorithm to Rcpp this week."
  },
  {
    "objectID": "index.html#changes-to-dp-parameter-updates",
    "href": "index.html#changes-to-dp-parameter-updates",
    "title": "Weekly Reports",
    "section": "Changes to DP Parameter Updates",
    "text": "Changes to DP Parameter Updates\nI rewrote the code for updating the Dirichlet process parameters \\(\\alpha\\) and \\(\\phi\\). First, I implemented a slice sampling update for \\(\\alpha\\) and attempted the same for \\(\\phi\\), though I wasn’t able to debug that in time. Instead, I adjusted the update step for \\(\\phi\\) by implementing a minimum standard deviation for the proposal density of its rejection sampling algorithm.\nCurrently, the proposal is a normal density with variance equal to the hessian of the density function for \\(\\phi\\). This was recommended by Rasmussen et al since it removes the need for a tunable parameter, but I have found that the Hessian produces extremely small proposal variances, leading to extremely small step sizes. After making the changes described, I fit the model to the motorcycle dataset with 5000 iterations. The trace plots for \\(\\alpha\\) and \\(\\phi\\) after these adjustments are shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe trace plot for \\(\\alpha\\) looks good, but the one for \\(\\phi\\) failed to converge. It may be that the average step size is still too small."
  },
  {
    "objectID": "index.html#updated-results-1",
    "href": "index.html#updated-results-1",
    "title": "Weekly Reports",
    "section": "Updated Results",
    "text": "Updated Results\nThe distribution of cluster counts is shown below. It is roughly the same as in last meeting, concentrating around 6 to 8 clusters.\n\n\n\n\n\n\n\n\n\nFor every fifth iteration, I predicted the value of the response at each point in time. I took the 2.5th and 97.5th quantiles of this sample to get a credible interval for the fitted value at each point. Below, I have plotted the true data as black dots, the mean predicted values as a red line, and the \\(95\\%\\) credible intervals as the gray ribbon.\n\n\n\n\n\n\n\n\n\nThe credible interval bounds are much narrower than the prediction intervals plotted in the last meeting, especially for the early times.\n\nMarch 13, 2025"
  },
  {
    "objectID": "index.html#summary-13",
    "href": "index.html#summary-13",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I improved the update steps for the parameters \\(\\alpha\\) and \\(\\phi\\) in the Infinite Mixture of GP Experts function. I also altered the function output to report a credible interval for the value of each point in the data set.\nI have begun converting parts of the iMGPE algorithm to C++ with Rcpp. Performance gains are modest for now, but may increase in the future."
  },
  {
    "objectID": "index.html#initial-results",
    "href": "index.html#initial-results",
    "title": "Weekly Reports",
    "section": "Initial Results",
    "text": "Initial Results\nI tested the algorithm first on the motorcycle crash dataset, a simple dataset of the acceleration of the head of a crash test dummy over time as its car hit a wall. There was thus one continuous input, time, and one continuous output, acceleration. Below is a plot of the motorcycle dataset.\n\n\n\n\n\n\n\n\n\nI fit a model to this dataset using the iMGPE algorithm, which ran for 5000 iterations. In each iteration, this algorithm generated a random assignment of data points to clusters using a modified Dirichlet process, fit a Gaussian process model to each cluster, and updated the cluster assignment parameters using MCMC. The posterior distributions of those parameters are discussed below, along with some other key features of the fitted model.\nThe cluster assignment probabilities were controlled by two parameters, \\(\\alpha\\) and \\(\\phi\\), where \\(\\alpha\\) is the Dirichlet process concentration parameter and \\(\\phi\\) is the lengthscale vector for a Gaussian kernel. In this example, \\(\\phi\\) is scalar since there is only one input variable. Their trace plots are shown here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBelow is a histogram of the number of clusters generated in each iteration. The algorithm generally used between six and seven clusters.\n\n\n\n\n\n\n\n\n\nAt each step of the algorithm, I drew a random sample from the posterior distributions of each data point, for a new set of 94 points. I discarded the first 2500 draws as burn-in and recorded the mean, median, and the 2.5th and 97.5th quantiles for each data point. These are plotted below, alongside the original data. The red line is the median and the grey ribbon covers the \\(95\\%\\) confidence band."
  },
  {
    "objectID": "index.html#conclusions",
    "href": "index.html#conclusions",
    "title": "Weekly Reports",
    "section": "Conclusions",
    "text": "Conclusions\nThe trace plots for \\(\\alpha\\) and \\(\\phi\\) indicate some problems with their MH updates. The plot for \\(\\phi\\) clearly shows that new proposals are being rejected far too often. I will have to experiment with the acceptance probability to bring it into a reasonable range. The plot for \\(\\alpha\\) could indicate that the step size of their random walk is too small, another parameter that may need fine tuning.\nRasmussen and Ghahramani’s own experiment on this data set yielded a much smoother median line. My algorithm may be overfitting compared to theirs.\nFurthermore, it is not clear from Rasmussen’s paper if or how they intended to perform inference on new data given their model. Inference could be performed for any given set of experts but to draw from a posterior predictive distribution the same way the authors draw from the posterior would require the new data to be available during the model fitting process.\n\nFebruary 27, 2025"
  },
  {
    "objectID": "index.html#summary-14",
    "href": "index.html#summary-14",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I completed coding a version of the Infinite Mixture of GP Experts model proposed by Rasmussen and Ghahramani. I fit it to the motorcycle crash dataset, an example also used in the original paper. The algorithm is performing as intended, though the update steps for the Dirichlet process parameters may need to be adjusted."
  },
  {
    "objectID": "index.html#theory-of-algorithm",
    "href": "index.html#theory-of-algorithm",
    "title": "Weekly Reports",
    "section": "Theory of Algorithm",
    "text": "Theory of Algorithm\nFor this model we have \\(y\\), a vector of \\(n\\) outputs, and \\(X\\), an \\(n\\times d\\) matrix of inputs. The data will be partitioned into \\(J\\) clusters and a GP expert fit to each cluster. Cluster assignments are represented by a vector of indicator variables \\(z={z_i: i=1,\\dots,n}\\). These values are controlled by a Dirichlet process with concentration parameter \\(\\alpha\\) and a kernel function parameterized by a lengthscale vector \\(\\phi\\). The parameters of each GP expert are represented by the vector \\(\\theta_j\\), with \\(\\theta\\) representing all GP parameters.\nTo begin, \\(y\\) and \\(X\\) are given and we place priors on \\(\\alpha\\), \\(\\phi\\), and \\(\\theta\\) and choose an initial cluster assignment for \\(z\\). The authors suggest a vague inverse Gamma prior for \\(\\alpha\\) and vague independent log-normal priors for \\(\\phi\\). Then, we run a Gibbs sampling sweep over each data point, updating their cluster assignments with Dirichlet process clustering. The conditional probability of point \\(i\\) being assigned to a cluster \\(j\\), is expressed below. This probability is in two parts, the second of which is input dependent and can be modified to accommodate qualitative inputs without much trouble. The first part factors into the conditional probabilities of one output given all other outputs in the expert.\n\\[p(z_i=j|z_{-i},X,y,\\theta,\\phi,\\alpha) \\propto p(y|z_i=j,z_{-i},X,\\theta)p(z_i=j|z_{-i}, x,\\phi,\\alpha)\\] \\[= p(y_i|y_{-i},x_j,\\theta_j)p(z_i=j|z_{-i}, x,\\phi,\\alpha)\\] \\[\\text{where } p(z_i=j|z_{-i},X,\\phi,\\alpha)=\\frac{n-1}{n-1+\\alpha}\\frac{\\sum_{i'\\neq i,z_{i'}=j} K_{\\phi}(X_i,X_{i'})}{\\sum_{i'\\neq i} K_{\\phi}(X_i,X_{i'})}\\]\nWhere \\(K_{\\phi}\\) is a Gaussian kernel function with lengthscales \\(\\phi\\). I am not sure how best to sample the conditional probabilities above, given that they depend on the covariance matrix between all points except \\(i\\) and given that the Gibbs sampling updates gradually change which points are assigned to which cluster. The authors remark that they can “reuse [covariance matrices] for consecutive Gibbs updates by performing rank one updates (since Gibbs sampling changes at most one indicator at a time),” but do not explain how to perform such updates.\nThen, we fit a Gaussian process to each cluster. I have been using the package ‘laGP’ to fit a standard squared exponential model. However, ‘laGP’ doesn’t let me set priors for the GP parameters, so I may need to use a different package. I do not plan to include qualitative inputs in this step to save on computation time.\nWe then update the parameters of our gating function, the DP concentration parameter \\(\\alpha\\) and the gating kernel lengthscales \\(\\phi\\). The posterior distribution of \\(\\alpha\\), below, is sampled using Adaptive Rejection Sampling.\n\\[p(\\alpha|n, J)\\propto \\alpha^{J-3/2}\\exp(-1/2\\alpha)\\Gamma(\\alpha)/\\Gamma(n+\\alpha)\\]\nFor \\(\\phi\\), we sample from the pseudo-posterior, which is the product of the conditional distributions of the indicator variables and the prior. We use vague independent log-normal priors for \\(\\phi\\), with parameters \\(\\mu=0\\) and \\(\\sigma^2=1\\).\n\\[p^*(\\phi|\\alpha, z, x) = \\left[\\prod_{i=1}^n p(z_i=j|z_{-i},\\phi,\\alpha)\\right] p(\\phi)\\]\nThis completes one iteration of the iMGPE algorithm. We repeat until all parameters have converged."
  },
  {
    "objectID": "index.html#future-work",
    "href": "index.html#future-work",
    "title": "Weekly Reports",
    "section": "Future Work",
    "text": "Future Work\nAs stated above, the algorithm is incomplete. Once I cross the last hurdle, I will be able to begin testing the accuracy and efficiency of this method on different datasets. I can experiment with different approaches to involving qualitative inputs in the gating function to see how they affect the speed and performance of the algorithm.\n\nFebruary 13, 2025"
  },
  {
    "objectID": "index.html#summary-15",
    "href": "index.html#summary-15",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I worked on developing R code to run the iMGPE method by Rasmussen and Ghagramani. Most of the algorithm is complete, but I am not sure how to calculate a conditional probability necessary for the Gibbs sampling update of the indicator variables."
  },
  {
    "objectID": "index.html#summary-of-alternative-imgpe",
    "href": "index.html#summary-of-alternative-imgpe",
    "title": "Weekly Reports",
    "section": "Summary of Alternative iMGPE",
    "text": "Summary of Alternative iMGPE\nThe 2005 paper “An Alternative Infinite Mixture of Gaussian Process Experts” by Meeds and Osindero presents an extension of the iMGPE approach using generative modeling. The generative approach to Mixture of Experts modeling assumes that the experts generate the inputs which generate the outputs, rather than conditioning the experts on the inputs. This technique can handle missing or incomplete data easily and allows for reverse-conditioning: assessing where in the input space a particular output is likely to have originated.\nThe generative model is most easily explained as a data generation algorithm comprised of a series of conditionals. To generate a set of \\(N\\) data points, we would take the following steps.\n\nSample the Dirichlet process concentration parameter \\(\\alpha_0\\) from a prior.\nPartition a set of \\(N\\) objects using a Dirichlet process, denoting the partitions with the indicator variables \\(\\{z_i\\}_{i=1}^N\\) taking values \\(r=1,\\dots,E\\).\nSample the gate hyperparameters \\(\\phi\\) from their priors.\nFor each partition, sample the input space parameters \\(\\psi_r\\) conditioned on \\(\\phi\\). These define the density in each input space.\nGiven the parameters for each group, sample the locations of the input points \\(X_r=\\{x_i:z_i=r\\}\\).\nFor each group, sample the hyperparameters \\(\\theta_r\\) of the GP associated with it.\nGiven \\(X_r\\) and \\(\\theta_r\\) for each group, formulate the GP output covariance matrix and sample the set of output values.\n\nInference for this model is accomplished through an MCMC algorithm to identify the expert partition and hyperparameters most likely to have generated the training inputs and output. Qualitative inputs could possibly be incorporated into the GP experts. This would require defining a generative multinomial (or similar) distribution for each qualitative input that allows correlation between them and the quantitative inputs, as well as priors for the hyperparameters of those distributions."
  },
  {
    "objectID": "index.html#summary-of-variational-inference",
    "href": "index.html#summary-of-variational-inference",
    "title": "Weekly Reports",
    "section": "Summary of Variational Inference",
    "text": "Summary of Variational Inference\nThe paper “Variational Inference for Infinite Mixture of Gaussian Processes” by Sun and Xu further develops the generative mixture of experts method by employing a form of variational inference called mean field approximation to evaluate the model parameters. This is a method of approximating a probability distribution that can serve as a faster alternative to MCMC approximation.\nTake some data \\(X\\) generated by some latent variables \\(\\Omega=(\\alpha_0, \\phi)\\) through a hierarchical model. Suppose we have a joint prior on \\(\\Omega\\), \\(P(\\Omega)\\) and we want to approximate the posterior \\(P(\\Omega|X)\\). Standard variational inference defines a distribution \\(Q(\\Omega)\\) over \\(\\Omega\\) to be of a certain family of distributions similar to the posterior distribution. The similarity between them is measured through a dissimilarity function \\(d(P;Q)\\) and inference is performed by selecting \\(Q(\\Omega)\\) so as to minimize \\(d(P;Q)\\).\nFor mean field approximation, \\(Q(\\Omega)\\) is assumed to factorize over some partition of the unobserved variables \\(Z_1,\\dots,Z_M\\). It can be shown that the best distribution \\(q_m^*\\) for some factor \\(q_m\\), in terms of Kullback-Leibler divergence, is\n\\[q_m^*(\\Omega_m|X)= \\frac{e^{E_{q^*_{-m}}[\\ln p(\\Omega,X)]}}{\\int e^{E_{q^*_{-m}}[\\ln p(\\Omega,X)]} d\\Omega_m}\\]\nor equivalently,\n\\[\\ln q_m^*(\\Omega_m|X) = E_{q^*_{-m}}[\\ln p(\\Omega,X)] + \\text{constant}\\]\nwhere \\(E_{q^*_{-m}}[\\ln p(\\Omega,X)]\\) is the expectation of the logarithm of the joint distribution over all data and variables taken with respect to \\(q^*\\) over all variables not in the partition. This expectation can usually be determined to be of a known type of distribution. The factors can then be iteratively updated, much like the E-M algorithm.\nIn Sun and Xu’s case, they formulate the joint distribution of the inputs and outputs and then perform variational inference on each hyperparameter in turn. Their method also differs from the Alternative iMGPE by learning a support set for each GP expert that serves as the training data for that expert. These support sets are conditioned on the inputs and the expert partitions but are not synonymous with them."
  },
  {
    "objectID": "index.html#viability-for-qualitative-inputs",
    "href": "index.html#viability-for-qualitative-inputs",
    "title": "Weekly Reports",
    "section": "Viability for Qualitative Inputs",
    "text": "Viability for Qualitative Inputs\nI am ambivalent towards the generative model proposed by Meeds and Osindero. It seems to be more popular than Rasmussen’s conditional model but I’m not sure its advantages are really useful in the case of WEPP emulation. Missing data is not a problem for us, nor is imputing inputs from outputs. Furthermore, it no longer makes sense to condition the Dirichlet process gating function on the inputs, meaning we can’t reduce the number of parameters by only incorporating the qualitative inputs into the gating function.\nVariational inference could be a faster alternative to MCMC, though the variational distributions have to be derived analytically, which could be challenging or even intractable depending on our model. Though Sun and Xu used a generative model, it is not needed to apply variational inference.\n\nFebruary 6, 2025"
  },
  {
    "objectID": "index.html#summary-16",
    "href": "index.html#summary-16",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I researched improvements on the iMGPE method proposed by Rasmussen and Ghahramani. I found two papers that looked particularly useful, a 2005 paper by Meeds and Osindero and a 2010 paper by Sun and Xu, which extend the approach through generative modeling and variational inference respectively. More recent papers exist, but are largely focused on directions not relevant to us, such as multivariate responses and general non-stationary probabilistic regression.\nI have not yet found code for the iMGPE method. In the meantime, I have begun replicating the algorithm from Rasmussen and Ghahramani’s original paper."
  },
  {
    "objectID": "index.html#mixture-of-experts-overview",
    "href": "index.html#mixture-of-experts-overview",
    "title": "Weekly Reports",
    "section": "Mixture of Experts Overview",
    "text": "Mixture of Experts Overview\nThe mixture of expert (ME) approach uses a set of expert models and a gating function to perform regression or classification on a dataset. The gating function makes a soft split of the input space, meaning that the partitioned regions may overlap. The experts are trained on the data, each one focusing on a partition. Parameter estimation for both the gating function and the experts is most often done through the Expectation-Maximization algorithm.\nAssume going forward that we have data \\(\\mathbf{x}\\) and response \\(\\mathbf{y}\\), that \\(n=1,\\dots ,N\\) is the number of data points, and \\(i=1,\\dots, I\\) is the number of experts. Let \\(\\theta=(\\theta_g,\\theta_e)\\) represent the parameters of the gating function and the experts. In this case, the probability of observing \\(\\mathbf{y}\\) given \\(\\mathbf{x}\\) is\n\\[P(\\mathbf{y}|\\mathbf{x},\\theta) = \\sum_{i=1}^I g_i(\\mathbf{x},\\theta_g)P(\\mathbf{y}|i, \\mathbf{x},\\theta_e)\\]\nFor regression, the gate \\(g_i(\\cdot)\\) is generally defined as the softmax function.\n\\[g_i(\\mathbf{x,v}) = \\frac{\\exp(\\beta_i(\\mathbf{x,v}))}{\\sum_{j=1}^I \\exp(\\beta_j(\\mathbf{x,v}))}\\] where \\(\\mathbf{v}\\) is the gate parameter and the functions of the gate parameters are linear: \\(\\beta_i(\\mathbf{x,v})=\\mathbf{v_i^T}[\\mathbf{x},1]\\). By introducing indicator variables \\(Z=\\{\\{z_i^{(n)}\\}_{n=1}^N\\}_{i=1}^I\\) representing the expert assignment for each observation, we can write the full log likelihood of the data and solve it.\n\\[l(\\mathbf{x},\\mathbf{y},Z,\\theta) = \\sum_{n=1}^n \\sum_{i=1}^I z_i^{(n)}\\left[ \\log g_i(\\mathbf{x^{(n)}}, \\theta_g) + \\log P_i(\\mathbf{y^{(n)}}) \\right]\\]\nNote that in the original ME formulation the parameters for the experts and the gating function are learned simultaneously and each expert is trained on all data, not just the data “assigned” to it."
  },
  {
    "objectID": "index.html#infinite-mixture-of-gp-experts",
    "href": "index.html#infinite-mixture-of-gp-experts",
    "title": "Weekly Reports",
    "section": "Infinite Mixture of GP Experts",
    "text": "Infinite Mixture of GP Experts\nThe 2002 paper “Infinite Mixtures of Gaussian Process Experts” by Rasmussen and Ghahramani describes a modification of the ME approach that uses a Dirichlet process for the gating function and a potentially infinite number of Gaussian processes as experts, which they call iMGPE. The Dirichlet process works similarly to the clustering algorithm described on November 5, 2024, but is made input dependent through a squared exponential kernel parameterized by a lengthscale \\(\\phi\\), rather than by a Gaussian density.\nIt places Bayesian priors on the model parameters: an inverse gamma prior for the DP concentration parameter \\(\\alpha\\), inverse gamma priors on the spatial variance and error variance of each expert, with common hyperparameters \\((a_1,b_1)\\) and \\((a_2,b_2)\\), and independent log normal priors for the lengthscale parameters of each expert and the gating kernel parameter \\(\\phi\\).\nThis approach then iterates through the following MCMC algorithm.\n\nInitialize indicator variables \\(z_i^{(n)}\\) to a single value or set of values.\nUpdate each of the indicators with a Gibbs sampling sweep.\nDo Monte Carlo estimation of the parameters of each GP expert in turn.\nOptimize the hyperparameters \\((a_1,b_1)\\) and \\((a_2,b_2)\\) of the GP variances.\nSample the Dirichlet process concentration parameter, \\(\\alpha\\), using adaptive rejection sampling.\nSample the gating parameter \\(\\phi\\).\nRepeat from step 2 until the MCMC output has converged.\n\nThis method has several advantages over the standard ME approach. It allows the Dirichlet process to determine the appropriate number of experts to represent the data rather than specifying a certain number of experts beforehand. It also fits each GP expert only to a subset of the data, speeding up computation significantly."
  },
  {
    "objectID": "index.html#application-to-qualitative-inputs",
    "href": "index.html#application-to-qualitative-inputs",
    "title": "Weekly Reports",
    "section": "Application to Qualitative Inputs",
    "text": "Application to Qualitative Inputs\nMy current objective is to incorporate qualitative inputs into Gaussian process regression in the context of large data sets. The mixture of experts approach, particularly the variation described by Rasmussen, is a promising avenue of research. The primary tension between qualitative inputs and big data is that every qualitative input requires many more parameters be estimated to capture the relationship between levels, which complicates and slows down model fitting. The iMGPE reduces computation time while having plenty of room for qualitative inputs.\nAs a mixture of experts model does not much care what its component experts are, the most straightforward approach to incorporating qualitative inputs is to incorporate them into the GP experts through any previously described approach (EC, MC, LV). The viability of this option would only depend on the complexity of the chosen approach and the minimum number of experts you permit.\nA second option would be to utilize qualitative inputs in the gating function, the Dirichlet process, but not in the experts themselves. This would save significantly on computation as their parameters would only be estimated once per iteration instead of for each expert. Such an approach would be similar to the Naive Local Expert models I have already presented, but with two distinctions: the data partitions are learned rather than decided a priori and use all inputs, not just the qualitative ones. The downside is that we cannot easily interpret the relationship between qualitative inputs and the response.\n\nJanuary 30, 2025"
  },
  {
    "objectID": "index.html#summary-17",
    "href": "index.html#summary-17",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I studied the Mixture of Experts approach to modeling large datasets and considered its compatibility with qualitative inputs. A 2012 paper by Seniha Yuksel, et al, explained the fundamentals of ME modeling and surveyed the available variations. This paper is available at IEEE.\nI considered the approaches described by Yuksel and settled on one developed by Rasmussen and Ghahramani, called an Infinite Mixture of GP Experts (iMGPE), which uses the Dirichlet Process as a gating function to partition the dataset among a potentially infinite number of Gaussian process experts. Their paper is available for download at the NIPS website.\nLastly, I considered how these methods could be applied to my own research. Options for incorporating qualitative inputs in a Mixture of Experts are presented, with an eye to minimizing computational complexity."
  },
  {
    "objectID": "index.html#latent-variable-method-example",
    "href": "index.html#latent-variable-method-example",
    "title": "Weekly Reports",
    "section": "Latent Variable Method Example",
    "text": "Latent Variable Method Example\nI return to the paper “A Latent Variable Approach to Gaussian Process Modeling with Qualitative and Quantitative Factors” by Zhang et al, available at https://www.tandfonline.com/doi/10.1080/00401706.2019.1638834\nI replicated one example from this paper related to beam bending: a metal beam with one of six different cross-sectional shapes is fixed at one end to a wall and downward pressure of \\(600\\) N is applied to the other end. The response is the amount of deformation in the beam. The sixth shape is known to behave significantly differently from the others, but this is not included in the model.\nI downloaded the author’s code and data and ran the example in MATLAB. It used a dataset with \\(60\\) observations of two quantitative variables and one qualitative variable with six levels. The RMSE was \\(8.46e-7\\) and the relative RMSE was \\(0.0623\\). The factor level latent variables were fit to 2D space as in the following plot:\n\n\n\n\n\n\n\n\n\n\n\n\nlevel\ncoords\n\n\n\n\n1\n(0,0)\n\n\n2\n(-0.0537,0)\n\n\n3\n(0.0121,-0.0001)\n\n\n4\n(-0.0290,0.0001)\n\n\n5\n(0.0403,-0.0007)\n\n\n6\n(0.7025,-0.1555)\n\n\n\nNote that the sixth factor level is far from the others, indicating that our model has successfully captured the underlying physical structure."
  },
  {
    "objectID": "index.html#qualitative-gp-with-big-data",
    "href": "index.html#qualitative-gp-with-big-data",
    "title": "Weekly Reports",
    "section": "Qualitative GP with Big Data",
    "text": "Qualitative GP with Big Data\nMost methods of fitting a GP with qualitative inputs are not designed for situations with large \\(n\\). While adding one more quantitative variable to a model rarely means more than one additional parameter to estimate, as with a separated squared exponential kernel function, adding one qualitative variable can easily add half a dozen or more parameters, depending on the number of levels it possesses. The exact number of parameters used varies significantly depending on the method of estimation chosen. Here, I compare some of the methods discussed on October 27 to the latent variable method.\nThe exchangeable covariance (EC) method uses just one parameter per factor variable. The unrestricted covariance (UC) method uses \\(\\sum_{j=1}^r m_j(m_j-1)/2\\) parameters, assuming \\(r\\) factors and \\(m_j\\) levels in the \\(r^{th}\\) factor. The multiplicative covariance (MC) method uses \\(\\sum_{j=1}^r m_j\\) parameters. The latent variable (LV) method uses \\(\\sum_{j=1}^r 2m_j-3\\) parameters. Thus, EC is best in terms of the number of parameters needed, followed by MC, but both are unable to model the full range of possible relationships between levels. The LV method, meanwhile, is superior to the UC method when at least one factor has four or more levels.\nIn an ideal situation we would be able to do the following:\nOne research direction available to us is to develop a statistical test to determine whether a qualitative variable can be safely excluded from a regression model. Similarly, we could develop a test to determine whether the correlation structure of a factor is significantly different from the exchangeable correlation structure.\nAnother is to investigate how any of the factor modeling methods described could be integrated into known big data techniques. We have attempted one form of integration with local expert models by stratifying by factor levels, but there may be better ways of combining these methods.\n\nJanuary 23, 2025"
  },
  {
    "objectID": "index.html#summary-18",
    "href": "index.html#summary-18",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I studied the latent variable approach to Gaussian process regression that was discussed on November 19. I downloaded the author’s code and ran it in MATLAB, replicating an example from the paper.\nI also considered how we might approach the task of GP regression with qualitative variables in situations with massive data or many inputs. I compared the known methods by the number of parameters needed and the flexibility of their correlation structures. I then discussed several ways to build upon the existing research."
  },
  {
    "objectID": "index.html#gp-vs-rf-on-simple-data",
    "href": "index.html#gp-vs-rf-on-simple-data",
    "title": "Weekly Reports",
    "section": "GP vs RF on Simple Data",
    "text": "GP vs RF on Simple Data\nI simulated a dataset consisting of one continuous variable x1 and one categorical variable x2. The categorical variable had three levels: \\({1,2,3}\\). The response was generated using a function proposed by Han, et al, where a different quadratic function of the continuous variable is used for each level of the categorical variable. I generated a training dataset with \\(900\\) observations, and two test datasets, each with 93. The x1 values in the training set and one of the test sets were generated from a \\(N(0,1)\\) distribution, while those in the second test set were evenly spaced between \\(-3\\) and \\(3\\). The training data is plotted below.\n\n\n\n\n\n\n\n\n\nI then stratified the training data by factor and compared fitting a GP to each group with fitting a random forest model to each. I compared the predictive accuracy of these models on both test sets, starting with the random set. For the random forest models, the RMSE was 0.68 and the MAE was 0.55. For the Gaussian process models, the RMSE was 0.30 and the MAE was 0.18. Comparing on the regularly spaced set, the RF models had an RMSE of 4.66 and an MAE of 2.35 and the GP models had an RMSE of 8.83 and an MAE of 4.38.\nThe x1 values of the random test set are clustered around zero, where most of the training data is, while those of the regular test set extend into regions with little training data. GPs seem to outperform RF models on predicting data points similar to the training data, but this relationship is reversed for data points outside the training data.\n\nJanuary 17, 2025"
  },
  {
    "objectID": "index.html#summary-19",
    "href": "index.html#summary-19",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week, I developed a tentative academic plan, which is attached separately. I also experimented with local expert methods on a simple dataset with no outliers. I compared using random forest models for each factor level to Gaussian process models and found that they performed equally well on a well-behaved dataset."
  },
  {
    "objectID": "index.html#stratified-random-forest-models",
    "href": "index.html#stratified-random-forest-models",
    "title": "Weekly Reports",
    "section": "Stratified Random Forest Models",
    "text": "Stratified Random Forest Models\nI took the positive soil loss dataset and tested the performance of random forest models when using a local expert technique. I used the soil loss data from the years 2009 and 2010 as the training set and the data from 2011 as the test set. First, I took the training set and stratified it by ‘crop’. Then I fit a random forest model to each strata, predicting soil loss with the variables ‘rad’, ‘tmin’, ‘precip’, ‘max_prcp’, and ‘slp_wavg4’. Then I predicted soil loss on the test set and recorded the RMSE and mean absolute error of the predictions. I repeated this, stratifying on ‘till’ and then on both ‘crop’ and ‘till’.\n\n\n\nModel(Factor)\nRMSE\nMAE\n\n\n\n\nRF(crop)\n17.82\n3.75\n\n\nRF(till)\n17.47\n3.66\n\n\nRF(both)\n17.57\n3.75\n\n\n\nThe stratified random forest models did not perform significantly differently from the RF models that included crop or till. The performance gap between the local expert GP models and the RF models is not due to this stratification. For stratification by ‘crop’, plots of the predicted by actual response are shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe local expert random forest models have a much narrower range in predicted values than the local expert GPs."
  },
  {
    "objectID": "index.html#model-structures-for-factor-fusion",
    "href": "index.html#model-structures-for-factor-fusion",
    "title": "Weekly Reports",
    "section": "Model Structures for Factor Fusion",
    "text": "Model Structures for Factor Fusion\nHere, I describe the model and prior correlation structure presented by Pauger and Wagner in their 2017 paper. We have a linear model based on the dummy variable expression of a categorical covariate with \\(c+1\\) levels. Let \\(B_0(\\delta,\\tau^2)\\) be the prior correlation matrix of the regression coefficients \\(\\beta_1,\\dots ,\\beta_c\\) with respect to a baseline category. That is, \\(\\mathbf{\\beta}\\sim N(0,B_0(\\delta, \\tau^2))\\). It depends on a scale parameter \\(\\tau^2\\) and a vector of binary indices \\(\\delta\\) with one element for each pair of levels that may be fused. \\[B_0(\\delta,\\tau^2)=\\gamma \\tau^2 Q^{-1}(\\delta)= \\gamma\\tau^2\\left(\\begin{array}{cccc}\n\\sum_{j\\neq 1} \\kappa_{1j} & -\\kappa_{12} & \\dots & -\\kappa_{1c}\\\\\n-\\kappa_{21} & \\sum_{j\\neq 2} \\kappa_{2j} & \\dots & -\\kappa_{2c}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n-\\kappa_{c1} & -\\kappa_{c2} & \\dots & \\sum_{j\\neq c} \\kappa_{cj}\n\\end{array}\\right)\\]\nHere, \\(\\gamma=c/2\\) is a fixed constant. The \\(\\kappa_{kj}\\) (for \\(k&gt;j\\)) are defined as \\(\\delta_{kj}+r(1-\\delta_{kj})\\) where \\(r\\) is a fixed large number called the precision ratio and \\(\\delta_{kj}\\) is the indicator variable for whether levels \\(k\\) and \\(j\\) are separate. Then \\(\\kappa_{jk}=\\kappa_{kj}\\). Note that this means two levels of a factor will be highly correlated if \\(\\delta_{kj}=0\\) and weakly correlated if \\(\\delta_{kj}=1\\).\nThe precision ratio is so called because it is the ratio of the maximum prior precision of \\(\\beta_k\\) to the minimum prior precision. Lastly, hyperpriors are assigned to \\(\\tau^2\\) and \\(\\delta\\), an inverse Gamma prior on \\(\\tau^2\\) and iid Bernoulli priors on the elements of \\(\\delta\\). After fitting the model, the elements of \\(\\delta\\) tell you which pairs of factor levels were fused.\nCruz-Reyes, in her 2023 paper, starts with the same model structure but models \\(\\mathbf{\\beta}\\sim N(\\mathbf{0},\\sigma^2Q^{-1})\\) where \\(\\sigma^2\\) is a scale parameter and the \\(c\\times c\\) precision matrix \\(Q\\) is defined according to a parameter vector \\(\\rho\\). The vector \\(\\rho\\) has \\(c+\\frac{c(c-1)}{2}\\) elements, one for each factor level and one for each pair of factor levels (though some of the latter elements may be set to zero). Then \\(Q\\) is as follows.\n\\[Q(\\rho) = \\left(\\begin{array}{ccc}\n1+\\rho_1+\\sum_{j\\neq 1} |\\rho_{1j}| & \\dots & -\\rho_{1c}\\\\\n-\\rho_{21} & \\dots & -\\rho_{2c}\\\\\n\\vdots & \\ddots & \\vdots\\\\\n-\\rho_{c1} & \\dots & 1+\\rho_{c}+\\sum_{j\\neq c} |\\rho_{cj}|\n\\end{array}\\right)\\]\nIn \\(Q\\), the \\(\\rho_i&gt;0\\) for \\(i=1,\\dots, c\\) and \\(\\rho_{ij}=\\rho_{ji}\\). Lastly, Cruz-Reyes defines a custom prior for \\(\\rho\\) that accounts for these conditions. Both approaches combine variable selection with level fusion, as the merge of all levels of a categorical variable implies it has no effect on the response. Cruz-Reyes’s prior structure is arranged to allow for correlation between the levels of the variable, such as when represent distinct, possibly neighboring, geographic areas."
  },
  {
    "objectID": "index.html#further-research-options",
    "href": "index.html#further-research-options",
    "title": "Weekly Reports",
    "section": "Further Research Options",
    "text": "Further Research Options\nThe most obvious application to our soil erosion research is to use priors like the ones presented for the correlation matrix of a qualitative input in a Gaussian process model. Interpretation would be the same: a correlation of one between two levels of a factor would indicate that those levels can be clustered together. The optimal form of that prior would depend on the nature of the variable being modeled. The Cruz-Reyes prior permits spacial correlation between levels but requires more parameters than the Pauger-Wagner prior.\nA further avenue of research is how best to adapt these methods to situations with many qualitative variables, variables of many levels, or large \\(n\\). The existing methods depend on large numbers of parameters to model all the possible interactions. The question of how the model could be simplified or how the model fitting process could be sped up are worth exploring.\n\nDecember 19, 2024"
  },
  {
    "objectID": "index.html#summary-20",
    "href": "index.html#summary-20",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I continued my comparison of local expert Gaussian process models to random forest models. The random forest models previously outperformed the LE GP models, which we hypothesized to be because the RF models were fit to the whole training data set while the GP models were each fit to a single strata. However, when I stratified the data by qualitative variables and fit one RF model to each strata, the accuracy on the test data did not significantly change.\nComparing the predicted by actual plots for each method showed that the difference is mostly in how they respond to large (&gt;100) soil loss values, which make up about \\(2\\%\\) of observations. The RF method predicts all values as small while the GP method predicts all large values as small and some small values as large, resulting in a higher RMSE. There may simply not be enough large values in our dataset to accurately model them with a Gaussian process. Data augmentation to increase the number of large soil loss values in the training data may help.\nI also studied the factor level combination methods proposed by Cruz-Reyes and Pauger and Wagner, focusing on their design of prior correlation structures for factor effects and their relative advantages. Lastly, I considered how these methods could be adapted for use in our own WEPP research."
  },
  {
    "objectID": "index.html#random-forest-model-comparisons",
    "href": "index.html#random-forest-model-comparisons",
    "title": "Weekly Reports",
    "section": "Random Forest Model Comparisons",
    "text": "Random Forest Model Comparisons\nI have previously fit a number of GP models to the positive soil loss data set, using the data from 2009 and 2010 as a training set and the data from 2011 as a test set. I now fit three random forest models to the same training set, using the same numeric variables as well as ‘crop’, ‘till’, or both. I evaluate these models on the 2011 test set and display the results in the table below.\n\n\n\nModel\nRMSE\nMAE\n\n\n\n\nRF(crop)\n17.79\n3.73\n\n\nRF(till)\n17.56\n3.61\n\n\nRF(both)\n17.52\n3.63\n\n\nGP(crop)\n18.60\n4.26\n\n\nGP(till)\n26.44\n6.60\n\n\nGP(both)\n21.73\n5.70\n\n\n\nThe GP models stratified on ‘till’ and ‘crop’+‘till’ significantly underperform their random forest equivalents, while the GP model stratified on ‘crop’ performs close to the random forest including crop."
  },
  {
    "objectID": "index.html#combining-factor-levels",
    "href": "index.html#combining-factor-levels",
    "title": "Weekly Reports",
    "section": "Combining Factor Levels",
    "text": "Combining Factor Levels\nSimilar levels of a categorical variable may be identified at three stages: before, during, or after a model has been fit. Before, the distribution of response values within each category could be compared for statistical similarity. This could be based on mean values or other summaries, or test statistics such as KL divergence. It would be necessary to control for the other covariates for such statistics to have meaning.\nDuring, or as a part of, model fit, the most obvious solution is to convert factor inputs into dummy variables and apply any existing variable selection technique to the dummy variables. The downside of this approach is that dropping a dummy variable ‘combines’ that level with the reference level but cannot combine it with any other level. A better approach is presented by Cruz-Reyes in the section below, based on defining a multivariate normal prior for the effects of the factor levels. Numeric covariates could possibly be included in this process by modeling the covariances between factor levels as a function of the numeric variables.\nAfter fitting a model, there are a few possible ways of evaluating level similarity depending on the type of model that was fit. If fitting a local expert GP model, the levels could be compared on their lengthscale parameter values or on their predictive accuracy on other strata. If a GP trained on one level can accurately predict data from another level, the levels may not be significantly different. As another option, since GP regression has a multivariate normal posterior distribution, we could calculate confidence intervals for the expected value of the response conditional on each factor level and see if they overlap."
  },
  {
    "objectID": "index.html#spacial-shrinkage-prior-by-cruz-reyes-et-al.",
    "href": "index.html#spacial-shrinkage-prior-by-cruz-reyes-et-al.",
    "title": "Weekly Reports",
    "section": "Spacial Shrinkage Prior by Cruz-Reyes et al.",
    "text": "Spacial Shrinkage Prior by Cruz-Reyes et al.\nThe paper “Spacial Shrinkage Prior: A Probabilistic Approach to Models for Categorical Variables with Many Levels” by Danna Cruz-Reyes describes a Bayesian method for combining levels of categorical variables within a linear regression model. Her technique is to encode the categorical variable with a parameter \\(\\mathbf{\\theta}=(\\theta_1, \\dots,\\theta_r)\\) representing the effects of its \\(r\\) levels.\nShe imagines the levels as nodes on a map with edges connecting each pair. She then puts a multivariate normal prior on \\(\\mathbf{\\theta}\\) with a covariance matrix depending on the random weights of the edges connecting nodes. Finally, she puts a hyperprior on the random edge weights that allows the weights to go to zero. If, upon fitting the model, they do, then the two categorical levels connected by that edge have been combined.\n\nDecember 10, 2024"
  },
  {
    "objectID": "index.html#summary-21",
    "href": "index.html#summary-21",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I fit a set of random forest models on the positive soil loss dataset, as a comparison with the local expert GP models I had fit previously. The local expert GPs underperformed the random forest models except for the GP stratified on ‘crop’.\nI also considered several approaches to identifying and consolidating significantly similar levels of categorical variables. A paper by Danna Cruz-Reyes suggested setting a Bayesian prior on the correlation matrix of the factor levels, acting as a penalty term to induce sparsity. It is designed for linear regression but may provide inspiration for Gaussian processes.\nThe paper by Cruz-Reyes is available at https://link.springer.com/chapter/10.1007/978-3-031-48415-5_11. It draws heavily on a paper by Daniela Pauger and Helga Wagner, available at https://projecteuclid.org/journals/bayesian-analysis/volume-14/issue-2/Bayesian-Effect-Fusion-for-Categorical-Predictors/10.1214/18-BA1096.full."
  },
  {
    "objectID": "index.html#crop-by-till",
    "href": "index.html#crop-by-till",
    "title": "Weekly Reports",
    "section": "Crop by Till",
    "text": "Crop by Till\nThe number of hillslopes with a particular combination of crop and tilling operation before 2012 is shown in the table below.\n\n\n\nseason\ncrop\\till\n0\n1\n4\n5\n6\n\n\n\n\nwinter\nalfalfa\n14\n10\n2\n25\n1\n\n\nwinter\nbromegr\n52\n0\n2\n11\n0\n\n\nwinter\nnone\n0\n88\n5\n258\n5\n\n\nspring\nalfalfa\n52\n0\n2\n11\n0\n\n\nspring\ncorn\n0\n98\n7\n283\n6\n\n\nspring\ntre\n14\n0\n0\n0\n0\n\n\n\nThe distribution of spring after 2012 is shown below. (The distribution for winter is unchanged.)\n\n\n\nseason\ncrop\\till\n0\n1\n4\n5\n6\n\n\n\n\nspring\nalfalfa\n52\n0\n0\n0\n0\n\n\nspring\ncorn\n0\n98\n9\n281\n6\n\n\nspring\nsoy\n0\n0\n0\n13\n0\n\n\nspring\ntre\n14\n0\n0\n0\n0\n\n\n\nFor each combination of crop and till, I recorded summary statistics of soil loss: the mean, median, and first and third quartiles. The results are shown below, from largest to smallest mean soil loss.\n\n\n\ncrop\ntill\nQ1\nmedian\nmean\nQ3\n\n\n\n\ncorn\n6\n0.2\n1.30\n15.44\n10.0\n\n\nsoy\n6\n0.2\n1.10\n13.07\n7.60\n\n\nalfalfa\n6\n0.2\n0.60\n12.04\n1.53\n\n\ncorn\n5\n0.2\n0.50\n6.99\n1.80\n\n\nsoy\n5\n0.2\n0.50\n6.54\n1.90\n\n\nalfalfa\n5\n0.1\n0.30\n5.29\n0.90\n\n\nnone\n5\n0.1\n0.30\n3.41\n0.90\n\n\nnone\n6\n0.1\n0.30\n3.34\n0.90\n\n\ncorn\n4\n0.1\n0.40\n3.10\n1.50\n\n\nnone\n4\n0.1\n0.30\n2.44\n0.80\n\n\nbromegr\n0\n0.1\n0.20\n1.62\n0.60\n\n\nbromegr\n5\n0.1\n0.20\n1.43\n0.43\n\n\ntre\n0\n0.1\n0.40\n1.36\n1.45\n\n\nsoy\n1\n0.1\n0.30\n1.36\n1.00\n\n\ncorn\n1\n0.1\n0.30\n1.24\n0.90\n\n\nalfalfa\n0\n0.1\n0.40\n1.22\n1.20\n\n\nbromegr\n4\n0.1\n0.30\n0.79\n1.10\n\n\nalfalfa\n1\n0.1\n0.20\n0.66\n0.43\n\n\nnone\n1\n0.1\n0.20\n0.60\n0.60\n\n\nalfalfa\n4\n0.1\n0.25\n0.48\n0.60\n\n\nbromegr\n1\n0.1\n0.20\n0.44\n0.50"
  },
  {
    "objectID": "index.html#nle-by-crop-and-till",
    "href": "index.html#nle-by-crop-and-till",
    "title": "Weekly Reports",
    "section": "NLE by Crop and Till",
    "text": "NLE by Crop and Till\nFrom my stratified data sample, I extracted only the dates between 2009 and 2010 which had nonzero soil loss, a total of 5938 observations. I combined crop and till into a single variable. There were 18 combinations present in the data set but three of them had too few observations in them to fit a model. After removing these, there were 5923 observations in the training data set. I then fit a local expert GP model by crop and till using the variables ‘rad’, ‘tmax’, ‘precip’, ‘max_prcp’, and ‘slp_wavg4’. The ML estimates of the component models are shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\ngroup\nrad\ntmax\nprecip\nmax_prcp\nslp_wavg4\nnugget\nsp.var\n\n\n\n\nalfalfa 0\n23378\n0.15\n3.07\n5e4\n3.52\n1.014\n1.494\n\n\nalfalfa 1\n1.88\n636.2\n3065\n5854\n0.634\n0.001\n1.371\n\n\nalfalfa 4\n5e4\n42585\n47815\n41995\n41908\n0.712\n0.124\n\n\nalfalfa 5\n34596\n41995\n5e4\n34577\n1.470\n9.254\n0.003\n\n\nbromegr 0\n2.45\n4.52\n29445\n5e4\n1.991\n8.204\n0.011\n\n\nbromegr 1\n2.07\n7.48\n12.17\n23.51\n10.15\n1e-8\n0.954\n\n\nbromegr 5\n39731\n5e4\n44022\n38292\n1.377\n4.407\n0.249\n\n\ncorn 1\n66.15\n50.62\n4.32\n50.25\n0.002\n0.122\n24.91\n\n\ncorn 4\n6.80\n3394\n4.12\n3058\n3.702\n0.027\n293.7\n\n\ncorn 5\n0.37\n7.40\n0.42\n6.59\n0.020\n0.002\n6283\n\n\ncorn 6\n5e4\n32206\n18.02\n5.25\n1.304\n0.406\n411.1\n\n\nnone 1\n0.95\n18351\n2.29\n5e4\n0.130\n0.323\n0.799\n\n\nnone 5\n17213\n10259\n0.95\n0.23\n0.215\n0.035\n39.80\n\n\nnone 6\n2.64\n6.63\n6.21\n1e-8\n5.712\n1e-8\n7.499\n\n\ntre 0\n44524\n1.02\n1.69\n5e4\n3.338\n0.445\n6.448\n\n\n\nThe model was tested on the positive soil loss data from 2011, with 1762 observations. The overall RMSE was \\(21.73\\) and the MAE was \\(5.70\\). The within group RMSEs on the test set were relatively low for all strata except for ‘corn 5’, ‘corn 6’, ‘none 5’, and ‘none 6’.\n\nDecember 3, 2024"
  },
  {
    "objectID": "index.html#summary-22",
    "href": "index.html#summary-22",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I performed a deeper exploratory analysis of the variables crop and till together. Notably, some combinations of crop and till did not occur in the stratified data set at all. Corn, for example, was never associated with zero tilling operations while tre was only associated with zero tilling operations. These imbalances may have significant effects on the performance of local expert methods.\nI also fit a local expert GP model by crop and till together. It outperformed the local expert model stratified on till but not the model stratified on crop. As with previous LE models, there were a small number of categories that contributed the majority of error."
  },
  {
    "objectID": "index.html#gp-of-soil-loss-by-till",
    "href": "index.html#gp-of-soil-loss-by-till",
    "title": "Weekly Reports",
    "section": "GP of Soil Loss by Till",
    "text": "GP of Soil Loss by Till\nFrom my stratified data sample, I extracted only the dates between 2009 and 2010 which had nonzero soil loss, a total of 5938 observations. I then fit a local expert GP model by till using the variables ‘rad’, ‘tmax’, ‘precip’, ‘max_prcp’, and ‘slp_wavg4’. The ML estimates of the component models are shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\ntill\nrad\ntmax\nprecip\nmax_prcp\nslp_wavg4\nnugget\nsp.var\n\n\n\n\n0\n2.51\n1.01\n3.71\n5e4\n3.84\n3.258\n0.482\n\n\n1\n83.05\n59.05\n5.71\n85.96\n0.002\n0.116\n25.67\n\n\n4\n1297\n12483\n0.594\n11744\n0.407\n0.052\n162.2\n\n\n5\n0.53\n5.58\n0.232\n14.53\n0.017\n0.002\n4822.4\n\n\n6\n131.3\n159.3\n6.62\n0.012\n5.521\n0.092\n928.8\n\n\n\nThe model was tested on the positive soil loss data from 2011, with 1769 observations. The overall RMSE was \\(26.44\\) and the MAE was \\(6.60\\). The within group RMSEs on the test set for Till = 0, 1, 4, 5, 6 were \\(3.60\\), \\(1.93\\), \\(12.30\\), \\(35.27\\), and \\(39.45\\). Till levels 5 and 6 had relatively high RMSEs while the rest had low, since those had a much greater range in soil loss values than the others."
  },
  {
    "objectID": "index.html#gp-of-average-detachment-by-crop-and-till",
    "href": "index.html#gp-of-average-detachment-by-crop-and-till",
    "title": "Weekly Reports",
    "section": "GP of Average Detachment by Crop and Till",
    "text": "GP of Average Detachment by Crop and Till\nAgain from the stratified sample, I extracted the days with positive average detachment between 2009 and 2010, a total of \\(4662\\) observations. I then fit a local expert GP model by crop using the variables ‘rad’, ‘tmax’, ‘precip’, ‘max_prcp’, and ‘slp_wavg4’. The ML estimates of the component models are shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\ntill\nrad\ntmax\nprecip\nmax_prcp\nslp_wavg4\nnugget\nsp.var\n\n\n\n\nalfalfa\n90.70\n0.44\n42.34\n0.009\n414.7\n0.027\n0.014\n\n\nbromegr\n4.77\n11.53\n5e4\n24388\n35410\n0.495\n0.002\n\n\ncorn\n1.62\n0.53\n0.181\n1.92\n0.017\n0.003\n0.200\n\n\ntre\n0.48\n4.22\n8.50\n8.09\n9.228\n0.203\n0.0007\n\n\nnone\n146.6\n4.03\n608.3\n0.10\n0.668\n0.064\n0.007\n\n\n\nThe model was tested on the positive average detachment data from 2011, with \\(946\\) observations. The overall RMSE was \\(0.268\\) and the MAE was \\(0.095\\). The within group RMSEs on the test set for alfalfa, bromegrass, corn, tre, and none were \\(0.044\\), \\(0.055\\), \\(0.295\\), \\(0.020\\), and \\(0.278\\). I then fit a GP with the same inputs but stratified on ‘till’, with its ML estimates shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\ntill\nrad\ntmax\nprecip\nmax_prcp\nslp_wavg4\nnugget\nsp.var\n\n\n\n\n0\n1.89\n8.66\n3.97\n273.1\n454.8\n0.211\n0.001\n\n\n1\n1.78\n7.19\n1.08\n12.59\n1.574\n0.048\n0.003\n\n\n4\n32.45\n2.90\n6.12\n4.66\n0.748\n0.131\n0.010\n\n\n5\n0.26\n1.40\n0.347\n13.45\n0.028\n0.002\n0.209\n\n\n6\n0.91\n782.8\n7.22\n0.274\n0.183\n0.147\n0.040\n\n\n\nThe model was tested on the positive average detachment data from 2011, with \\(946\\) observations. The overall RMSE was \\(0.249\\) and the MAE was \\(0.084\\). The within group RMSEs on the test set for till = 0, 1, 4, 5, 6 were \\(0.041\\), \\(0.039\\), \\(0.125\\), \\(0.298\\), and \\(0.417\\)."
  },
  {
    "objectID": "index.html#linear-models-with-interactions",
    "href": "index.html#linear-models-with-interactions",
    "title": "Weekly Reports",
    "section": "Linear Models with Interactions",
    "text": "Linear Models with Interactions\nUsing the positive soil loss dataset, I fit a series of linear models to test for interactions between the categorical and numeric variables. The table below displays parameter estimates for the variables ‘crop’, ‘rad’, ‘tmin’, precip’, ‘max_prcp’, and ‘slp_wavg4’. It is filtered to only display the interaction terms that were statistically significant.\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.23\n2.26\n-0.99\n0.32\n\n\ncropbromegr\n-1.24\n4.50\n-0.27\n0.78\n\n\ncropcorn\n-10.10\n2.58\n-3.92\n0.00\n\n\ncropnone\n2.53\n3.30\n0.77\n0.44\n\n\ncropsoy\n-11.63\n2.85\n-4.09\n0.00\n\n\ncroptre\n-0.77\n5.83\n-0.13\n0.89\n\n\nrad\n0.00\n0.00\n0.49\n0.62\n\n\ntmin\n-0.16\n0.09\n-1.83\n0.07\n\n\nprecip\n0.09\n0.02\n3.80\n0.00\n\n\nmax_prcp\n0.01\n0.04\n0.32\n0.75\n\n\nslp_wavg4\n16.51\n13.16\n1.25\n0.21\n\n\ncropsoy:rad\n0.01\n0.00\n2.13\n0.03\n\n\ncropcorn:tmin\n0.19\n0.11\n1.81\n0.07\n\n\ncropnone:tmin\n0.39\n0.18\n2.17\n0.03\n\n\ncropcorn:precip\n0.08\n0.03\n3.19\n0.00\n\n\ncropsoy:precip\n0.07\n0.03\n2.21\n0.03\n\n\ncropcorn:max_prcp\n0.10\n0.04\n2.20\n0.03\n\n\ncropcorn:slp_wavg4\n162.59\n15.32\n10.62\n0.00\n\n\ncropsoy:slp_wavg4\n208.82\n17.65\n11.83\n0.00\n\n\n\n\n\nBelow are the model results for the variable ‘till’, filtered as before.\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.41\n2.05\n-1.18\n0.24\n\n\ntill1\n1.13\n2.50\n0.45\n0.65\n\n\ntill4\n0.45\n5.60\n0.08\n0.94\n\n\ntill5\n-13.61\n2.28\n-5.97\n0.00\n\n\ntill6\n-34.88\n6.28\n-5.55\n0.00\n\n\nrad\n0.00\n0.00\n0.34\n0.73\n\n\ntmin\n-0.04\n0.08\n-0.51\n0.61\n\n\nprecip\n0.06\n0.02\n2.92\n0.00\n\n\nmax_prcp\n0.00\n0.03\n0.07\n0.94\n\n\nslp_wavg4\n10.73\n12.05\n0.89\n0.37\n\n\ntill5:rad\n0.01\n0.00\n2.42\n0.02\n\n\ntill6:rad\n0.01\n0.01\n1.86\n0.06\n\n\ntill5:tmin\n0.15\n0.09\n1.70\n0.09\n\n\ntill5:precip\n0.12\n0.02\n4.89\n0.00\n\n\ntill6:precip\n0.70\n0.07\n10.40\n0.00\n\n\ntill5:max_prcp\n0.12\n0.04\n3.12\n0.00\n\n\ntill6:max_prcp\n0.31\n0.07\n4.49\n0.00\n\n\ntill5:slp_wavg4\n267.98\n14.36\n18.66\n0.00\n\n\ntill6:slp_wavg4\n184.26\n57.92\n3.18\n0.00\n\n\n\n\n\nLastly, here are the model results for the variable ‘cover’.\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-7.91\n0.79\n-10.02\n0.00\n\n\ncoverTRUE\n1.87\n3.04\n0.61\n0.54\n\n\nrad\n0.01\n0.00\n4.87\n0.00\n\n\ntmin\n0.01\n0.04\n0.36\n0.72\n\n\nprecip\n0.12\n0.01\n13.19\n0.00\n\n\nmax_prcp\n0.09\n0.01\n7.80\n0.00\n\n\nslp_wavg4\n106.57\n5.18\n20.56\n0.00\n\n\ncoverTRUE:rad\n-0.01\n0.01\n-0.91\n0.36\n\n\ncoverTRUE:tmin\n0.00\n0.16\n0.03\n0.98\n\n\ncoverTRUE:precip\n0.03\n0.03\n1.03\n0.31\n\n\ncoverTRUE:max_prcp\n-0.08\n0.07\n-1.06\n0.29\n\n\ncoverTRUE:slp_wavg4\n-59.80\n20.91\n-2.86\n0.00\n\n\n\n\n\n\nNovember 19, 2024"
  },
  {
    "objectID": "index.html#summary-23",
    "href": "index.html#summary-23",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I fit a series of local expert GP models to the stratified data sample, fitting on soil loss based on ‘till’ and on average detachment based on the variables ‘crop’ and ‘till’. I found that the crop model fit to soil loss better than the till model, while the two models were about equal in performance when fit to average detachment.\nI also fit a set of linear regression models to the positive soil loss dataset to test for interactions between ‘crop’, ‘till’, and the numeric variables. Both had particularly strong interactions with slope and precip and weaker interactions with the other variables."
  },
  {
    "objectID": "index.html#latent-variable-approach-to-gp-modeling",
    "href": "index.html#latent-variable-approach-to-gp-modeling",
    "title": "Weekly Reports",
    "section": "Latent Variable Approach to GP Modeling",
    "text": "Latent Variable Approach to GP Modeling\nIn their paper “A Latent Variable Approach to GP Modeling with Qualitative and Quantitative Variables”, the authors Zhang et al. argue that in any physical process qualitative variables are stand-ins for unobserved numeric variables. Therefore, they can be modeled by modeling the underlying latent variables they represent. For a GP with numeric inputs \\(x_1,\\dots, x_p\\) and a qualitative input \\(t\\), they suggest this modification of the squared exponential function. \\[K(X, X') =\\exp\\left(-\\sum_{i=1}^p \\frac{|x_i-x_i'|^2}{\\theta_i} -|z(t)-z(t')|^2 \\right)\\] Where \\(\\theta_i\\) are lengthscale parameters and \\(z(t)\\) is a function mapping each level of \\(t\\) to a 2D numeric quantity. The \\(z(t)\\) values can then be fit through MLE.\nA two dimensional latent variable is deemed superior to a one dimensional variable for the following reason. Suppose the qualitative factor has three levels with equal correlation between each pair of levels. To represent this with a one dimensional mapping is impossible, but with two or more dimensions it can be done. To prevent indeterminacy in ML estimation, they further restrict the first level of each qualitative variable to be mapped to \\((0,0)\\) and the second to be mapped to the horizontal axis, so for a variable with \\(m\\) levels, there are \\(2m-3\\) scalar values to estimate.\nThe authors compared their method to two other methods of qualitative GP analysis: multiplicative covariance and hypersphere covariance, which they called unrestricted covariance. I discussed these methods on October 22 of this report. The latent variable method outperformed both in terms of restricted RMSE, which is the RMSE divided by the sum of the squared differences between the predicted response and the true mean of the responses, on multiple real-life datasets."
  },
  {
    "objectID": "index.html#additional-exploratory-analysis",
    "href": "index.html#additional-exploratory-analysis",
    "title": "Weekly Reports",
    "section": "Additional Exploratory Analysis",
    "text": "Additional Exploratory Analysis\nI performed a more in-depth analysis of the relationships between the new categorical variables and the numeric variables. Shown below are plots of significant inputs by response with points colored according to crop or to tilling operations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI also fit a linear model to soil loss, including ‘crop’ and ‘till’ as inputs. I used ‘precip’, ‘rad’, ‘tmin’, and ‘slp_wavg4’ as my numeric inputs.\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.79\n0.69\n-8.41\n0.00\n\n\nprecip\n3.07\n0.20\n15.76\n0.00\n\n\nrad\n0.83\n0.22\n3.75\n0.00\n\n\ntmin\n0.12\n0.25\n0.49\n0.62\n\n\nslp_wavg4\n5.84\n0.22\n26.58\n0.00\n\n\ncropbromegr\n1.83\n1.30\n1.40\n0.16\n\n\ncropcorn\n6.47\n1.16\n5.56\n0.00\n\n\ncropnone\n4.77\n1.37\n3.48\n0.00\n\n\ncropsoy\n6.69\n1.20\n5.56\n0.00\n\n\ncroptre\n1.70\n1.31\n1.31\n0.19\n\n\ntill1\n1.48\n1.29\n1.14\n0.25\n\n\ntill4\n1.09\n1.81\n0.60\n0.55\n\n\ntill5\n7.63\n1.24\n6.16\n0.00\n\n\ntill6\n8.90\n1.68\n5.30\n0.00\n\n\n\n\n\n\nNovember 12, 2024"
  },
  {
    "objectID": "index.html#summary-24",
    "href": "index.html#summary-24",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I studied an article by describing an approach to incorporating qualitative variables in Gaussian processes based on latent variables. By expressing each level of a quantitative variable as a value of a 2D latent numeric variable, they can model the underlying numeric process behind the variable directly. This method had significantly lower restricted RMSE than comparable qualitative methods.\nI also conducted further exploratory analyses of the stratified data set. I graphed soil loss by several input variables and colored the points according to the ‘crop’ and ‘till’ variables. The results confirmed that alfalfa and bromegrass generally had lower soil loss than corn and soy, even when controlling for inputs such as precipitation, temperature, and slope."
  },
  {
    "objectID": "index.html#soil-loss-prediction-with-random-forest",
    "href": "index.html#soil-loss-prediction-with-random-forest",
    "title": "Weekly Reports",
    "section": "Soil Loss Prediction with Random Forest",
    "text": "Soil Loss Prediction with Random Forest\nI fit a random forest model to the stratified sample of hillslopes using the new categorical variables. I first fit a model including all the new variables and five variables that had proven significant in previous models. Second, I fit a model using only the top 5 variables. The table displays the importance of each variable in the models by the mean decrease in the Gini index (a measure of the purity of sets).\n\n\n\nVariable\nFull\nReduced\n\n\n\n\nprecip\n54803\n56420\n\n\nslp_wavg1\n1918\n-\n\n\ntmin\n7300\n8151\n\n\nrad\n3982\n3259\n\n\nmax_prcp\n1128\n-\n\n\ncrop\n2813\n3022\n\n\ntill\n338\n-\n\n\nmonth\n4309\n4120\n\n\ncover\n251\n-\n\n\n\nThe confusion matrix of the second model showed that it never predicted any observation in the training set as having nonzero soil loss. This occurred even though I tried to weight the nonzero cases in the training set. Its performance was not bad on the test set, however.\n\n\n\nModel\nTrue\\Pred\n0\n1\nClassError\n\n\n\n\nTrain\n0\n149877\n2911\n0.0191\n\n\n.\n1\n0\n0\nNA\n\n\nTest\n0\n16646\n330\n0.0194\n\n\n.\n1\n9\n279\n0.0313\n\n\n\nI decided to try fitting a random forest model to a more balanced training set by undersampling the zero cases. I took my training set and randomly selected \\(10\\%\\) of the zero cases and added them to all the nonzero cases in the training set. The resulting data had \\(15279\\) zero observations and \\(2593\\) nonzero observations. This model performed much better on predicting the nonzero observations and a bit worse on the zero observations.\n\n\n\nModel\nTrue\\Pred\n0\n1\nClassError\n\n\n\n\nTrain\n0\n14891\n388\n0.0254\n\n\n.\n1\n36\n2557\n0.0139\n\n\nTest\n0\n16556\n420\n0.0247\n\n\n.\n1\n4\n284\n0.0139"
  },
  {
    "objectID": "index.html#gp-nle-by-crop",
    "href": "index.html#gp-nle-by-crop",
    "title": "Weekly Reports",
    "section": "GP NLE by Crop",
    "text": "GP NLE by Crop\nFrom my stratified data sample, I extracted only the dates between 2009 and 2010 which had nonzero soil loss, a total of 5938 observations. I then fit a local expert GP model by crop using the variables ‘rad’, ‘tmax’, ‘precip’, and ‘max_prcp’. The ML estimates of the component models are shown below.\n\n\n\ncrop\nrad\ntmax\nprecip\nmax_prcp\nnugget\nsp.var\n\n\n\n\nalfalfa\n32.19\n47.70\n26.98\n14.77\n9.318\n0.003\n\n\nbromegr\n4.14\n12.06\n39033\n50000\n11.534\n0.0003\n\n\ncorn\n39052\n2302\n0.689\n0.003\n0.169\n3249\n\n\ntre\n0.39\n3.04\n3.22\n4.95\n0.494\n6.672\n\n\nnone\n50000\n38244\n0.053\n0.43\n0.272\n15.345\n\n\n\nThe model was tested on the positive soil loss data from 2011, with 1769 observations. The overall RMSE was \\(18.60\\) and the MAE was \\(4.26\\). The within group RMSEs on the test set for Alfalfa, Bromegrass, Corn, Tre, and None were \\(3.33\\), \\(5.17\\), \\(20.60\\), \\(2.21\\), and \\(24.06\\). Corn and None had relatively high RMSEs while Alfalfa, Bromegrass, and Tre had low, since the first two had a much greater range in soil loss values than the others."
  },
  {
    "objectID": "index.html#dpmm-with-unbalanced-cluster-sizes",
    "href": "index.html#dpmm-with-unbalanced-cluster-sizes",
    "title": "Weekly Reports",
    "section": "DPMM with Unbalanced Cluster Sizes",
    "text": "DPMM with Unbalanced Cluster Sizes\nI repeated the Dirichlet Process Mixture Modeling example from last week, but with unbalanced cluster sizes. I drew \\(20\\) points from Cluster 1, \\(100\\) from cluster 2 and \\(60\\) from the other two, rather than \\(60\\) from each. The training data is plotted below.\n\n\n\n\n\n\n\n\n\nThe results of the DPMM algorithm with \\(\\alpha=0.01\\) are shown below, comparing the true cluster assignments to the estimated ones. The class imbalance did not seem to significantly affect the accuracy of the result, though Cluster 4 is still poorly handled. I also tried running the algorithm for 2000 iterations instead of 1000 but it didn’t meaningfully change the results.\n\n\n\nTrue\\Est\n1\n2\n3\n4\n\n\n\n\n1\n19\n0\n1\n0\n\n\n2\n0\n100\n0\n0\n\n\n3\n0\n0\n60\n0\n\n\n4\n0\n4\n19\n37\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 5, 2024"
  },
  {
    "objectID": "index.html#summary-25",
    "href": "index.html#summary-25",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I reran the Dirichlet process cluster analysis on a dataset with unbalanced cluster sizes, having one large cluster, two medium, and one small. The small cluster was still fit fairly accurately, so the bias towards adding points to large clusters doesn’t seem to be very big.\nI experimented with random forest models to predict whether a observation had positive soil loss or not. I found that ‘crop’ was a much more significant variable than ‘till’, and ‘month’ was also significant. By undersampling the zero cases to form a new training set and refitting the model, I was able to somewhat improve the predictive accuracy on a test set.\nI also fit an NLE GP model to positive soil loss data from 2009 and 2010, separated by crop. Based on test data from 2011, Corn and No Crop were much harder to predict than the others, as there is much more variability in soil loss within those groups."
  },
  {
    "objectID": "index.html#dirichlet-process-mixture-modeling",
    "href": "index.html#dirichlet-process-mixture-modeling",
    "title": "Weekly Reports",
    "section": "Dirichlet Process Mixture Modeling",
    "text": "Dirichlet Process Mixture Modeling\nI borrowed the following example from . Dirichlet Process Mixture Modeling is a method of cluster analysis based on the Gaussian mixture models. The data is divided into clusters where the responses in each cluster are drawn from a different multivariate Gaussian distribution. The special characteristic of DPMM is that the number of clusters is estimated organically from the data and doesn’t need to be specified beforehand.\nThe cluster assignment process is based on an MCMC algorithm that generates a sequence of cluster assignments. For each iteration, the data points are assigned to clusters with a Chinese Restaurant Process. Imagine a Chinese restaurant with an infinite number of tables. Diners enter one by one to be seated. The first diner always sits at the first table. The second diner sits at the first table with probability \\(1/(1+\\alpha)\\) and at the second table with probability \\(\\alpha/(1+\\alpha)\\) where \\(\\alpha\\) is a positive real number. The \\(k^{th}\\) diner sits at an occupied table with probability proportional to the number of people already sitting there and at the next unoccupied table with probability proportional to \\(\\alpha\\).\nIn our case, those probabilities are adjusted by the posterior probability of the new point belonging to each cluster given prior assumptions about the distribution of points within a cluster. Once you have a sequence of such assignments, you note for each point the cluster to which it was most often assigned, which is its final assignment.\nSuppose we have a data vector \\(y\\) normally distributed according to a mean \\(\\mu\\) and a known measurement error \\(\\sigma_y^2\\), and that \\(\\mu\\) is distributed with a prior mean \\(\\mu_0\\) and precision \\(\\tau_0^2\\). Then, to be precise, the probability of a new point \\(y_i\\) being assigned to cluster \\(k\\) is \\[p(y_i\\in c_k)\\propto n_{-i,k}\\times \\text{pdf}(y_i|\\mu_k,\\tau_k)\\] \\[\\propto \\frac{n_{-i,k}}{n-1+\\alpha}\\Phi\\left(y_i| \\frac{\\bar y_kn_k \\tau_k+\\mu_0\\tau_0}{n_k\\tau_k+\\tau_0}, (n_k\\tau_k+\\tau_0)^{-1}+\\sigma_y^2 \\right)\\] where \\(n_{-i,k}\\) is the number of points in cluster \\(k\\) not counting \\(y_i\\), \\(n\\) is the total number of points assigned to clusters, and \\(\\bar y_k\\) and \\(\\tau_k\\) are the estimated mean and precision of cluster \\(k\\). Finally, \\(\\Phi()\\) is the pdf of the Normal distribution.\nFor our example we sixty points each from four bivariate normal distributions, which are our four clusters. The data set is plotted below.\n\n\n\n\n\n\n\n\n\nThe results of the DPMM algorithm with \\(\\alpha=0.01\\) are shown below, comparing the true cluster assignments to the estimated ones.\n\n\n\nTrue\\Est\n1\n2\n3\n4\n\n\n\n\n1\n60\n0\n0\n0\n\n\n2\n0\n0\n60\n0\n\n\n3\n1\n0\n0\n59\n\n\n4\n7\n40\n4\n9\n\n\n\n\n\n\n\n\n\n\n\n\nRepeating this process on the same data but with an \\(\\alpha\\) value of \\(0.1\\) are shown here. The cluster assignments are very similar,\n\n\n\nTrue\\Est\n1\n2\n3\n4\n\n\n\n\n1\n0\n0\n0\n60\n\n\n2\n0\n0\n60\n0\n\n\n3\n60\n0\n0\n0\n\n\n4\n18\n38\n4\n0"
  },
  {
    "objectID": "index.html#tilling-operations-description",
    "href": "index.html#tilling-operations-description",
    "title": "Weekly Reports",
    "section": "Tilling Operations Description",
    "text": "Tilling Operations Description\nAfter studying, I can better define some of the equipment referenced in the tilling operations in the WEPP data set. A planter is used to cut open the soil and drop the seeds in. Chisel plows, field cultivators, and row cultivators are all used to break up the soil before or after the growing season. Chisel plows dig deepest, while cultivators are shallower. Row cultivators are used to break up the soil between planted rows, possibly for weed control. Tandem disks and double disk openers can be added to cultivators to further disturb the soil. Double disk openers are designed for handling stubble, field debris, or other trashy conditions."
  },
  {
    "objectID": "index.html#categorical-and-numeric-data-merge",
    "href": "index.html#categorical-and-numeric-data-merge",
    "title": "Weekly Reports",
    "section": "Categorical and Numeric Data Merge",
    "text": "Categorical and Numeric Data Merge\nI added three variables to the stratified data set this week: Crop, Cover, Till, and Month. Crop records the kind of crop we think was growing on a hillslope at a particular time, Cover records whether the crop was a cover crop or not, Till records the set of tilling operations performed on that hillslope, and Month is the month the observation was recorded.\nI performed Kruskal-Wallis rank sum tests between each categorical variable and response to test for correlation between them. Crop was significantly correlated to soil loss, runoff, and average detachment with chi-squared statistics of \\(16916\\), \\(4409\\), and \\(16196\\) respectively. Cover was as well, with test statistics of \\(1391\\), \\(491\\), and \\(1852\\) respectively. Till also was, with test statistics of \\(805\\), \\(1010\\), and \\(472\\) respectively. The p-values were less than \\(2.2\\text{e-}16\\) for each case.\nHere, I have plotted boxplots of the response variables with respect to the categorical inputs ‘crop’ and ‘till’. It is easy to see that main crops (corn and soy) have higher soil loss than cover crops.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 29, 2024"
  },
  {
    "objectID": "index.html#summary-26",
    "href": "index.html#summary-26",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I prepared an example of Dirichlet Process Mixture Modeling, which sorts data values into clusters by modeling each cluster as a multivariate Gaussian distribution. The advantage of this method is that you do not need to specify the number of clusters beforehand; it is estimated from the data.\nI also merged the management data with the weather data for the stratified sample of HUC12s. I tested the categorical variables for correlation with the response variables and prepared some boxplots displaying their relationships. These plots showed that soil loss was highest for main crops like corn and soy and for tilling with row cultivators. This seems reasonable since row cultivators can be used multiple times per season."
  },
  {
    "objectID": "index.html#wepp-crops-deep-dive",
    "href": "index.html#wepp-crops-deep-dive",
    "title": "Weekly Reports",
    "section": "WEPP Crops Deep Dive",
    "text": "WEPP Crops Deep Dive\nThe original analysis of the crop variables in WEPP considered the full stratified sample drawn in June which contained all hillslopes in three HUC12 watersheds. This sample contained the watershed \\(071000030704\\) with \\(227\\) hillslopes, \\(070801051103\\) with \\(113\\) hillslopes, and \\(102802010405\\) with \\(133\\) hillslopes. Separating the watersheds, we see the distribution of crop variants differs by location.\nFor location \\(071000030704\\), we see the following.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor location \\(070801051103\\), we see:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor location \\(102802010405\\), we see:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA detailed analysis of the crop data for the stratified sample of HUC12s found a sharp delineation between crops labeled in 1998 and crops labeled in 2013. For crops labeled February 1998, only corn, alfalfa, brome grass, and Tre_2932 are present. The table below displays the frequency of each combination of these crops, where a 1 in the first four columns means that that crop is present and a 0 means it is absent.\n\n\n\nCorn\nAlfalfa\nBrome\nTre2932\nN\n\n\n\n\n0\n1\n0\n1\n13\n\n\n0\n1\n1\n1\n1\n\n\n0\n1\n1\n0\n65\n\n\n1\n0\n0\n0\n356\n\n\n1\n1\n0\n0\n31\n\n\n1\n1\n1\n0\n7\n\n\n\nFor crops labeled April 2013, if they exist they are always a corn variant and a soy variant. The WEPP User Summary adds some clarity to this situation, explaining that “A different type of residue on a field besides the current crop growth being simulated also needs to be assigned a crop number.” In other words, crops marked as present may not actually be growing on the field if the remains from past growth are present.\nThe logical next step is to merge the crop data with the daily weather and soil loss data, but three major complications exist to implementing this at this point. First, we must decide when each crop type is on the field. A reasonable estimation is that main Crops run from May 1 to October 31 while cover crops run from November 1 to April 30. Second, many hillslopes list two main crops or two cover crops present at the same time. One solution might be to say that if two main crops or two cover crops coexist, they alternate years (e.g. the farmer plants corn one year, soy the next, corn the next, and so on). Lastly, we must decide whether Thre_2932 is a main crop or a cover crop. It never coexists with corn or soy so a main crop may be more likely, but a definitive answer is needed. Alternatively, we can just omit the \\(14\\) hillslopes containing Tre_2932 from our analysis."
  },
  {
    "objectID": "index.html#tilling-operations-analysis",
    "href": "index.html#tilling-operations-analysis",
    "title": "Weekly Reports",
    "section": "Tilling Operations Analysis",
    "text": "Tilling Operations Analysis\nThere are six tilling operations found within the stratified WEPP data set. They are listed below, alongside their descriptions as taken from the management files.\n\nPLNTFC: Planter, no-till with fluted coulter.\nFCSTACDP: Field cultivator, secondary tillage, after duckfoot points. Maximum depth of 10 cm (4 inches).\nTAND0002: Tandem disk.\nPLDDO: Planter, double disk openers. Tillage depth of 2 inches.\nCULTMUSW: Cultivator, row, multiple sweeps per row.\nCHISSTSP: A chisel plow, straight with spike points.\n\nSome distinct patters emerge in how these tilling operations co-occur. PLNTFC occurs alone in 98 hillslopes. The other five operations occur together in 294 hillslopes. The other five minus CULTMUSW occur in 9 hillslopes. All six occur together in 6 hillslopes. No tilling operations are listed for 66 hillslopes\nThe results for \\(071000030704\\) are below.\n\n\n\n\n\n\n\n\n\nThe results for \\(070801051103\\) are below.\n\n\n\n\n\n\n\n\n\nThe results for \\(102802010405\\) are below.\n\n\n\n\n\n\n\n\n\n\nOctober 22, 2024"
  },
  {
    "objectID": "index.html#summary-27",
    "href": "index.html#summary-27",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week, I performed a deep analysis into the qualitative variables available in the WEPP stratified sample, specifically the crops present at each location and the tilling operations performed at that location. As a reminder, the stratified sample consists of three HUC12 watersheds, spread across Iowa, and all the hillslopes recorded within each watershed.\nEach crop listed in a hillslope’s management file is associated with a comment that may include a date, either in 1998 or 2013. There is a stark division between the crops labeled 1998 and those labeled 2013. There are also distinctions between the different watersheds, which are displayed through separate graphs. Several questions of how to interpret the crop data remain before it can be merged with the main dataset.\nI also performed analysis of the tilling operations data. There are six tilling operations in the sample, with four unique patterns of co-occurence. Notable differences between watersheds are explored through graphs of the operation frequency. This data could be merged with the main dataset now, possibly alongside a date or month variable, since the effect of tilling is time-dependent."
  },
  {
    "objectID": "index.html#qualitative-input-lit-review",
    "href": "index.html#qualitative-input-lit-review",
    "title": "Weekly Reports",
    "section": "Qualitative Input Lit Review",
    "text": "Qualitative Input Lit Review\nA literature review of existing work on incorporating qualitative variables into Gaussian process models shows that the existing methods generally approach the issue by defining a kernel (covariance) function for qualitative variables that differs from the kernel function for quantitative variables. This is a contrast to my previous approach, which focused on defining a distance function for qualitative variables that could be fed into the same covariance function as the quantitative variables. The covariance-based approach allows for much greater flexibility in how correlations between levels of a factor are defined, but has some weaknesses of its own.\nThe methods discussed below derive from three main articles: by X. Deng, C. Devon Lin, et al.  by Peter Quian, C. F. Jeff Wu, and Huaiqing Wu. by Peter Quian, Qiang Shu, and Shiyu Zhu.\nIf your distance function on qualitative variables doesn’t depend on any parameters, then the distance matrix between your observed data points can be calculated once and reused in the GP estimation process, saving much time. This is lost when you have parameters to estimate in your distance matrix as you have to calculate it again every time. However, calculating distance between the levels of a nominal variable is hardly useful without prior knowledge of how the levels differ. The only sensible choice is to let \\(d(x_j,x_{j'})=0\\) when \\(j=j'\\) and \\(d(x_j,x_{j'})=c\\) when \\(j\\neq j'\\) for some constant \\(c\\). This is equivalent to using the Gower dissimilarity as the distance metric.\nAlternative distance metric, cosine dissimilarity, is based on encoding qualitative inputs with dummy variables and taking the angle between two normalized vectors representing the qualitative inputs values of two points. This returns a single distance representing the collective difference between two observations across all qualitative variables.\nThe methods I’ve studied are exchangeable correlation (EC) functions, multiplicative correlation (MC) functions, and hypersphere-based correlation (HC) functions. In EC, MC, and HC, one kernel function is used for quantitative variables and another kernel function is used for qualitative variables. The qualitative kernel functions for EC and MC are \\(EC: k_Z(z_j,z_{j'})= c\\) if \\(j\\neq j'\\) where \\(0&lt;c&lt;1\\) and \\(MC: k_Z(z_j,z_{j'})= \\exp(-(\\theta_j+\\theta_{j'}))\\) where \\(\\theta_j,\\theta_{j'} &gt;0\\) respectively. The HC method projects the correlation matrix for each qualitative variable onto a hypersphere and expresses the corresponding elements as the sum of sines and cosines of estimated parameters.\nA major difference between these three methods is the number of parameters they need to estimate. Supposing we have \\(k\\) qualitative inputs with \\(m_k\\) levels each, the EC method has \\(k\\) parameters, the MC method has \\(\\sum_k m_k\\) parameters and the HC method has \\(\\sum_k m_k(m_k-1)\\) parameters. Also noteworthy is that the HC method can accommodate negative correlations between different levels of a factor, which is possible if, for example, \\(y=\\cos(x)\\) given factor level \\(1\\) and \\(y=-cos(x)\\) given factor level \\(2\\). Given that we are working with a very large dataset, it is of interest to minimize the complexity of the approach we take."
  },
  {
    "objectID": "index.html#wepp-qualitative-variables",
    "href": "index.html#wepp-qualitative-variables",
    "title": "Weekly Reports",
    "section": "WEPP Qualitative Variables",
    "text": "WEPP Qualitative Variables\nThe WEPP dataset contains a large number of qualitative variables, mostly representing the crops present and the tilling operations performed on each hillslope. There are ten crops present, or five if you combine different variants of corn and soy. The five are alfalfa, bromegrass, corn, soy, and something called ‘Tre_2932’. Multiple crops can be present on a hillslope at the same time and the slopes seem to have been measured twice: once in 1998 and once in 2013. Graphs of the number of different crops found on the same hillslope and of the prevalence of each crop are shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMost qualitative variables in this dataset have only two levels, but there are a large number of them. It is reasonable to expect interactions between factors and between factors and numeric variables. There may also be negative correlations between some factor levels, though it is hard to predict this one way or another."
  },
  {
    "objectID": "index.html#dirichlet-process-example",
    "href": "index.html#dirichlet-process-example",
    "title": "Weekly Reports",
    "section": "Dirichlet Process Example",
    "text": "Dirichlet Process Example\nThe Dirichlet process is a stochastic process of probability distributions specified by a base distribution \\(F_0\\) and a concentration parameter \\(\\alpha\\). When used as a Bayesian model to estimate the distribution generating data, \\(F_0\\) is our prior guess at the distribution and \\(\\alpha\\) controls the relative weight of the prior versus the data. When the number of data points is much greater than \\(\\alpha\\), the posterior approaches the empirical CDF of the data. We do not directly calculate the posterior though, but sample from it by generating a Dirichlet process of posterior CDFs and averaging them together.\nGiven \\(30\\) draws from a \\(N(3,1)\\) distribution and a \\(N(1,1)\\) prior, I estimated the data distribution with a Dirichlet process with \\(50\\) runs, obtaining the results shown below.\n\n\n\n\n\n\n\n\n\nSince our prior distribution is light-tailed, there is not enough data at the extremes to overrule the prior. This can be mitigated by choosing a heavy-tailed prior, such as a t-distribution with \\(df=2\\) and \\(ncp=1\\). The estimation result is shown below.\n\n\n\n\n\n\n\n\n\n\nOctober 15, 2024"
  },
  {
    "objectID": "index.html#summary-28",
    "href": "index.html#summary-28",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I reviewed a number of papers discussing how to incorporate qualitative variables into GPs. Existing solutions focus on defining a correlation function for qualitative variables, separate from that for quantitative variables. These correlation functions often include a large number of estimated parameters to capture the correlations between factor levels. For this reason, most methods increase the computational complexity of model fitting. How best to fit qualitative variables while minimizing computation time is a less explored question.\nI also summarized the crop qualitative variables in the WEPP dataset, and described some of the other qualitative variables in that dataset."
  },
  {
    "objectID": "index.html#additive-gp-with-mixed-data",
    "href": "index.html#additive-gp-with-mixed-data",
    "title": "Weekly Reports",
    "section": "Additive GP with Mixed Data",
    "text": "Additive GP with Mixed Data\nI studied the additive GP model with mixed data proposed by . Starting with a response variable \\(Y\\) based on quantitative variables \\(X_j\\), \\(j=1,\\dots p,\\), and qualitative variables \\(Z_k\\), \\(k=1,\\dots,q\\), they define the model as a sum of Gaussian processes. \\[Y(X,Z)=\\mu+ GP_1(Z_1,X) +\\dots +GP_q(Z_q,X)\\] so \\(q\\) GPs are fit to \\(Y\\), each including one qualitative variable and all the quantitative variables. Then each GP follows a MVN distribution \\(N(0,\\Sigma)\\), where the kernel function of \\(\\Sigma\\) given two inputs \\(\\textbf{w}_1= (\\textbf{x}_1, \\textbf{z}_1)\\) and \\(\\textbf{w}_2=(\\textbf{x}_2, \\textbf{z}_2)\\) is \\[K(\\textbf{w}_1, \\textbf{w}_2)= \\tau^2 k_X(\\textbf{x}_1, \\textbf{x}_2|\\theta) \\prod_{k=1}^q  k_{Z_q}(\\textbf{z}_{1q}, \\textbf{z}_{2q}|\\theta)\\] where \\(k_X(\\cdot)\\) represents the covariance function between the quantitative variables and \\(k_{Z_q}(\\cdot)\\) represents the covariance function for the qth qualitative variable.\nThe authors choose to use separate covariance or kernel functions, not just separate distance functions like I had. Since the overall kernel function \\(K\\) is the product of the individual covariance functions, a value of zero for any of them could cause the entire covariance to become zero. Thus, they opted for an additive model that splits the qualitative variables among an equal number of GPs.\nFor the covariance function of a qualitative variable, the authors proposed three options. First, let \\(k_{Z_q}(\\textbf{z}_{iq}, \\textbf{z}_{jq}|\\theta_z)=\\theta_z\\), \\(0&lt;\\theta_z&lt;1\\) for \\(i\\neq j\\), called the exchangeable method. Second \\(k_{Z_q}(\\textbf{z}_{iq}, \\textbf{z}_{jq}|\\theta)=\\exp(-(\\theta_{iq}+ \\theta_{jq}))\\), \\(\\theta_{iq}&gt;0, \\theta_{jq}&gt;0\\) for \\(i\\neq j\\), called the multiplicative method. Third, a covariance function based on a hypersphere decomposition of the covariance matrix for each factor.\nThe multiplicative method is most similar to my previous approach of using the same kernel but different distance functions for qualitative and quantitative variables. However, the third method is preferred by the authors as the show that it has a higher RMSE on average. Nevertheless, I think the second method or a variation of it will be most suitable for the WEPP project. The hypersphere method favored by the authors has many parameters and needs a lot of computation time, which runs against our interests, while the exchangeable method assumes different levels of a factor always have the same ‘distance’ between them, which may not be justified."
  },
  {
    "objectID": "index.html#gp-with-gowerpam-clustering",
    "href": "index.html#gp-with-gowerpam-clustering",
    "title": "Weekly Reports",
    "section": "GP with Gower/PAM Clustering",
    "text": "GP with Gower/PAM Clustering\nI tried clustering our previous training dataset with a 3-level factor using the Partitioning around Medoids method. When I specified \\(3\\) clusters, it perfectly captured the three levels of the factor input, so I also clustered the data into \\(2\\) and \\(4\\) clusters. For each set of clusters, I fit a GP to each cluster and predicted on the corresponding cluster in the training data.\nFor two clusters, the estimated lengthscales for the continuous input were \\(1.33\\) and \\(1.72\\) respectively for clusters 1 and 2. The cluster structure relative to the factor input is shown in this table.\n\n\n\nFactor\nC1\nC2\n\n\n\n\n1\n300\n0\n\n\n2\n0\n300\n\n\n3\n167\n133\n\n\n\nThe RMSE of this model on the test set was \\(9.91\\) and its estimated functions with \\(95\\%\\) confidence bands are plotted below. As indicated in the table, factor level 3 is treated the same as 1 for values below zero and the same as 2 for values roughly above zero.\n\n\n\n\n\n\n\n\n\nFor four clusters, the estimated lengthscales for the continuous input were \\(1.60\\), \\(1.38\\), \\(1.04\\), and \\(2.54\\) respectively for clusters 1 to 4. The cluster structure relative to the factor input is shown in this table.\n\n\n\nFactor\nC1\nC2\nC3\nC4\n\n\n\n\n1\n300\n0\n0\n0\n\n\n2\n0\n180\n120\n0\n\n\n3\n0\n0\n0\n300\n\n\n\nThe RMSE of this model on the test set was \\(9.06\\) and its estimated functions with \\(95\\%\\) confidence bands are plotted below. This method succeeded in estimating the functions fairly well, though the uncertainty on Factor 2 varies according to what cluster it is in.\n\n\n\n\n\n\n\n\n\n\nOctober 8, 2024"
  },
  {
    "objectID": "index.html#summary-29",
    "href": "index.html#summary-29",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week, I explored several papers discussing methods of handling mixed data in GPs. These methods centered around defining covariance functions for the qualitative variables, rather than using the same covariance function for qualitative and quantitative variables and defining separate distance functions.\nI tried clustering data with the PAM (Partitioning Around Medoids) method, specifying more or fewer clusters than optimal. Then I used a local expert method, fitting a GP to the data in each cluster, to train a model and make inference on an out-of-sample set.\nI also updated the simulation study of a GP with an interaction term, using the same input data each time and comparing models with interaction terms to models without. There was not a consistent relationship between the relevance of the interaction term and the size of the interaction component of the response."
  },
  {
    "objectID": "index.html#summary-30",
    "href": "index.html#summary-30",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week, I tried clustering our previous datasets with the PAM (Partitioning Around Medoids) method. I was able to perfectly retrieve the original cluster structure of the factor input for both the one-factor and the two-factor datasets.\nI also tried generating a dataset with two continuous inputs and fitting a GP using just those two inputs and again with a third input that was the product of the other two. I repeated this with several datasets with different amounts of interaction between the inputs. My hope was that the relevance of the interaction parameter (represented by the lengthscale) would increase as the amount of interaction increased. This occurred, but the relative relevance of the interaction term was not consistent, hampering interpretability. A new approach may be needed.\nLastly, I finished coding a GP model with a custom distance function. I fit a GP with the old single-factor dataset from September 17 using the Gower distance (Euclidean distance for numeric variables and 0-1 distance for categorical variables). I then predicted on the test set of evenly spaced numeric values. The model performed very well, even better than the NLE model with a GP fit to each factor level independently.\n\nGP with Interaction Variable\nTo test how a Gaussian process handles interactions between inputs, I generated a dataset based on two continuous variables. I fit a GP to the data using the following kernel function.\n\\[K(x,x')= \\exp\\left(-\\left[\\frac{d(x_1,x_1')}{\\theta_1} + \\frac{d(x_2,x_2')}{\\theta_2} +\\frac{d(x_1x_2,x_1'x_2')}{\\theta_3}\\right] \\right)\\] In practice, all I needed to do was add a third variable that was the product of the first two. I then scaled all three inputs, to make their lengthscales comparable. The response depended on the inputs by the equation \\[y_i = 1.3x_{1i}+1.8x_{2i}+\\alpha\\exp(x_{1i}+x_{2i})+\\epsilon_i\\] where \\(\\epsilon_i~N(0,0.1)\\). I generated five such datasets with \\(\\alpha\\) equal to \\(0.25\\), \\(0.5\\), \\(1\\), \\(1.5\\) and \\(2\\). The MLE parameter estimates of these models were as follows:\n\n\n\nInteract\nAlpha\nX1\nX2\nX1X2\nNugget\n\n\n\n\nY\n0.25\n0.83\n49.9\n40.4\n0.0001\n\n\nN\n0.25\n0.69\n42.4\n–\n0.0001\n\n\nY\n0.5\n0.75\n49.9\n40.6\n0.0001\n\n\nN\n0.5\n0.64\n7.36\n–\n0.0001\n\n\nY\n1\n0.71\n50\n40.5\n0.0001\n\n\nN\n1\n0.57\n6.72\n–\n0.0001\n\n\nY\n1.5\n0.64\n50\n10.4\n0.0001\n\n\nN\n1.5\n0.56\n6.60\n–\n0.0001\n\n\nY\n2\n0.67\n33.4\n41.7\n0.0001\n\n\nN\n2\n0.55\n6.56\n–\n0.0001\n\n\n\nThe lengthscale of the interaction term does decrease as \\(\\alpha\\) increases, suggesting increasing relevance for that input. However, the lengthscales of the other inputs do not follow a consistent pattern, so it is difficult to judge the relevance of the interaction term by comparing it to the other terms. An interaction term in the kernel function thus may not be an effective way of measuring variable interaction within a GP.\n\n\nGP with Gower Dist\nI fit a custom Gaussian process model to a subset of the single-factor dataset with two levels of the factor similar, composed of \\(100\\) observations drawn from each factor level for a total of \\(300\\) observations. This GP used the Gower dissimilarity to quantify the difference between observations on the categorical input. If two observations matched on the factor input the distance was zero, if they differed the distance was one.\nThe lengthscales are \\(37.39\\) and \\(21.00\\) for the numeric and factor inputs respectively, and the nugget was \\(0.001\\). The RMSE for this model was \\(2.20\\), far better than any other method. The \\(95\\%\\) confidence bands for the predicted functions are plotted below.\n\n\n\n\n\n\n\n\n\n\n\nOctober 1, 2024"
  },
  {
    "objectID": "index.html#covariate-interactions-in-a-gp",
    "href": "index.html#covariate-interactions-in-a-gp",
    "title": "Weekly Reports",
    "section": "Covariate Interactions in a GP",
    "text": "Covariate Interactions in a GP\nDr. Niemi mentioned last week that he was interested in whether there was a way to make use of the fact that the similarity between levels of the factor input varied according to the value of the numeric input in last week’s dataset. I considered two ways to approach that question, first as a question of clustering/partitioning and second as a question of model fitting.\nCurrently I treat each level of the categorical input as a cluster, with no involvement from the numeric input, but this need not be the case. Clustering algorithms such as those described by Costa, Papatsouma, and Markos (see Sep. 17) partition the data based on all inputs, allowing for interactions between them. If we want to study or visualize the interactions, we could cluster in two steps: first partition the data based on the numeric inputs, then partition each cluster based on the categorical inputs. Alternatively, we could cluster the data by the numeric inputs, then cluster it again by the categorical inputs and see how the two sets of clusters overlap.\nAnother approach is to fit a Gaussian process model and study how its inputs interact within the fitted model. This is hard to do in practice because while a GP accommodates interaction between inputs with a squared exponential kernel, there is no clean way to visualize interactions between them. The expected value of a new observation is based on the correlations between it and each other observation. Changing one input will change these correlations, and the expected value, to a degree that depends on the value of the other inputs.\nTo demonstrate this, consider a squared exponential Gaussian process fit to a dataset with inputs \\(X_i=(x_{i,con},x_{i,cat}), i=1,\\dots,n\\) and response \\(Y\\), where \\(x_{con}\\) is a continuous variable and \\(x_{cat}\\) is a categorical variable. The covariance between two points \\[K(X_1, X_2)=\\exp\\left(-\\left[\\frac{||x_{1,con}-x_{2,con}||^2}{\\theta_{con}} + \\frac{||x_{1,cat}-x_{2,cat}||^2}{\\theta_{cat}} \\right] \\right)\\] \\[=\\exp\\left(-\\frac{||x_{1,con}-x_{2,con}||^2}{\\theta_{con}} \\right) \\exp\\left( -\\frac{||x_{1,cat}-x_{2,cat}||^2}{\\theta_{cat}} \\right) =k_{con}(X_1,X_2)k_{cat}(X_1,X_2)\\]\nThe covariance is then a product of functions of the squared distances between inputs. The size of the change in covariance resulting from a change in one input will thus depend on the size of the contributions of the other inputs.\nAs is, the GP provides no insight on the general nature interactions between its inputs. However, it may be possible to modify the model to draw out those interactions. Adding an interaction term inside the kernel function (or distance function) could be a way to achieve this."
  },
  {
    "objectID": "index.html#gp-with-two-factor-inputs",
    "href": "index.html#gp-with-two-factor-inputs",
    "title": "Weekly Reports",
    "section": "GP with Two Factor Inputs",
    "text": "GP with Two Factor Inputs\nTo extend my previous research into categorical methods, I constructed a new dataset with two factor inputs each with two levels and one numeric input and generated a training set of 800 observations and a test set of 200. I applied three methods of addressing categorical inputs with GPs: to ignore them (Method 1), to partition the data based on the factors and fit a different GP to each combination of levels (Method 2), and to fit a linear model using the factor inputs and then fit a GP using the residuals of the linear model (Method 3). The training data is plotted below.\n\n\n\n\n\n\n\n\n\nFor Method 1, the prediction RMSE was \\(13.7\\). The maximum likelihood estimates of the parameters were \\(107.66\\) for the lengthscale of x1 and \\(0.223\\) for the nugget. For Method 2, the prediction RMSE was \\(0.72\\). The maximum likelihood estimates of the parameters were \\(2.80\\), \\(3.40\\), \\(1.32\\), and \\(1.94\\) for the lengthscales of x1 given \\((x2,x3) = \\{(1,1), (1,2), (2,1), (2,2)\\}\\). For Method 3, the prediction RMSE was \\(11.01\\). Its lengthscale MSE was \\(110.80\\) and its nugget was \\(0.203\\). The predicted vs residual plots for each method looked normal, and the estimated functions with \\(95\\%\\) confidence bands are plotted below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 24, 2024"
  },
  {
    "objectID": "index.html#summary-31",
    "href": "index.html#summary-31",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I considered how we might design interpretable interactions between covariates, based on a remark by Dr. Niemi. This could be done either as part of the clustering process in a local experts method or as a modification of the Gaussian process model. I show that the Gaussian process allows for interactions between terms, but does not have any clear interpretation of them.\nI also tested my best performing methods for GPs with categorical inputs on a new dataset with two categorical inputs instead of one. The linear mean function method suffered, as the data did not follow a consistent pattern even with the factor effects removed, but the NLE method stayed effective.\nI worked on developing a GP function that calls a customizable distance function, but was not able to get it to run consistently in time for this meeting."
  },
  {
    "objectID": "index.html#distances-for-functional-data",
    "href": "index.html#distances-for-functional-data",
    "title": "Weekly Reports",
    "section": "Distances for Functional Data",
    "text": "Distances for Functional Data\nThere are a number of ways of handling functional inputs for clustering or regression purposes. One is to use a distance metric between two curves such as the Frechet distance as a basis for clustering. Another is to convert the functional input into a linear combination of basis functions or principal components and use the linear coefficients as inputs.\n suggest a clustering method using Frechet distance and show that it is competitive for functional inputs that are irregularly measured. propose a variation of basis function-based dimension reduction for cases when the response is categorical where the basis functions are chosen adaptively to maximize classification accuracy and are restricted to piecewise linear functions to improve their interpretability."
  },
  {
    "objectID": "index.html#gp-with-categorical-mean-fn",
    "href": "index.html#gp-with-categorical-mean-fn",
    "title": "Weekly Reports",
    "section": "GP with Categorical Mean Fn",
    "text": "GP with Categorical Mean Fn\nAs a fourth method to handle categorical inputs, I tried fitting a GP with a categorical mean function by first fitting a linear model with the factor variable as the predictor. Then I fit a GP using the residuals of the linear model as my response. The RMSE of this method was \\(3.15\\). The first plot below shows the predicted values versus the residuals while the second shows the \\(95\\%\\) confidence bands for the estimated function for each level."
  },
  {
    "objectID": "index.html#categorical-data-with-similar-levels",
    "href": "index.html#categorical-data-with-similar-levels",
    "title": "Weekly Reports",
    "section": "Categorical Data with Similar Levels",
    "text": "Categorical Data with Similar Levels\nI repeated the four methods described before on a new dataset where levels 1 and 2 of the factor were much closer together while level 3 was different. The new training data is plotted below.\n\n\n\n\n\n\n\n\n\nTo fit a model and perform inference, I compared the four methods used on the last dataset. First, simply ignoring the categorical variable (Method 1). Second, partitioning the data set by the categorical input and fitting a different GP to each section (Method 2). Third, encoding the categorical input as dummy variables and including those in the GP (Method 3). Fourth, fitting a linear model with the factor input followed by a GP with the continuous input (Method 4).\nFor Method 1, the prediction RMSE was \\(14.49\\). The maximum likelihood estimates of the parameters were \\(3.74\\) for the lengthscale of x1 and \\(0.288\\) for the nugget. For Method 2, the prediction RMSE was \\(9.09\\). The maximum likelihood estimates of the parameters were \\(1.60\\), \\(1.28\\), and \\(2.54\\) for the lengthscales of x1 given \\(x2 = {1,2,3}\\). For Method 3, the prediction RMSE was \\(16.98\\). I was not able to get MLE estimates for its parameters. For Method 4, the prediction RMSE was \\(6.67\\) and the MLE of its lengthscale was \\(1.29\\).\nFor each method, the predicted versus residual plot on the test data is shown alongside the \\(95\\%\\) confidence band of the predicted function. Note that though the residuals for Methods 2 and 3 look to be strictly positive, the median residual for Method 2 is \\(0.04\\) and the median residual for Method 3 is \\(1.01\\). They simply have large positive errors that obscure the distribution of errors."
  },
  {
    "objectID": "index.html#september-17-2024",
    "href": "index.html#september-17-2024",
    "title": "Weekly Reports",
    "section": "September 17, 2024",
    "text": "September 17, 2024\n\nSummary\nThis week I explored possible methods of incorporating categorical variables into a Gaussian process. I discovered several existing dissimilarity metrics for categorical variables that are in use for cluster analysis, including Gower’s, HL, and cosine dissimilarity. Gower’s dissimilarity returns the range-normalized Manhattan distance for continuous inputs and a 0 for a match and 1 otherwise for categorical inputs. HL (Hennig-Laou) dissimilarity is a modification of Gower’s that scales dummy-encoded categorical inputs to have roughly unit variance. Cosine dissimilarity is the dot product between two normalized vectors. In the paper, it is only applied to dummy-encoded categorical inputs, while continuous inputs are still treated with the Euclidean distance.\nI also compared three possible methods for handling categorical variables when fitting a GP: first, to simply ignore categorical inputs, second to partition the data before fitting models, and third to encode the categorical data as numeric dummy variables. The second method performed best, though I am still working on incorporating custom distance metrics and mean functions into a Gaussian process, which must be coded by hand since I couldn’t find an R package with that much flexibility.\n\n\nClustering Mixed Data\nA useful paper by described several clustering methods for data including both numeric and categorical variables (mixed data). Eight methods are described, all of which rely on distance or dissimilarity measures. Gower’s dissimilarity is one such measure, that encodes categorical distances as 0 if the levels are the same and 1 if they’re different. Another is proposed by Hennig and Laou, that modifies Gower’s method with a weighting scheme that controls variable importance.\nThe first two methods are Gower/PAM and HL/PAM, since they start with one of the dissimilarity measures described and pair it with a “Partitioning around Medoids” method that finds a set of points that minimize the average dissimilarity within clusters.\nK-Prototypes seeks to minimize the trace of the within cluster dispersion matrix cost function. It relies on the mode of each categorical input and does not weight categorical distances, which inspired Ahmad and Dey to propose a Mixed K-Means method that calculates categorical distances based on the co-occurrence of categorical values. Modha-Spengler K-Means is related, but uses the cosine dissimilarity for categorical variables.\nOther approaches to cluster analysis seek to reduce dimensionality before searching for clusters. One example is Factor Analysis for Mixed Data (FAMD), which is a compromise between principal component analysis and multiple correspondence analysis. When FAMD is followed by a K-Means clustering approach, the method is called FAMD/K-Means. Other authors suggest performing dimensionality reduction and K-means clustering simultaneously, called Mixed Reduced K-Means, where mixed refers to the presence of mixed data.\n\n\nGP with Categorical Data\nFirst, I simulated a dataset consisting of one continuous variable x1 and one categorical variable x2. The categorical variable had three levels: \\({1,2,3}\\). The response was generated using a function proposed by Han, et al, where a different quadratic function of the continuous variable is used for each level of the categorical variable. I generated a training dataset with \\(900\\) observations, and a test dataset with 93. The training data is plotted below.\n\n\n\n\n\n\n\n\n\nTo fit a model and perform inference, I tried three methods. First, simply ignoring the categorical variable (Method 1). Second, partitioning the data set by the categorical input and fitting a different GP to each section (Method 2). Third, encoding the categorical input as dummy variables and including those in the GP (Method 3).\nFor Method 1, the prediction RMSE was \\(8.45\\). The maximum likelihood estimates of the parameters were \\(1.82\\) for the lengthscale of x1 and \\(0.061\\) for the nugget. For Method 2, the prediction RMSE was \\(8.60\\). The maximum likelihood estimates of the parameters were \\(0.86\\), \\(2.52\\), and \\(3.11\\) for the lengthscales of x1 given \\(x2 = {1,2,3}\\). For Method 3, the prediction RMSE was \\(16.51\\). I was not able to get MLE estimates for its parameters.\nPlots of the predicted values versus residuals are shown below for all three methods."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Weekly Reports",
    "section": "",
    "text": "Over break, I studied what the conditional density of \\(\\alpha\\) looks like for small values of \\(n\\), when the normalizing constant of \\(p(z|\\alpha)\\) can be computed exactly. I also investigated the conditional density of \\(\\alpha\\) under the distance dependent CRP and concluded that, like the MCRP, it is not tractable for large \\(n\\).\nLastly, I considered what qualities we need in a clustering algorithm for analysis of large data sets and analyze how the iMGPE algorithm could be improved to improve its predictive performance.\n\n\n\nThe distribution of the cluster assignment vector \\(z\\) is a pmf over all possible partitions of \\(n\\) data points. Indexing each cluster by the index of its leftmost data point, I can recursively construct a list of all possible partitions over \\(n\\) elements, given such a list for \\(n-1\\) elements. It works by taking each entry of the previous list and appending all possible \\(n^{th}\\) elements. For example, taking \\(z=(1,1,3,4)\\), we could append a 1, 3, 4, or 5 in the fifth position.\nThe number of possible partitions for low values of \\(n\\) are plotted below.\n\n\n\n\n\n\n\n\n\nI then explored the shape of the conditional distribution of \\(\\alpha\\) for the small value of \\(n=7\\), when the normalizing constant can be computed exactly. I used \\(p(\\alpha|z)\\propto p(z|\\alpha)p(\\alpha)\\) where \\(p(z|\\alpha)\\) is as I calculated before and \\(p(\\alpha)\\) is a Gamma(1,1) prior. This conditional distribution is plotted below for six different values of \\(z\\).\n\n\n\n\n\n\n\n\n\nNote how the shape of the distribution does not change between the first two plots or between the next two, but does change between the first, third, and fifth plots.From this, we can confirm visually that the distribution of \\(\\alpha\\) depends only on the number of singleton clusters in \\(z\\) and not on the total number of clusters. The distributions look like gamma distributions, particularly in how they may have either a hump over a particular mode value of \\(\\alpha\\) or an infinite peak at zero.\n\n\n\nThe description of the Distance Dependent CRP by Blei and Frazier does not try to learn the \\(\\alpha\\) parameter but instead holds it constant. I believe their variation on the Chinese Restaurant Process also does not produce a computationally tractable joint density over the cluster assignment vector. Rather, like the MCRP, the density’s normalizing constant can only be found as the inverse sum of an exponentially increasing number of terms.\nTo demonstrate this, we can apply Brook’s lemma to the DD-CRP. Note that the DD-CRP defines a point assignment vector \\(c\\) where each point in the data set is assigned to another point or to itself. Thus, for \\(n=2\\), there are four potential point assignments: (1,1), (1,2), (2,1), and (2,2). The state (1,1), for example, indicates that point 1 is assigned to itself and point 2 is also assigned to point 1. All points that can be connected through these assignments are in the same cluster. Thus, only (1,2) represents a state in which there are two clusters. The rest all represent a single cluster.\nLet \\(c\\) be a cluster assignment vector and let \\(c'=1:n\\). Assume that \\(p(c'|\\alpha)\\propto \\alpha^n\\). Then, according to Brook’s lemma, the probability of \\(c\\) given \\(\\alpha\\) is\n\\[p(c|\\alpha)\\propto \\prod_{i=1}^n \\left[\\frac{P(c_i=j|c_{1:i-1}',c_{i+1:n})} {P(c_i'=j|c_{1:i-1},c_{i+1:n}')} \\right]p(c'|\\alpha) =\\prod_{i=1}^n f(d_{ic_i})I(c_i\\neq i) +\\alpha I(c_i=i)\\]\nThe normalizing constant for this distribution is the inverse sum of \\(n^n\\) terms, as that is the number of possible cluster assignments. The normalizing constant when \\(n=2\\) is \\((\\alpha^2+ 2\\alpha f(d_{12}) + f(d_{12})^2)^{-1}\\), for example. Since this constant involves \\(\\alpha\\) and cannot feasibly be determined for reasonably sized \\(n\\), we cannot directly sample from \\(p(\\alpha|c)\\) under the DD-CRP.\n\n\n\nAs things currently stand, there are two feasible paths before us. We can modify the iMGPE algorithm presented by Rasmussen et al by fixing \\(\\alpha\\) instead of learning it. This would be a relatively small change that allows us to continue pursuing our current research options: the incorporation of categorical inputs, deep GPs, or variable selection.\nWe could also abandon the MCRP and seek to find or devise a new clustering mechanism to replace it. If we do so, we have the opportunity to reconsider what properties we actually need in our clustering mechanism.\nThe situation that originally drew us to the iMGPE algorithm was that we have a very large data set such that fitting a single Gaussian process to all of it is infeasible. The iMGPE algorithm was a way to partition the data into smaller clusters so we could fit multiple GPs, one to each cluster. Randomly generating a new partition at each step of a Markov chain allowed us to capture our uncertainty in the partition.\nFor our purposes, we do not necessarily need to assume a “true” underlying cluster structure in our model. We don’t necessarily care whether the partitions generated by the Markov chain are “accurate” or “informative” except in so far as we would like the data in each cluster to be similar and easily fit by a single GP. We would prefer that the data be somewhat evenly distributed between the clusters, as clusters with few points make for poor GP experts and clusters with too many take much longer to fit.\nIn conclusion, if we stick with the iMGPE, then the primary drain on predictive performance is the quality of the clusters, rather than the number. I would investigate how we could improve the similarity of points within clusters and even out the distribution of points between clusters. If we choose to select a different clustering mechanism, I would look for one with the qualities described above."
  },
  {
    "objectID": "index.html#july-17-2025",
    "href": "index.html#july-17-2025",
    "title": "Weekly Reports",
    "section": "July 17, 2025",
    "text": "July 17, 2025\n\nSummary\nThis week I mostly finished coding a version of the iMGPE algorithm that uses a distance dependent Chinese Restaurant Process as described by Blei and Frazier to cluster the data.\n\n\nDDCRP-iMGPE\nAfter rewriting the iMGPE code to include DDCRP, I ran \\(5000\\) iterations of the combined algorithm on our simulated data set \\(1\\), with ML optimization as the GP fitting method. The results were promising. After the first couple of iterations, it chose 2 clusters every time. The trace plots of the parameters \\(\\alpha\\) and \\(\\phi\\) are shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhile \\(\\alpha\\) converges to about \\(0.4\\) quickly, \\(\\phi\\) seemingly fails to converge even after climbing almost to \\(100\\). The sampling step for \\(\\phi\\) had to be modified, as it involves calculating the probability of observing the current cluster assignment, so this could be due to an error in my code, though it also may be that \\(\\phi\\) just behaves differently under this clustering scheme.\nI was not able to get the prediction section of the algorithm to work, though I expect to get that working shortly.\n\n\nGP Method Comparison\nTo study why the different GP fitting methods perform differently on prediction, I compared the predictive performance of my three GP methods (ML optimization via ‘laPG’, STAN sampling, and slice sampling) on two fixed cluster assignments. First, I tested them on the full Simulated Data Set \\(1\\). Then I partitioned that data set into ten clusters of ten points with cluster 1 containing the ten smallest points and cluster 2 containing the ten next smallest and so on.\nI found that different GP methods did not perform that differently on prediction given the same cluster assignment. However, the MLE experts did differ from the STAN and slice sampling experts on their estimated parameter values. The ML lengthscale and nugget estimates were much smaller than the STAN and slice sampling parameter estimates.\nBelow, I have plotted the estimated function for each method, based on the test set. The true function line is shown in black. The red line is the slice sampling prediction, the blue line is the STAN prediction, and the green line is the MLE prediction.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll methods perform very well when trained on the full data. When I use ten models, each trained on a tenth of the data set, their predictions collapse towards zero, though the ML-based method performs better than the sampling methods. I believe the cause is my formula for posterior predictions, which is based on a sum of experts weighted by the probability of assigning a new point to each cluster. This formula puts too much weight on experts that have no knowledge of the target area and thus predict the new point as zero.\nThis can be demonstrated by focusing on one point from the test data. I will use \\(x^*=0.01\\), which has an associated response of \\(y^*=2.33\\). The ten estimates generated by my ten slice sampling GP experts are \\((2.17, 1.20, 1.12, 0.80, 0.50, 0.35, 0.06, -0.08, -0.01, 0.01)\\). Recall that the first expert is trained on the leftmost points in the training data and so on, and we see reasonably enough that the first expert provides the best estimate of \\(x^*\\), the second provides the next best and so on. My prediction algorithm then calculates the probability of adding \\(x^*\\) to each cluster and weights the ten estimates by these probabilities. The ten probabilities of \\(x^*\\) being assigned to clusters 1 to 10 are \\((0.1034, 0.0960, 0.1013, 0.1039, 0.1021, 0.1019. 0.1002, 0.1004, 0.1020, 0.0889)\\). While the probabilities are decreasing from left to right, as we would expect, they are close to evenly distributed, which puts way too much credence on experts who have no information on the target point.\nThe obvious route to improve my predictions is by adjusting the expert weights. I intend to experiment with this. One option would be to predict each point using only the expert to which it is most likely to be assigned. Another would be to implement a cutoff, where experts that are deemed too far from the target point would not contribute to the weighted sum."
  },
  {
    "objectID": "index.html#july-10-2025",
    "href": "index.html#july-10-2025",
    "title": "Weekly Reports",
    "section": "July 10, 2025",
    "text": "July 10, 2025\n\nSummary\nThis week I built a Quarto website to store my thesis work and weekly reports going forward. I also investigated possible factors for a factorial study of the existing variations of the iMGPE algorithm. Lastly, I began coding an iMGPE algorithm that uses the distance dependent Chinese Restaurant Process.\n\n\nPrep for Factorial Study\nWhile I was not able to arrange a factorial study of the iMGPE algorithm, I did put some thought into what levels should be included in one. The first part of the algorithm I can vary is how the GP experts are fit, the options being ML optimization, STAN, and my homemade slice sampler. The next is the priors placed on \\(\\alpha\\), \\(\\phi\\), and \\(\\theta\\). A third, discussed by this method’s authors, is to cap the number of points that can be included in a single cluster with the parameter “maxSize”.\nI considered a couple of other factors such as the initial cluster assignment and the proposal distributions for the samplers for \\(\\alpha\\), \\(\\phi\\), and the GPs. I decided not to include the first since testing indicated that the initial cluster assignment has no real effect on future clusters. Likewise, I decided against the latter since a good proposal distribution shouldn’t affect the output of the sampler.\nI settled on testing two plausible priors for \\(\\theta\\): an InverseGamma\\((1,1)\\) distribution and a LogNormal\\((\\mu=0,\\sigma^2=1)\\) distribution. The lognormal distribution should have heavier tails than the inverse gamma. Both priors were tested with each GP fitting method. Additionally, each GP method was tested with unlimited cluster sizes and with cluster size capped at \\(50\\).\nFor the slice sampler version, I completed three runs: one with the default inverse gamma prior on \\(\\theta\\), one with the lognormal prior on \\(\\theta\\), and one with the inverse gamma prior and “maxSize” set to \\(50\\). The estimated mean functions of each run are plotted below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the STAN version, I completed three runs: one with the default inverse gamma prior on \\(\\theta\\), one with the lognormal prior on \\(\\theta\\), and one with “maxSize” set to \\(50\\). The estimated mean functions of each run are plotted below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe clearest takeaway was that my jury-rigged slice sampling method to fit a GP expert is not very effective. The slice sampler version displays much more oversmoothing, to the point of seeming to ignore the data entirely. Interestingly, the slice sampler version seems to break entirely when the \\(\\theta\\) prior or the maximum cluster size is changed, while the STAN version handles both just fine.\n\n\nDDCRP\nBased on R code provided by Blei and Frazier, I was able to begin coding a version of the iMGPE algorithm that uses the Distance Dependent Chinese Restaurant Process to cluster the data. This required rewriting the cluster update step, the \\(\\phi\\) sampling step, and the posterior prediction step.\nThe code is not quite functional yet, but it will be soon."
  },
  {
    "objectID": "index.html#june-26-2025",
    "href": "index.html#june-26-2025",
    "title": "Weekly Reports",
    "section": "June 26, 2025",
    "text": "June 26, 2025\n\nSummary\nThis week I researched variations of the Chinese Restaurant Process similar to the modified CRP presented by Rasmussen and Ghahramani. The MCRP does not appear to be well documented in literature, but I found a similar algorithm called the distance dependent CRP. These variations appear to be a subset of the field of random measures over data partitions.\nI also studied the MCRP prior on a new data set with three distinct clusters, allowing me to examine how well the true cluster structure is captured for varying parameter values. As expected, \\(\\phi\\) controls the degree to which similar points prefer to cluster together, seen in how two measures of cluster quality improve as \\(\\phi\\) decreases.\nI continue to test the STAN and slice sampler iMGPE algorithms on strictly positive data sets. The STAN version performs very well, while the slice sampler version struggled.\n\n\nDistance Dependent CRP\nThere does not appear to be any research about the specific modified Chinese Restaurant Process used by Rasmussen and Ghahramani. However, the authors Blei and Frazier discuss a similar algorithm in their 2010 paper Distance Dependent Chinese Restaurant Processes. The DD-CRP differs from the regular and modified CRP in that rather than assigning customers to tables (that is, points to clusters) each customer is assigned to sit with another customer or with themselves. The table assignments are derived from these customer assignments by grouping together all customers who can be linked by a sequence of customer pairings into a table.\nThe conditional probability of customer \\(i\\) being assigned to sit with customer \\(j\\) is dependent on the distance between them. Given \\(D\\), the pairwise distances between all points, and \\(\\alpha\\), the concentration parameter, the probability of customer assignment \\(c_i\\) is as follows.\n\\[p(c_i=j|D,\\alpha)\\propto \\begin{cases}\nf(d_{ij}) \\text{ if } j\\neq i\\\\\n\\alpha \\text{ if } i=j\n\\end{cases}\\]\nNote that this differs from our MCRP (and the regular CRP) in that the assignment of point \\(i\\) only depends on the distances between customers, and not on the customer or cluster assignments of other points.\nThe authors demonstrate that the regular CRP can be characterized as a special case of the distance dependent CRP. Furthermore, the regular CRP is the only marginally invariate distance dependent CRP, meaning that marginalizing over a particular customer yields the same probability distribution as if the customer was not included at all. They go on to describe a Gibbs sampling formula for the DD-CRP.\nThe modified CRP used by Rasmussen and Ghahramani does not appear to have been theoretically explored or justified. While certain properties of the DD-CRP are described, it is not considered in the context of a mixture of experts model. I came across a decent sized array of papers on the more general topic of using random measures to define probability distributions over partitions, of which the CRP, DD-CRP, and probably the MCRP are all examples, but did not have time to synthesize them here.\nIn summary, I think it may be worthwhile to consider reworking the iMGPE algorithm to use a clustering process that is more clearly defined or easier to work with in terms of theoretical properties. It may be possible to prove, or disprove, qualities such as consistency for the MCRP as well.\n\n\nPrior Analysis of MCRP\nI tested the prior distribution of the Modified Chinese Restaurant Process on a fully segregated data set with three clusters and 100 points in total. I drew \\(40\\) points from the interval \\([0,1]\\), \\(30\\) from \\([3,4]\\), and \\(30\\) from \\([7,8]\\). Since this data set has a true cluster structure, we can evaluate how well the MCRP captures it through metrics such as cluster purity and the Adjusted Rand Index.\nCluster purity is calculated by assigning each cluster to the class most frequent in that cluster and then finding the percent of the total number of data points that were classified correctly given those assignments. It ranges from \\(0\\) to \\(1\\) with \\(1\\) being perfect purity, though this is also affected by the number of points and clusters. The Adjusted Rand Index is a measure of the similarity between the generated partition and the true partition of the data set. It ranges from roughly \\(0\\) to \\(1\\), with \\(1\\) indicating a perfect match to the true cluster structure and \\(0\\) indicating a fully random cluster assignment.\nAfter correcting a typo in my MCRP code, my initial intuition was borne out. I started with all points assigned to the same cluster and performed \\(1000\\) MCMC updates according to the MCRP algorithm. I repeated this process with a few different initial states, but they did not have a significant effect on the final state. Considering the results, we see both metrics of cluster accuracy improve as \\(\\phi\\) gets smaller, while the number of clusters varied only with \\(\\alpha\\) staying at about \\(5\\) clusters for \\(\\alpha=1\\), \\(17\\) clusters for \\(\\alpha=3\\), and \\(34\\) clusters for \\(\\alpha=9\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that cluster purity tends to increase slightly as the number of clusters increases (most visible on the third column) since smaller clusters are more likely to be pure even when generated randomly.\n\n\nMonte Carlo Methods on New Data\nThis week I tested the STAN and slice sampler versions of the iMGPE on strictly positive data sets. The slice sampler version was tested on the data set from last week, where \\(f(x)=1+0.3x+\\frac{\\sin(\\pi x)}{5x}\\). The STAN version was tested on a similar data set where \\(f(x)=1+0.5x+\\frac{\\sin(\\pi x)}{2x}\\). The posterior estimate of the function is plotted below for each, with STAN on the left and the slice sampler on the right.\nThe blue line is the true function path and the red line is the median of the fitted values. The upper grey band shows the \\(95\\%\\) credible band over the test set while the lower grey band is a visual aid displaying the width of the credible band with its lower bound fixed at a level below the graph. The mean MC standard error for the \\(2.5th\\), \\(50th\\) and \\(97.5th\\) quantiles is \\(0.03\\), \\(0.02\\), and \\(0.01\\) for the STAN version and \\(0.04\\), \\(0.02\\) and \\(0.01\\) for the slice sampler version.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnexpectedly, the slice sampler displays high uncertainty towards the right half of the plot and severely underestimates not just the true function path but the observed data as well. I suspect the algorithm’s estimating function, which takes a weighted mean of every cluster’s estimate of a new data point, is still putting too much weight on the estimates of distant clusters. I am not sure why it would perform so much worse than the STAN version though."
  },
  {
    "objectID": "index.html#june-19-2025",
    "href": "index.html#june-19-2025",
    "title": "Weekly Reports",
    "section": "June 19, 2025",
    "text": "June 19, 2025\n\nSummary\nThis week I investigated the Modified Chinese Restaurant Process and attempted to reason out the role of the parameter \\(\\phi\\) with some success. I also studied the new versions of iMGPE with STAN and slice sampling based GP experts and analyzed the Monte Carlo standard error of their quantile estimates.\nI also tested the iMGPE algorithm on a new simulated data set with a strictly positive response. The predictive posterior distribution appears to be over-smoothed; it estimates the function as closer to a straight line than it really is.\n\n\nMCRP Prior\nThis week, I continued to study the Modified Chinese Restaurant Process used in the iMGPE algorithm with particular attention to the role played by the parameter \\(\\phi\\). Previous experiments had indicated that \\(\\phi\\) had little influence on the average number of clusters and new experiments over a broader range of \\(\\phi\\) values simply bore this out.\nUpon reflection, I realized that \\(\\phi\\) shouldn’t have a major impact on the number of clusters anyways. The number of clusters would be controlled primarily by the probability of assigning a point to a new cluster, which is \\(\\alpha/(n+\\alpha)\\); that is, it depends only on \\(\\alpha\\) and the number of data points. Experimentation with the MCRP conditional assignment probabilities seemed to indicate that \\(\\phi\\) controls how strongly a point prefers to join a cluster that is close to it in \\(X\\). As \\(\\phi\\) approaches infinity, the probability of a point joining a specific cluster approaches proportionality with the number of points in that cluster. As \\(\\phi\\) approaches zero, the probability of a point joining a specific cluster approaches one if that cluster is the closest to the point and zero otherwise.\nTherefore, we should not expect \\(\\phi\\) to influence the number of clusters generated but rather how points are distributed among the clusters, with smaller \\(\\phi\\) values tending towards more separation between clusters. Unfortunately, I struggled to verify this empirically, as it is difficult to express separation between clusters in a compact form. I found one single number summary, the Dunn Index, but when tested on a range of values of \\(\\alpha\\) and \\(\\phi\\) (\\((0.1,1,10)\\) and \\((1,10,100)\\) respectively) it did not vary significantly.\nA clearer pattern was visible when considering the spread of cluster sizes, that is, the difference between the largest cluster size and the smallest. As \\(\\phi\\) increases, the spread shrinks slightly. Admittedly, I am not certain why this would be the case. Further investigation is warranted.\n\n\n\n\n\n\n\n\n\nAnother plausible use of the \\(\\phi\\) parameter would be if we had multiple input variables. Then \\(\\phi\\) could determine the importance of a given \\(X\\) input for clustering.\n\n\nUpdated Experimental Results\nI tested the STAN version of the iMGPE algorithm with \\(2000\\) iterations per expert instead of \\(500\\). For now, I am still limited to \\(500\\) MCMC iterations in total due to memory restraints. The results did not appear to be significantly different.\nThe predicted posterior of the STAN version is plotted below on the left. The blue line is the true function path and the red line is the median of the fitted values. The upper grey band shows the \\(95\\%\\) credible band over the test set while the lower grey band is a visual aid displaying the width of the credible band with its lower bound fixed at \\(-2\\).\nOn the right is a plot of the \\(95\\%\\) credible intervals for the median and \\(95\\%\\) credible interval bounds of the estimated function given the Monte Carlo standard error. The blue line is the true function path. The solid red line is the median of the fitted vales and the two dashed red lines are the \\(2.5th\\) and \\(97.5th\\) quantiles of the fitted values. The grey ribbons are the MCSE credible intervals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe average MC standard error for the median was \\(0.012\\) and for the \\(2.5th\\) and \\(97.5th\\) quantiles it was \\(0.019\\) and \\(0.022\\) respectively.\nFor comparison, here are the same plots for the slice sampling version. On the left is the predicted posterior with the true and median fitted functions plotted in blue and red and the \\(95\\%\\) credible band as the grey band. On the right is the plot of the \\(95\\%\\) credible bands for the \\(2.5th\\), \\(50th\\), and \\(97.5th\\) percentiles given the Monte Carlo standard error of their estimates.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe average MC standard error for the median was \\(0.015\\) and for the \\(2.5th\\) and \\(97.5th\\) quantiles it was \\(0.022\\) and \\(0.027\\) respectively.\n\n\nNew Simulated Data Set\nOur previous experiments on both the motorcycle data and the simulated data set displayed an unexpected shrinkage effect on the fitted function line, which we theorized to be towards the grand mean or perhaps \\(y=0\\). To distinguish between these possibilities, I fit a new instance to a new data set with a strictly positive response. A new data set of \\(100\\) points was generated by the following function.\n\\[f(x)=1+0.3x+\\frac{\\sin(\\pi x)}{5x}\\]\nI tried fitting this data set with the optimization method and obtained the plot of the fitted function below. The fitted line undershoots the true line (in blue) in some places but overshoots in others. Overall, it seems to under-fit the function line, preferring a straighter path than what the function actually follows."
  },
  {
    "objectID": "index.html#june-12-2025",
    "href": "index.html#june-12-2025",
    "title": "Weekly Reports",
    "section": "June 12, 2025",
    "text": "June 12, 2025\n\nSummary\nThis week I refined my data model description and DAG chart. I also evaluated the prior distribution of the number of clusters generated by the modified Chinese Restaurant Process used in iMGPE, finding that it is often much higher than the observed number of clusters.\nI developed two new versions of the iMGPE algorithm that replaced the optimization-based GP evaluation step with two sampling-based methods: STAN and slice sampling. These methods take much longer to run than the optimization version but produce estimates of similar quality.\n\n\nData Model\nWe have an \\(n\\times 1\\) continuous response vector \\(y\\) and an \\(n\\times d\\) data matrix \\(X\\). The estimated value of a data point \\(y_i\\) under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing \\(y_i\\) and weighted by a Dirichlet process. Let \\(z\\) represent a possible vector of cluster assignments and let \\(z^{(k)}\\) be the \\(k^{th}\\) element of some ordered list of all possible \\(z\\). Then \\(j=1,\\dots,J_k\\) index the clusters within \\(z^{(k)}\\). Let \\(C_j^{(k)}\\) be the number of observations in cluster \\(j\\) given assignment \\(z^{(k)}\\). Then we have as follows.\n\\[y\\sim \\sum_{k=1}^{p(n)} \\left[\\prod_{j=1}^{J_k} N_{C_j^{(k)}}(0,\\Sigma_{\\theta_j}) \\right] w_k\\] \\[w_k=P(z=z^{(k)}|\\alpha,\\phi)\\] where the \\(w_z\\) are the marginal probabilities of a modified Chinese Restaurant Process that generates cluster assignments \\(z\\) and \\(p(n)\\) is the partition function. The CRP used here has been modified to depend on the input data \\(X\\). It is controlled by two parameters, \\(\\alpha\\) and \\(\\phi\\), the first being the usual concentration parameter and the second controlling the cluster occupancy estimates. A more in-depth explanation of this CRP is provided in the iMGPE Algorithm section from last week. It should also be noted that there is no known closed form for \\(w_k\\). The closest I could find was the multivariate Ewen’s distribution, which describes the distribution on the set of \\(p(n)\\) that arises from a regular Chinese Restaurant Process. However, it only accounts for the number of clusters of different sizes, and does not serve as a marginal distribution of \\(z\\).\nThe full joint distribution, including the priors for all parameters, is as follows. Here, \\(\\Phi\\) is the pdf of a normal distribution.\n\\[p(y,\\theta,\\phi,\\alpha)=\\left[ \\sum_{k=1}^{p(n)} \\left[\\prod_{j=1}^{J_k} \\Phi_{C_j^{(k)}}(0,\\Sigma_{\\theta_j}) \\right] p(z(k)|\\alpha,\\phi)\\right] p(\\theta)p(\\phi)p(\\alpha)\\]\nAlternatively, the model can be expressed in hierarchical terms where \\(J_z\\) is the number of clusters in \\(z\\) and \\(C_{j,z}\\) is the number of observations in the \\(j^{th}\\) cluster in \\(z\\).\n\\[y|z\\sim \\prod_{j=1}^{J_z} N_{C_{j,z}}(0,\\Sigma_{\\theta_{j}})\\] \\[z|\\alpha,\\phi \\sim MCRP(\\alpha,\\phi)\\]\nHere, \\(N_{C_j^{(z)}}(0,\\Sigma_{\\theta_j})\\) is the \\(C_j^{(z)}\\)-dimensional multivariate normal distribution with covariance matrix \\(\\Sigma_{\\theta_j}\\) defined by a Gaussian kernel function with parameters \\(\\theta_j\\).\nThen \\(\\theta_j\\) is the parameter vector for the GP expert assigned to cluster \\(j\\), while \\(\\alpha\\) is the CRP concentration parameter and \\(\\phi\\) is the parameter vector for the CRP’s occupation number estimate. Note that \\(\\phi\\) is purely a vector of lengthscales for a Gaussian kernel. The priors on \\(\\theta\\), \\(\\alpha\\), and \\(\\phi\\) are described below.\n\\[\\theta_{j,k}\\stackrel{ind}{\\sim} Gamma(a_k,b_k) \\text{ for } k=1,\\dots,d\\] \\[\\alpha\\sim Inv.Gam(1,1),\\text{   } \\phi_k\\stackrel{iid}{\\sim} LogN(0,1) \\text{ for } k=1,\\dots,d\\] That is, each element \\(k\\) of \\(\\theta_j\\) (the dimension of \\(X\\) plus a noise parameter) is assigned a independent Gamma prior with fixed parameters \\(a_k\\) and \\(b_k\\). Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of \\(\\phi\\) receives an independent log-normal prior.\nA third visualization of the model structure is a directed acyclic graph, shown below. Starting with the priors for \\(\\alpha\\), \\(\\phi\\), and \\(\\theta\\), we can draw their values and generate our latent variables \\(z\\) and our true variables \\(y\\). Note that \\(\\theta\\) depends on \\(z\\) as well as its prior, as \\(z\\) defines the number of clusters and thus the number of GP parameters to be drawn. The red and blue boxes indicate the quantities which are drawn multiple times for each of the \\(n\\) data points or each of the \\(J\\) clusters.\n\n\n\n\n\n\n\n\n\n\n\nPrior of MCRP\nThe modified Chinese Restaurant process is dependent on the input data \\(X\\) through the parameter \\(\\phi\\) in addition to the parameter \\(\\alpha\\) used by a regular CRP. It is worth considering how this affects the prior distribution of our cluster assignment vector \\(z\\). While there is no closed form for the marginal distribution of \\(z\\), we can study it empirically by generating samples of \\(z\\) using the algorithm described in the iMGPE Algorithm section last week.\nUsing the simulated dataset, described in the April 24 section, I studied the average number of clusters generated by the MCRP. I chose the values \\(\\alpha=1,3,5\\) and \\(\\phi=2.5,5,7.5\\) to cover the most common range of values observed in our data and generated a set of \\(1000\\) \\(z\\) vectors for each combination of \\(\\alpha\\) and \\(\\phi\\). The mean and standard deviation of the number of clusters in those sets are shown below.\n\n\n\n\\(\\alpha\\)\n\\(\\phi\\)\nmean\nsd\n\n\n\n\n1\n2.5\n4.42\n1.86\n\n\n1\n5\n4.86\n1.84\n\n\n1\n7.5\n5.04\n1.99\n\n\n3\n2.5\n17.6\n3.26\n\n\n3\n5\n17.8\n3.55\n\n\n3\n7.5\n17.9\n3.27\n\n\n5\n2.5\n27.1\n3.66\n\n\n5\n5\n27.1\n3.87\n\n\n5\n7.5\n26.8\n3.75\n\n\n\nThese results can be compared to the expected numbers of clusters from a standard CRP with the same \\(\\alpha\\) value, which is \\(\\alpha(\\psi(n+\\alpha)-\\psi(\\alpha))\\) where \\(n\\) is the number of data points and \\(\\psi\\) is the digamma function. For \\(\\alpha\\) values of \\(1\\), \\(3\\), and \\(5\\), the expected numbers of clusters are \\(5.2\\), \\(11.1\\), and \\(15.7\\). We can see that the data dependency in the MCRP has increased the expected number of clusters and the inflation gets bigger as \\(\\alpha\\) increases. However, \\(\\phi\\) does not appear to have a strong influence on the number of clusters, at least within the range I examined.\n\n\niMGPE with STAN & Slice Sampling\nI coded two new versions of the iMGPE algorithm: one that fit GP experts using Hybrid Monte Carlo using STAN and one that fit GP experts by sampling their parameters with elliptical slice sampling. The STAN experts ran for \\(500\\) iterations each while the slice sampler drew \\(100\\) sample parameter values from the GP posterior and averaged them. Both took much longer to run than the optimization version and had to be run for fewer iterations. Particularly, running the STAN version for more than \\(600\\) or so iterations caused a “Maximum number of DLLs reached” error that I haven’t yet determined how to fix. Thus, I am presenting a \\(500\\) iteration run of the STAN version and a \\(1000\\) iteration run of the slice sampling version.\nBoth methods were trained on the simulated data set and compared on a test set of \\(500\\) evenly spaced points between \\(0\\) and \\(5\\). The predicted posterior of the STAN version is plotted below. The blue line is the true function path and the red line is the median of the fitted values (the mean proved to be much less coherent). The upper grey band shows the \\(95\\%\\) credible band over the test set while the lower grey band is a visual aid displaying the width of the credible band with its lower bound fixed at \\(-2\\).\n\n\n\n\n\n\n\n\n\nThe predictive posterior of the slice sampling version is plotted here. Unlike the STAN version, the red line here does portray the mean rather than the median.\n\n\n\n\n\n\n\n\n\nThe STAN and slice sampling versions are a bit less accurate than the optimization version in terms of their estimated function. That is, the red line strays further from the blue line in these versions. The STAN version, however, has equal or less uncertainty in its estimate than the optimization version, while the slice sampling version has more."
  },
  {
    "objectID": "index.html#may-29-2025",
    "href": "index.html#may-29-2025",
    "title": "Weekly Reports",
    "section": "May 29, 2025",
    "text": "May 29, 2025\n\nSummary\nSince our last meeting, I have improved the data model and algorithm descriptions and added a directed acyclic graph to portray the relationships between the model components. I also investigated the cause of high uncertainty in the estimated function occurring in areas with many data points. This does not seem to be caused by interplay between the estimated nugget and the function, as the nugget estimates are both small and highly consistent across the test data. More likely, it is an artifact of the clustering process, where points are occasionally put in the ‘wrong’ cluster or a cluster is stacked with disparate points, whose GP subsequently has greater uncertainty in its estimates.\nI also coded a functioning iMGPE algorithm that fits GP experts with an MCMC process via STAN. I have not yet completed a full run of the new model as it is much slower than the original.\n\n\nData Model Description\nWe have an \\(n\\times 1\\) continuous response vector \\(y\\) and an \\(n\\times d\\) data matrix \\(X\\). The estimated value of a data point \\(y_i\\) under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing \\(y_i\\) and weighted by a Dirichlet process. Let \\(z\\) represent a possible vector of cluster assignments and let \\(z^{(k)}\\) be the \\(k^{th}\\) element of some ordered list of all possible \\(z\\). Then \\(j=1,\\dots,J_k\\) index the clusters within \\(z^{(k)}\\). Let \\(C_j^{(k)}\\) be the number of observations in cluster \\(j\\) given assignment \\(z^{(k)}\\). Then we have as follows.\n\\[y\\sim \\sum_{k=1}^{p(n)} \\left[\\prod_{j=1}^{J_k} N_{C_j^{(k)}}(0,\\Sigma_{\\theta_j}) \\right] w_k\\] \\[w_k=P(z=z^{(k)}|\\alpha,\\phi)\\] where the \\(w_z\\) are the marginal probabilities of a modified Chinese Restaurant Process that generates cluster assignments \\(z\\) and \\(p(n)\\) is the partition function. The CRP used here has been modified to depend on two parameters, \\(\\alpha\\) and \\(\\phi\\), the first being the usual concentration parameter and the second controlling the cluster occupancy estimates. A more in-depth explanation of this CRP is provided in the next section. It should also be noted that there is no known closed form for \\(w_k\\). The closest I could find was the multivariate Ewen’s distribution, which describes the distribution on the set of \\(p(n)\\) that arises from a regular Chinese Restaurant Process. However, it only accounts for the number of clusters of different sizes, and does not serve as a marginal distribution of \\(z\\).\nThe full joint distribution, including the priors for all parameters, is as follows. Here, \\(\\Phi\\) is the pdf of a normal distribution.\n\\[p(y)=\\left[ \\sum_{k=1}^{p(n)} \\left[\\prod_{j=1}^{J_k} \\Phi_{C_j^{(k)}}(0,\\Sigma_{\\theta_j}) \\right] P(z(k)|\\alpha,\\phi)\\right] p(\\theta)p(\\phi)p(\\alpha)\\]\nAlternatively, the model can be expressed in hierarchical terms where \\(J_z\\) is the number of clusters in \\(z\\) and \\(C_{j,z}\\) is the number of observations in the \\(j^{th}\\) cluster in \\(z\\).\n\\[y|z\\sim \\prod_{j=1}^{J_z} N_{C_{j,z}}(0,\\Sigma_{\\theta_{j}})\\] \\[z|\\alpha,\\phi \\sim MCRP(\\alpha,\\phi)\\]\nHere, \\(N_{C_j^{(z)}}(0,\\Sigma_{\\theta_j})\\) is the \\(C_j^{(z)}\\)-dimensional multivariate normal distribution with covariance matrix \\(\\Sigma_{\\theta_j}\\) defined by a Gaussian kernel function with parameters \\(\\theta_j\\).\nThen \\(\\theta_j\\) is the parameter vector for the GP expert assigned to cluster \\(j\\), while \\(\\alpha\\) is the CRP concentration parameter and \\(\\phi\\) is the parameter vector for the CRP’s occupation number estimate. Note that \\(\\phi\\) is purely a vector of lengthscales for a Gaussian kernel. The priors on \\(\\theta\\), \\(\\alpha\\), and \\(\\phi\\) are described below.\n\\[\\theta_{j,k}\\stackrel{ind}{\\sim} Gamma(a_k,b_k) \\text{ for } k=1,\\dots,d\\] \\[\\alpha\\sim Inv.Gam(1,1),\\text{   } \\phi_k\\stackrel{iid}{\\sim} LogN(0,1) \\text{ for } k=1,\\dots,d\\] That is, each element \\(k\\) of \\(\\theta_j\\) (the dimension of \\(X\\) plus a noise parameter) is assigned a independent Gamma prior with fixed parameters \\(a_k\\) and \\(b_k\\). Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of \\(\\phi\\) receives an independent log-normal prior.\nA third visualization of the model structure is a directed acyclic graph, shown below. Starting with the priors for \\(\\alpha\\), \\(\\phi\\), and \\(\\theta\\), we can draw their values and generate our latent variables \\(z\\) and our true variables \\(y\\). Note that \\(\\theta\\) depends on \\(z\\) as well as its prior, as \\(z\\) defines the number of clusters and thus the number of GP parameters to be drawn.\n\n\n\n\n\n\n\n\n\n\n\nThe iMGPE Algorithm\nThe modified CRP used in this algorithm is defined by “R. M. Neal” (Algorithm 8 in that paper with \\(m=1\\)) and works as follows:\nWe represent the current cluster state with assignment labels \\(z=(z_1,\\dots,z_n)\\) and GP parameter vectors \\(\\theta_1,\\dots,\\theta_J\\) where \\(J\\) is the number of clusters in the current state. For \\(i=1,\\dots,n\\), repeat the following. Let \\(J^{-i}\\) be the number of clusters in \\(z\\) with point \\(i\\) removed. Let \\(\\theta_{J^{-i}+1}\\) be a parameter vector drawn from its prior distribution, in this case \\(Gamma^d(a,b)\\). Draw a new value for \\(z_i\\) with the following conditional probabilities:\n\\[P(z_i=j|z_{-i},y_i,\\dots)\\propto \\begin{cases}\n\\frac{n-1}{n+\\alpha-1}\\frac{\\sum_{i'\\neq i,z_{i'}=j} K_{\\phi}(X_i,X_{i'})}{\\sum_{i'\\neq i} K_{\\phi}(X_i,X_{i'})} f(y_i|\\theta_j) \\text{ for } j=1,\\dots,J^{-i}\\\\\n\\frac{\\alpha}{n+\\alpha-1}f(y_i|\\theta_{J^{-i}+1}) \\text{ for } j=J^{-i}+1\n\\end{cases}\\]\nwhere \\(f(y_i|\\theta_j)\\) is the normal density of \\(y_i\\) given the kriging equations with parameter vector \\(\\theta_j\\) defining the kernel function.\nI have implemented the Infinite Mixture of Gaussian Process Experts algorithm mostly as described by the authors Rasmussen and Ghahramani, though with a few alterations of my own which are noted below. First, I initialize indicator variables \\(z\\) to a set of values. I generally start by assigning all points to a single cluster. I set gamma prior distributions on the lengthscale and nugget parameters of the GP experts, using the ‘darg’ and ‘garg’ functions of the package ‘laGP’ and set initial values for \\(\\alpha\\) and \\(\\phi\\). This approach then iterates through the following MCMC algorithm.\n\nPerform a Gibbs sampling sweep over the cluster assignment indicators, using the modified Chinese Restaurant Process described in the model explanation, to generate a new cluster assignment vector \\(z\\).\nFit a Gaussian process expert to each cluster in \\(z\\) and get ML estimates of each expert’s parameters \\(\\theta_j\\). Note that this is not a sampling step but maximization.\nSample the Dirichlet process concentration parameter, \\(\\alpha\\), using quantile slice sampling with a \\(Gamma(1,1)\\) proposal distribution. The posterior distribution of \\(\\alpha\\) we sample from is \\[p(\\alpha|n, J)\\propto \\alpha^{J-3/2}\\exp(-1/2\\alpha)\\Gamma(\\alpha)/\\Gamma(n+\\alpha)\\]\nSample the other CRP parameter \\(\\phi\\) via random walk Monte Carlo. The random walk step uses a normal proposal distribution centered at the current value and with variance equal to \\((2.38^2/d)H^{-1}\\), \\(d\\) being the number of inputs and \\(H\\) the Hessian matrix of the distribution of \\(\\phi\\). The posterior distribution of \\(\\phi\\) we are sampling from is \\[p(\\phi|z,\\alpha,\\dots)\\propto p(z|y,\\phi,\\alpha)p(\\phi)\\approx \\left[\\prod_{i=1}^n p(z_i|y,\\phi,\\alpha) \\right] p(\\phi)\\]\nRepeat from step 1 until the MCMC output has converged.\n\n\n\nAnalysis of Current Results\nIn the current algorithm, we see variation in the uncertainty of the fitted mean on the test data that is hard to explain given our data, with patches of relatively high uncertainty in areas with many data points (such as around \\(x=3\\)). This does not seem to be due to variation in the nugget parameter estimates, as a plot of the average and \\(95\\%\\) credible band for the estimated nugget for each observation is strongly uniform and centered around the relatively small value of \\(0.01\\). On a positive note, this is quite close to the true constant nugget value of \\(0.02\\).\nAs before, we drew an estimated value for each point on the test set every fifth iteration, using the formula \\(\\hat\\mu_i = \\sum_{j=1}^J p(z_i=j|\\alpha,\\phi)\\mu_{i,j}\\) where \\(\\hat\\mu_i\\) is the estimate for observation \\(i\\) in the test set, \\(\\mu_{i,j}\\) is the fitted value of point \\(i\\) under the GP for cluster \\(j\\), and \\(p(z_i=j|\\alpha,\\phi)\\) is the conditional probability of point \\(i\\) being assigned to cluster \\(j\\) under our modified CRP.\nThe resultant credible band for the simulated function is graphed below, based on \\(1000\\) draws from the predictive posterior. The blue line is the true function path and the red line is the mean of the fitted values. The upper grey band shows the \\(95\\%\\) credible band over the test set while the lower grey band is a visual aid displaying the width of the credible band with its lower bound fixed at \\(-2\\).\n\n\n\n\n\n\n\n\n\nI also recorded the nugget parameter estimates for each point in each iteration. I had hoped that this could explain some of the variation we see in fitted function uncertainty. The solid line and the grey band represent the mean of the nugget and the \\(95\\%\\) credible band on the nugget across all values in the test set respectively.\n\n\n\n\n\n\n\n\n\nThe nugget likely has little effect on fitted function uncertainty as it is both small and uniform. A more likely candidate is unlucky cluster assignments creating clusters that cannot be fit with a high degree of certainty. For example, the plot below displays our data set colored by the cluster assignments of the 200th iteration. Note that the eighth cluster, the purple dots, contains several points in the upper left of the plot and one in the dip around \\(x=3\\) and another in the peak around \\(x=4\\). These points likely cannot be estimated with confidence given the other points in the cluster and their estimates could be significantly inaccurate and distort the tails of the overall function estimate.\n\n\n\n\n\n\n\n\n\n\n\nSingle GP Comparison\nFor comparison, I fit a single Gaussian process model to the same data set and plotted its predictive mean and uncertainty on the test data. I found that a single GP model arguably does a better job of matching the estimated function (in red) to the true function (in blue). However, its uncertainty in that estimate is much wider than in the iMGPE model almost everywhere. As before, the red and blue lines represent the estimated and true functions, the grey band around them represents the \\(95\\%\\) credible interval in the estimated function, and the grey band at the bottom displays the width of the credible band across \\(X\\)."
  },
  {
    "objectID": "index.html#may-15-2025",
    "href": "index.html#may-15-2025",
    "title": "Weekly Reports",
    "section": "May 15, 2025",
    "text": "May 15, 2025\n\nSummary\nThis week I refined my explanation of the data model to be more coherent and readable. I then wrote out the full update step for the MCMC algorithm, detailing how each component of the model is updated.\nLastly, I developed a method to estimate values on a test set to replace my old practice of fitting a credible interval around each data point. The new method produces a much smoother and narrower credible band for the estimated function on both the simulated data set and the motorcycle data set.\n\n\nExplanation of Data Model\nWe have an \\(n\\times 1\\) continuous response vector \\(y\\) and an \\(n\\times d\\) data matrix \\(X\\). The estimated value of a data point \\(y_i\\) under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing \\(y_i\\) and weighted by a Dirichlet process. Let \\(z\\) represent a possible vector of cluster assignments and let \\(z^{(k)}\\) be the \\(k^{th}\\) element of some ordered list of all possible \\(z\\). Then \\(j=1,\\dots,J_k\\) index the clusters within \\(z^{(k)}\\). Let \\(C_j^{(k)}\\) be the number of observations in cluster \\(j\\) given assignment \\(z^{(k)}\\). Then we have as follows.\n\\[y\\sim \\sum_{k=1}^{p(n)} \\left[\\prod_{j=1}^{J_k} N_{C_j^{(k)}}(0,\\Sigma_{\\theta_j}) \\right] w_k\\] \\[w_k=P(z=z^{(k)}|\\alpha,\\phi,\\dots)\\] where the \\(w_z\\) are the marginal probabilities of a Chinese Restaurant Process that generates cluster assignments \\(z\\) and \\(p(n)\\) is the partition function. The CRP used here has been modified to depend on two parameters, \\(\\alpha\\) and \\(\\phi\\), the first being the usual concentration parameter and the second controlling the cluster occupancy estimates. A more in-depth explanation of this CRP will be provided shortly.\nAlternatively, the model can be expressed in hierarchical terms where \\(J_z\\) is the number of clusters in \\(z\\) and \\(C_{j,z}\\) is the number of observations in the \\(j^{th}\\) cluster in \\(z\\).\n\\[y|z\\sim \\prod_{j=1}^{J_z} N_{C_{j,z}}(0,\\Sigma_{\\theta_{j}})\\] \\[z|\\alpha,\\phi \\sim CRP(\\alpha,\\phi)\\]\nHere, \\(N_{C_j^{(z)}}(0,\\Sigma_{\\theta_j})\\) is the \\(C_j^{(z)}\\)-dimensional multivariate normal distribution with covariance matrix \\(\\Sigma_{\\theta_j}\\) defined by a Gaussian kernel function with parameters \\(\\theta_j\\). Similarly, \\(Gamma^d(a,b)\\) is the joint prior over the GP parameters and is the product of \\(d\\) Gamma distributions.\nThen \\(\\theta_j\\) is the parameter vector for the GP expert assigned to cluster \\(j\\), while \\(\\alpha\\) is the CRP concentration parameter and \\(\\phi\\) is the parameter vector for the CRP’s occupation number estimate. Note that \\(\\phi\\) is purely a vector of lengthscales for a Gaussian kernel. The priors on \\(\\theta\\), \\(\\alpha\\), and \\(\\phi\\) are described below.\n\\[\\theta_{j,k}\\stackrel{ind}{\\sim} Gamma(a_k,b_k) \\text{ for } k=1,\\dots,d\\] \\[\\alpha\\sim Inv.Gam(1,1),\\text{   } \\phi_k\\stackrel{iid}{\\sim} LogN(0,1) \\text{ for } k=1,\\dots,d\\] That is, each element \\(k\\) of \\(\\theta_j\\) (the dimension of \\(X\\) plus a noise parameter) is assigned a independent Gamma prior with fixed parameters \\(a_k\\) and \\(b_k\\). Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of \\(\\phi\\) receives an independent log-normal prior.\nThe modified CRP used in this algorithm is defined by “R. M. Neal” (Algorithm 8 in that paper with \\(m=1\\)) and works as follows:\nWe represent the current cluster state with assignment labels \\(z=(z_1,\\dots,z_n)\\) and GP parameter vectors \\(\\theta_1,\\dots,\\theta_J\\) where \\(J\\) is the number of clusters in the current state. For \\(i=1,\\dots,n\\), repeat the following. Let \\(J^{-i}\\) be the number of clusters in \\(z\\) with point \\(i\\) removed. Let \\(\\theta_{J^{-i}+1}\\) be a parameter vector drawn from its prior distribution, in this case \\(Gamma^d(a,b)\\). Draw a new value for \\(z_i\\) with the following conditional probabilities:\n\\[P(z_i=j|z_{-i},y_i,\\dots)\\propto \\begin{cases}\n\\frac{n-1}{n+\\alpha-1}\\frac{\\sum_{i'\\neq i,z_{i'}=j} K_{\\phi}(X_i,X_{i'})}{\\sum_{i'\\neq i} K_{\\phi}(X_i,X_{i'})} f(y_i|\\theta_j) \\text{ for } j=1,\\dots,J^{-i}\\\\\n\\frac{\\alpha}{n+\\alpha-1}f(y_i|\\theta_{J^{-i}+1}) \\text{ for } j=J^{-i}+1\n\\end{cases}\\]\nwhere \\(f(y_i|\\theta_j)\\) is the normal density of \\(y_i\\) given the kriging equations with parameter vector \\(\\theta_j\\) defining the kernel function.\n\n\nThe iMGPE Algorithm\nI have implemented the Infinite Mixture of Gaussian Process Experts algorithm mostly as described by the authors Rasmussen and Ghahramani, though with a few alterations of my own which are noted below. This approach then iterates through the following MCMC algorithm.\n\nInitialize indicator variables \\(z\\) to a set of values. I generally start by assigning all points to a single cluster. Set gamma prior distributions on the lengthscale and nugget parameters of the GP experts, using the ‘darg’ and ‘garg’ functions of the package ‘laGP’. Set initial values for \\(\\alpha\\) and \\(\\phi\\).\nPerform a Gibbs sampling sweep over the cluster assignment indicators, using the Chinese Restaurant Process described in the model explanation, to generate a new cluster assignment vector \\(z\\).\nFit a Gaussian process expert to each cluster in \\(z\\) and get ML estimates of each expert’s parameters \\(\\theta_j\\).\nUse these GP experts to generate estimates on the test set. More detail on this step is included in the next section.\nSample the Dirichlet process concentration parameter, \\(\\alpha\\), using quantile slice sampling with a \\(Gamma(1,1)\\) proposal distribution. The posterior distribution of \\(\\alpha\\) we sample from is \\[p(\\alpha|n, J)\\propto \\alpha^{J-3/2}\\exp(-1/2\\alpha)\\Gamma(\\alpha)/\\Gamma(n+\\alpha)\\]\nSample the other CRP parameter \\(\\phi\\) via random walk Monte Carlo. The random walk step uses a normal proposal distribution centered at the current value and with variance equal to \\((2.38^2/d)H^{-1}\\), \\(d\\) being the number of inputs and \\(H\\) the Hessian matrix of the distribution of \\(\\phi\\). The posterior distribution of \\(\\phi\\) we are sampling from is \\[p(\\phi|z,\\alpha,\\dots)\\propto p(z|y,\\phi,\\alpha)p(\\phi)\\approx \\left[\\prod_{i=1}^n p(z_i|y,\\phi,\\alpha) \\right] p(\\phi)\\]\nRepeat from step 2 until the MCMC output has converged.\n\n\n\nExperiments on Simulated Data\nI set up a new method of sampling posterior predictive values on a test set and applied it to the simulated data set, using a test set of \\(500\\) evenly spaced points between \\(0\\) and \\(5\\). Every fifth iteration, I would estimate fitted values on the test set for each GP expert currently in use. Then, for each point in the test set, I would calculate the conditional probabilities of it belonging to each of the current clusters. Lastly, I found the sum of the GP estimates for each point weighted by the probabilities of their respective clusters and record the resulting set of values for that iteration.\nThe resultant credible band for the simulated function is graphed below, based on \\(1000\\) draws from the predictive posterior. The blue line is the true function path and the red line is the mean of the fitted values. The upper grey band shows the \\(95\\%\\) credible band over the test set while the lower grey band is a visual aid displaying the width of the credible band with its lower bound fixed at \\(-2\\). Uncertainty is greatest at the edges of the training data and around the rightmost peak, where there are few points. This is as we would expect.\n\n\n\n\n\n\n\n\n\n\n\nExperiments on Motorcycle Data\nI tested this posterior prediction method on the motorcycle data set, using a test set of \\(561\\) evenly spaced points between \\(2\\) and \\(58\\). I have made some changes to the body of the algorithm since I last tested on the motorcycle data, so the trace plots for \\(\\phi\\) and \\(\\alpha\\) are also included. Both parameters have converged, and the algorithm heavily favors dividing the training data into two clusters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI obtained \\(1000\\) draws from the predictive posterior distribution as described in the previous section and have plotted their mean and \\(95\\%\\) credible band below. As before, the grey band at the bottom of the graph is a visual aid displaying the width of the credible interval across time."
  },
  {
    "objectID": "index.html#may-8-2025",
    "href": "index.html#may-8-2025",
    "title": "Weekly Reports",
    "section": "May 8, 2025",
    "text": "May 8, 2025\n\nSummary\nThis week I updated my marginal model notation to be more readable and included a brief description of the hierarchical model. I also found and fixed the bug in my code that was causing unusually wide credible intervals.\nI am working on converting the algorithm fully to Rcpp and implementing support for multivariate and categorical \\(X\\) inputs. Now that the code structure of the algorithm has been developed, this should proceed quickly.\n\n\nExplanation of Data Model\nWe have an \\(n\\times 1\\) continuous response vector \\(y\\) and an \\(n\\times d\\) data matrix \\(X\\). The estimated value of a data point \\(y_i\\) under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing \\(y_i\\) and weighted by a Dirichlet process. Let \\(z\\) represent a possible vector of cluster assignments and let \\(z^{(k)}\\) be the \\(k^{th}\\) element of some ordered list of all possible \\(z\\). Then \\(j=1,\\dots,J_k\\) index the clusters within \\(z^{(k)}\\). Let \\(C_j^{(k)}\\) be the number of observations in cluster \\(j\\) given assignment \\(z(k)\\). Then we have as follows.\n\\[y\\sim \\sum_{k=1}^{p(n)} \\left[\\prod_{j=1}^{J_k} N_{C_j^{(k)}}(0,\\Sigma_{\\theta_j}) \\right] w_k\\] \\[w_k=P(z=z^{(k)}|\\alpha,\\phi,\\dots)\\] where the \\(w_z\\) are the marginal probabilities of a Chinese Restaurant Process that generates cluster assignments \\(z\\) and \\(p(n)\\) is the partition function. Alternatively, the model can be expressed in hierarchical terms as\n\\[y|z\\sim \\prod_{j=1}^J N_{C_j}(0,\\Sigma_{\\theta_j})\\] \\[z|\\alpha,\\dots \\sim CRP(\\alpha,\\phi)\\]\nThe specific CRP is defined by “R. M. Neal” (Algorithm 8 in that paper with \\(m=1\\)) and works as follows:\nWe represent the current cluster state with assignment labels \\(z=(z_1,\\dots,z_n)\\) and GP parameter vectors \\(\\theta_1,\\dots,\\theta_J\\) where \\(J\\) is the number of clusters in the current state. For \\(i=1,\\dots,n\\), repeat the following. Let \\(J^{-i}\\) be the number of clusters in \\(z\\) with point \\(i\\) removed. Let \\(\\theta_{J^{-i}+1}\\) be a parameter vector drawn from its prior distribution, in this case \\(Gamma^d(a,b)\\). Draw a new value for \\(z_i\\) with the following conditional probabilities:\n\\[P(z_i=j|z_{-i},y_i,\\dots)\\propto \\begin{cases}\n\\frac{n-1}{n+\\alpha-1}\\frac{\\sum_{i'\\neq i,z_{i'}=j} K_{\\phi}(X_i,X_{i'})}{\\sum_{i'\\neq i} K_{\\phi}(X_i,X_{i'})} f(y_i|\\theta_j) \\text{ for } j=1,\\dots,J^{-i}\\\\\n\\frac{\\alpha}{n+\\alpha-1}f(y_i|\\theta_{J^{-i}+1}) \\text{ for } j=J^{-i}+1\n\\end{cases}\\]\nwhere \\(f(y_i|\\theta_j)\\) is the normal density of \\(y_i\\) given the kriging equations with parameter vector \\(\\theta_j\\) defining the kernel function. The kriging equations are explained in the posterior predictive distribution section.\nMeanwhile, \\(N_{C_j^{(z)}}(0,\\Sigma_{\\theta_j})\\) is the \\(C_j^{(z)}\\)-dimensional multivariate normal distribution with covariance matrix \\(\\Sigma_{\\theta_j}\\) defined by a Gaussian kernel function with parameters \\(\\theta_j\\). Similarly, \\(Gamma^d(a,b)\\) is the joint prior over the GP parameters and is the product of \\(d\\) Gamma distributions.\nAbove, \\(\\theta_j\\) is the parameter vector for the GP expert assigned to cluster \\(j\\), while \\(\\alpha\\) is the DP concentration parameter and \\(\\phi\\) is the parameter vector for the DP’s occupation number estimate. Note that \\(\\phi\\) is purely a vector of lengthscales for a Gaussian kernel. The priors on \\(\\theta\\), \\(\\alpha\\), and \\(\\phi\\) are described below.\n\\[\\theta_{j,k}\\stackrel{ind}{\\sim} Gamma(a_k,b_k) \\text{ for } k=1,\\dots,d\\] \\[\\alpha\\sim Inv.Gam(1,1),\\text{   } \\phi_k\\stackrel{iid}{\\sim} LogN(0,1) \\text{ for } k=1,\\dots,d\\] That is, each element \\(k\\) of \\(\\theta_j\\) (the dimension of \\(X\\) plus a noise parameter) is assigned a independent Gamma prior with fixed parameters \\(a_k\\) and \\(b_k\\). Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of \\(\\phi\\) receives an independent log-normal prior.\n\n\nCredible interval investigation\nUp to now, the posterior distributions of the fitted values have been highly skewed by outliers, leading to unusually large credible intervals. After investigation, I determined that this was due to an error in the posterior sampling code.\nEvery so often, the algorithm will generate a cluster with only one data point in it and attempt to fit a GP to that cluster. I learned early on to wrap the model code in a try-catch loop in case the GP function failed. My sampler from the \\(y\\) posterior assumed that any cluster of size 1 could not fit a GP, but this was not always true! In those cases, the sampler used the wrong GP, resulting in inaccurate estimates that defaulted towards the GP mean of zero.\nAfter correcting this, I tested my code on the simulated dataset and found that the credible band was dramatically narrower.\nFor every fifth iteration, I took the fitted values of the response at each point in time, resulting in a sample of 1000 fitted values for each point. I took the 2.5th and 97.5th quantiles of this sample to get a credible interval for the fitted value at each point. Below, I have plotted the true data as black dots, the mean fitted values as a red line, and the \\(95\\%\\) credible intervals as the gray ribbon."
  },
  {
    "objectID": "index.html#may-1-2025",
    "href": "index.html#may-1-2025",
    "title": "Weekly Reports",
    "section": "May 1, 2025",
    "text": "May 1, 2025\n\nSummary\nThis week I updated my explanation of the data model and added a description of the posterior predictive distribution. I also revised my choice of standard deviation for the proposal distribution of \\(\\phi\\)’s random walk sampler, bringing its acceptance rate back up to acceptable levels.\n\n\nMarginal Data Model\nWe have an \\(n\\times 1\\) continuous response vector \\(y\\) and an \\(n\\times d\\) data matrix \\(X\\). The estimated value of a data point \\(y_i\\) under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing \\(y_i\\) and weighted by a Dirichlet process. Let \\(z\\) represent a possible vector of cluster assignments and let \\(z(k)\\) be the \\(k^{th}\\) element of some ordered list of all possible \\(z\\). Then \\(j=1,\\dots,J(k)\\) index the clusters within \\(z(k)\\). Let \\(C_j^{(z(k))}\\) be the number of observations in cluster \\(j\\) given assignment \\(z(k)\\). Then we have as follows.\n\\[y\\sim \\sum_{k=1}^{p(n)} \\left[\\prod_{j=1}^{J(k)} N_{C_j^{(z(k))}}(0,\\Sigma_{\\theta_j}) \\right] w_{z(k)}\\] \\[w_Z=P(z=Z|\\alpha,\\phi)\\] where the \\(w_z\\) are the marginal probabilities of a Chinese Restaurant Process that generates cluster assignments \\(z\\) and \\(p(n)\\) is the partition function. The specific CRP is defined by “R. M. Neal” (Algorithm 8 in that paper with \\(m=1\\)) and works as follows:\nWe represent the current cluster state with assignment labels \\(z=(z_1,\\dots,z_n)\\) and GP parameter vectors \\(\\theta_1,\\dots,\\theta_J\\) where \\(J\\) is the number of clusters in the current state. For \\(i=1,\\dots,n\\), repeat the following. Let \\(J^{-}\\) be the number of clusters in \\(z\\) with point \\(i\\) removed. Let \\(\\theta_{J^{-}+1}\\) be a parameter vector drawn from its prior distribution, in this case \\(Gamma^d(a,b)\\). Draw a new value for \\(z_i\\) with the following conditional probabilities:\n\\[P(z_i=j|z_{-i},y_i,\\theta)=\\begin{cases}\nb\\frac{n-1}{n+\\alpha-1}\\frac{\\sum_{i'\\neq i,z_{i'}=j} K_{\\phi}(X_i,X_{i'})}{\\sum_{i'\\neq i} K_{\\phi}(X_i,X_{i'})} F(y_i|\\theta_j) \\text{ for } j=1,\\dots,J^{-}\\\\\nb\\frac{\\alpha}{n+\\alpha-1}F(y_i|\\theta_{J^{-}+1}) \\text{ for } j=J^{-}+1\n\\end{cases}\\] where \\(b\\) is a normalizing constant. Note that \\(b\\) need not be calculated; we can just calculate the un-normalized probability vector and then normalize it.\nMeanwhile, \\(N_{C_j^{(z)}}(0,\\Sigma_{\\theta_j})\\) is the \\(C_j^{(z)}\\)-dimensional multivariate normal distribution with covariance matrix \\(\\Sigma_{\\theta_j}\\) defined by a Gaussian kernel function with parameters \\(\\theta_j\\). Similarly, \\(Gamma^d(a,b)\\) is the joint prior over the GP parameters and is the product of \\(d\\) Gamma distributions.\nAbove, \\(\\theta_j\\) is the parameter vector for the GP expert assigned to cluster \\(j\\), while \\(\\alpha\\) is the DP concentration parameter and \\(\\phi\\) is the parameter vector for the DP’s occupation number estimate. Note that \\(\\phi\\) is purely a vector of lengthscales for a Gaussian kernel. The priors on \\(\\theta\\), \\(\\alpha\\), and \\(\\phi\\) are described below.\n\\[\\theta_{j_k}\\stackrel{ind}{\\sim} Gamma(a_k,b_k) \\text{ for } k=1,\\dots,d\\] \\[\\alpha\\sim Inv.Gam(1,1),\\text{   } \\phi_k\\stackrel{iid}{\\sim} LogN(0,1) \\text{ for } k=1,\\dots,d\\] That is, each element \\(k\\) of \\(\\theta_j\\) (the dimension of \\(X\\) plus a noise parameter) is assigned a independent Gamma prior with fixed parameters \\(a_k\\) and \\(b_k\\). Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of \\(\\phi\\) receives an independent log-normal prior.\n\n\nPosterior Predictive Distribution\nThe paper by Rasmussen and Ghahramani does not discuss what a posterior predictive distribution would look like. However, it can be deduced based on the posterior predictive distribution of a lone Gaussian process. With \\(p(n)\\) and \\(w_{z(k)}\\) defined the same as in the original data model, we determine that the distribution of a new point \\(y^*\\) with data \\(x^*\\) is as follows.\n\\[y^*\\sim \\sum_{k=1}^{p(n)} \\left[\\prod_{j=1}^{J_k} N(\\mu_j^*,\\sigma_j^*) \\right] w_k\\] where \\(\\mu_j^*\\) and \\(\\Sigma_j^*\\) are found according to the kriging equations of a Gaussian process. That is,\n\\[\\mu_j^*=K(x^*,X^{(j)})^TK^{-1}y^{(j)}\\text{ and } \\sigma_j^*=K(x^*,x^*)-K(x^*,X^{(j)})^T K^{-1}K(x^*,X^{(j)})\\] where \\(K\\) is the covariance matrix based on parameters \\(\\theta_j\\) and \\(X^{(j)}\\subset X\\) and \\(y^{(j)} \\subset y\\) are the data and response values associated with cluster \\(j\\).\n\n\nSimulated Example\nWe previously had a very small acceptance rate in the random walk sampler for \\(\\phi\\), which seems to have been due to tiny Hessian values that result in a giant standard deviation in the (Normal) proposal distribution. Consequently, most proposals were far too large to be accepted. My current solution was to put a cap on the proposal standard deviation equal to the range of \\(x\\). As \\(\\phi\\) represents the rate of decay in correlation between neighboring points, values greater than the range of the data would be largely indistinguishable anyways. In result, \\(\\phi\\) seems well mixed and has an acceptance rate of \\(69.7\\%\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor every fifth iteration, I took the fitted values of the response at each point in time, resulting in a sample of 1000 fitted values for each point. I took the 2.5th and 97.5th quantiles of this sample to get a credible interval for the fitted value at each point. Below, I have plotted the true data as black dots, the mean fitted values as a red line, and the \\(95\\%\\) credible intervals as the gray ribbon. As the generating function was known, that too is plotted as a blue line.\n\n\n\n\n\n\n\n\n\nThe fitted mean line still does a good job of approximating the true function."
  },
  {
    "objectID": "index.html#april-24-2025",
    "href": "index.html#april-24-2025",
    "title": "Weekly Reports",
    "section": "April 24, 2025",
    "text": "April 24, 2025\n\nSummary\nThis week, I further refined my explanation of the data model, particularly the base distribution of the Dirichlet process. I studied the cluster assignment update algorithm described by Rasmussen and “R. M. Neal”, which clarified that the base distribution is used to generate the parameters controlling \\(y_i\\) given a new cluster. Based on this, I can identify the base distribution to be the prior on the Gaussian process parameters: a product of independent Gamma priors.\nWhile reviewing Neal’s explanation, I identified an error in my implementation of the cluster update step, stemming from how the ‘garg’ function in laGP may select \\((0,0)\\) for the parameters of the Gamma prior on the GP nugget. While laGP considers this to be “no prior” on the nugget, other parts of my algorithm that drew on this prior did not. Correcting this had a noticeable impact on the number of clusters generated.\nLastly, I simulated a new data set of \\(100\\) points using a custom mean function and tested the iMGPE algorithm on it. This allows me to examine how well my algorithm approximates the true mean function.\n\n\nData Model\nWe have an \\(n\\times 1\\) continuous response vector \\(y\\) and an \\(n\\times d\\) data matrix \\(X\\). The estimated value of a data point \\(y_i\\) under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing \\(y_i\\) and weighted by a Dirichlet process. Let \\(z\\) represent a possible vector of cluster assignments and \\(j=1,\\dots,|z|\\) index the clusters within \\(z\\), where \\(|z|\\) is the number of clusters. Let \\(C_j^{(z)}\\) be the number of observations in cluster \\(j\\) given assignment \\(z\\). Then we have as follows.\n\\[y|X\\sim \\sum_z \\left[\\prod_{j=1}^{|z|} N_{C_j^{(z)}}(0,\\Sigma_{\\theta_j}) \\right] w_z\\] \\[w\\sim DP_{\\phi}(\\alpha,  Gamma^d(a,b))\\] where \\(w\\) is a vector of probabilities defining a multinomial distribution drawn from a Dirichlet process and \\(w_z\\) is the probability of observing cluster assignment \\(z\\). That is, \\(P(z=Z|\\alpha,\\phi)=w_Z\\) and so \\(z|\\alpha,\\phi\\) is generated by a Chinese Restaurant process based on a \\(DP_{\\phi}(\\alpha, Gamma^d(a,b))\\) distribution.\nMeanwhile, \\(N_{C_j^{(z)}}(0,\\Sigma_{\\theta_j})\\) is the \\(C_j^{(z)}\\)-dimensional multivariate normal distribution with covariance matrix \\(\\Sigma_{\\theta_j}\\) defined by a Gaussian kernel function with parameters \\(\\theta_j\\). Similarly, \\(Gamma^d(a,b)\\) is the joint prior over the GP parameters and is the product of \\(d\\) Gamma distributions.\nAbove, \\(\\theta_j\\) is the parameter vector for the GP expert assigned to cluster \\(j\\), while \\(\\alpha\\) is the DP concentration parameter and \\(\\phi\\) is the parameter vector for the DP’s occupation number estimate. Note that \\(\\phi\\) is purely a vector of lengthscales for a Gaussian kernel. The priors on \\(\\theta\\), \\(\\alpha\\), and \\(\\phi\\) are described below.\n\\[\\theta_{j_k}\\stackrel{ind}{\\sim} Gamma(a_k,b_k) \\text{ for } k=1,\\dots,d\\] \\[\\alpha\\sim Inv.Gam(1,1),\\text{   } \\phi_k\\stackrel{iid}{\\sim} LogN(0,1) \\text{ for } k=1,\\dots,d\\] That is, each element \\(k\\) of \\(\\theta_j\\) (the dimension of \\(X\\) plus a noise parameter) is assigned a independent Gamma prior with fixed parameters \\(a_k\\) and \\(b_k\\). Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of \\(\\phi\\) receives an independent log-normal prior.\n\n\nDescription of Simulated Data\nThis week, I simulated a sample data set for use with the iMGPE algorithm. Whereas with the motorcycle data set we lack knowledge of the true generating function, the simulated data set is generated by a known custom function. Assuming that \\(y=f(X)+\\epsilon(X)\\), for the simulated data set we can compare the fitted estimate of \\(f(X)\\) with the truth. The generating function is \\[f(x)= \\begin{cases}\n\\frac{7}{3}-\\frac{2}{3}x\\text{ if } x\\leq 2\\\\\n\\cos(\\pi x)\\text{ if } x&gt;2\n\\end{cases}\\]\nI drew a random uniform sample of \\(x\\) of size \\(100\\) from the interval \\((0,5)\\) and added iid random noise \\(\\epsilon\\) drawn from a \\(N(0,0.04)\\) distribution. With \\(y=f(X)+\\epsilon\\), the simulated data set and the function \\(f\\) are plotted below.\n\n\n\n\n\n\n\n\n\n\n\nApplication of iMGPE\nI was using the ‘darg’ and ‘garg’ functions from the ‘laGP’ R package to select priors for the GP lengthscale and nugget parameters. However, the ‘garg’ function was setting a \\(Gamma(0,0)\\) prior on the nugget. For the purposes of fitting a GP, this is treated as no prior on the nugget, but I was drawing values from these priors elsewhere in the algorithm and ‘rgamma(n,0,0)’ always returns \\(0\\). Once I corrected this by specifying a vague prior for the nugget should ‘garg’ fail, the algorithm’s behavior changed significantly with respect to the value of \\(\\alpha\\) and the number of clusters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor every fifth iteration, I took the fitted values of the response at each point in time, resulting in a sample of 1000 fitted values for each point. I took the 2.5th and 97.5th quantiles of this sample to get a credible interval for the fitted value at each point. Below, I have plotted the true data as black dots, the mean fitted values as a red line, and the \\(95\\%\\) credible intervals as the gray ribbon. As the generating function was known, that too is plotted as a blue line.\n\n\n\n\n\n\n\n\n\nThe fitted mean line is following the true function fairly closely, though the credible bounds are still wider than seems reasonable."
  },
  {
    "objectID": "index.html#april-17-2025",
    "href": "index.html#april-17-2025",
    "title": "Weekly Reports",
    "section": "April 17, 2025",
    "text": "April 17, 2025\n\nSummary\nThis week I improved my breakdown of the data model and identified the base distribution of the Dirichlet process, which I believe to be a multivariate normal distribution. I detailed the derivation of the pseudo-posterior density for \\(\\phi\\) and explain that the full conditional density is never directly defined and thus not trivial to derive.\nI also determined why my trace plot for \\(\\alpha\\) seemed to hit a ‘ceiling’, which was that the proposal distribution for my quantile slice sampler was misspecified. I substituted a proposal with heavier tails and the apparent ceiling disappeared. However, this had significant repercussions on my parameter values. Now, \\(\\alpha\\) took on much larger average values, and the number of clusters increased dramatically. This resulted in many more experts fit to only a handful of data points, which had negative repercussions for the accuracy of the fitted model.\n\n\nData Model\nWe have an \\(n\\times 1\\) continuous response vector \\(y\\) and an \\(n\\times d\\) data matrix \\(X\\). The estimated value of a data point \\(y_i\\) under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing \\(y_i\\) and weighted by a Dirichlet process. Let \\(z\\) represent a possible vector of cluster assignments and \\(j\\) index the clusters within \\(z\\). Let \\(C_j\\) be the number of observations in cluster \\(j\\). Then we have as follows.\n\\[y\\sim \\sum_z \\left[\\prod_j N_{C_j}(0,\\Sigma_{\\theta_j}) \\right] w_z\\] \\[w\\sim DP_{\\phi}(\\alpha, N_n(0,I_n))\\] where \\(w\\) is a vector of probabilities defining a multinomial distribution drawn from a Dirichlet process and \\(w_z\\) is the probability of observing cluster assignment \\(z\\). That is, \\(P(z=Z|\\alpha,\\phi)=w_Z\\) and so \\(z|\\alpha,\\phi\\) is generated by a \\(DP_{\\phi}(\\alpha, N_n(0,I_n))\\) clustering process. Meanwhile, \\(N_{C_j}(0,\\Sigma_{\\theta_j})\\) is the \\(C_j\\)-dimensional multivariate normal distribution with covariance matrix \\(\\Sigma_{\\theta_j}\\) defined by a Gaussian kernel function with parameters \\(\\theta_j\\). Similarly, \\(N_n(0,I_n)\\) is the \\(n\\)-dimensional normal distribution where \\(I_n\\) is the \\(n\\)-dimensional identity matrix.\nAbove, \\(\\theta_j\\) is the parameter vector for the GP expert assigned to cluster \\(j\\), while \\(\\alpha\\) is the DP concentration parameter and \\(\\phi\\) is the parameter vector for the DP’s occupation number estimate. Note that \\(\\phi\\) is purely a vector of lengthscales for a Gaussian kernel. The priors on \\(\\theta\\), \\(\\alpha\\), and \\(\\phi\\) are described below.\n\\[\\theta_{j_k}\\stackrel{ind}{\\sim} Gamma(a_k,b_k)\\] \\[\\alpha\\sim Inv.Gam(1,1),\\text{   } \\phi_k\\stackrel{iid}{\\sim} LogN(0,1)\\] That is, each element \\(k\\) of \\(\\theta_j\\) (the dimension of \\(X\\) plus a noise parameter) is assigned a independent Gamma prior with fixed parameters \\(a_k\\) and \\(b_k\\). Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of \\(\\phi\\) receives an independent log-normal prior.\n\n\nHandling Alpha and Phi\nLast week, the trace plot of \\(\\alpha\\) appeared to hit a ‘ceiling’ at around \\(3.67\\). After some experimentation, I determined that my quantile slice sampling function was numerically unstable above that ceiling. The reason was that the quantile slice sampler relies on a proposal distribution and my initial choice of \\(Gamma(1,10)\\) was too light-tailed. I chose a more reasonable proposal distribution of \\(Gamma(1,1)\\) and the instability disappeared.\nWith regard to \\(\\phi\\), we wanted to know its true full conditional distribution. The conditional pseudo-posterior distribution for \\(\\phi\\) that we draw from in its Gibbs sampling step is shown on the right side of the equation below. We use it for now because \\(p(z|y,\\phi,\\alpha)\\) is not defined directly; only the conditionals \\(p(z_i|y,\\phi,\\alpha)\\) are defined. As before, the prior \\(p(\\phi)\\) is a log-normal density with parameters \\(\\mu=0\\) and \\(\\sigma^2=1\\).\n\\[p(\\phi|z,\\alpha,\\dots)\\propto p(z|y,\\phi,\\alpha)p(\\phi)\\approx \\left[\\prod_{i=1}^n p(z_i|y,\\phi,\\alpha) \\right] p(\\phi)\\] The individual conditionals are as follows, taking \\(j\\) to be the current value of \\(z_i\\) and \\(y^{(j)}\\) to be the subset of \\(y\\) belonging to cluster \\(j\\).\n\\[p(z_i=j|y,z_{-i},\\dots) \\propto p(y^{(j)}|z_i=j,z_{-i},\\theta_j)p(z_i=j|\\phi,\\alpha)\\] \\[\\text{where } p(y^{(j)}|z_i=j,z_{-i},\\theta_j)=N_{C_j}(0,\\Sigma_{\\theta_j}) \\text{ and}\\] \\[p(z_i=j|z_{-i},\\phi,\\alpha)=\\frac{n-1}{n-1+\\alpha}\\frac{\\sum_{i'\\neq i,z_{i'}=j} K_{\\phi}(X_i,X_{i'})}{\\sum_{i'\\neq i} K_{\\phi}(X_i,X_{i'})}\\]\nSince \\(p(y^{(j)}|\\cdot)\\) does not depend on \\(\\phi\\), it factors out of the posterior density and we are left with \\(p(\\phi|z,\\alpha,\\dots)\\approx \\left[\\prod_{i=1}^n p(z_i|y,\\phi,\\alpha) \\right] p(\\phi)\\).\n\n\nPractical Experiment\nI set the proposal distribution for \\(\\alpha\\) to \\(Gamma(1,1)\\) and ran the algorithm on the motorcycle dataset again. The concentration parameter \\(\\alpha\\) is now centered around \\(15\\) rather than \\(3\\), with a commensurate effect on the number of clusters, which now averages around \\(30\\). Though \\(\\alpha\\)’s trace plot now appears properly mixed, I am not sure I like the effect on the cluster output. With so many clusters and only \\(94\\) data points, most clusters now contain only \\(2\\) or \\(3\\) data points.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor every fifth iteration, I took the fitted values of the response at each point in time, resulting in a sample of 1000 fitted values for each point. I took the 2.5th and 97.5th quantiles of this sample to get a credible interval for the fitted value at each point. Below, I have plotted the true data as black dots, the mean fitted values as a red line, and the \\(95\\%\\) credible intervals as the gray ribbon.\n\n\n\n\n\n\n\n\n\nThe fitted mean line is not following the data very well, and the credible band is much wider than before. I suspect this loss of precision is due to the excessive number of experts fit to little data."
  },
  {
    "objectID": "index.html#april-10-2025",
    "href": "index.html#april-10-2025",
    "title": "Weekly Reports",
    "section": "April 10, 2025",
    "text": "April 10, 2025\n\nSummary\nThis week, I expanded upon my description of the data model. I also corrected some numerical stability issues in the \\(\\phi\\) update step that resulted in significant improvements in its acceptance rate. Lastly, I plotted a heatmap of cluster memberships that displayed which points were most often clustered together.\n\n\nFull Data Model\nThe estimated value of a data point \\(y_i\\) under the iMGPE model is the sum of the fitted estimates for that data point from all possible GP experts fit to subsets of the data containing \\(y_i\\) and weighted by a Dirichlet process. Letting \\(\\mathbf{z}\\) represent a possible vector of cluster assignments and \\(j\\) index the clusters within \\(\\mathbf{z}\\), we have as follows.\n\\[\\mathbf{y}\\sim \\sum_{\\mathbf{z}} \\left[\\prod_j GP_j(\\theta_j) \\right] w_{\\mathbf{z}}\\] \\[\\mathbf{w}\\sim DP(\\alpha, GP(\\phi))\\] where \\(\\mathbf{w}\\) is a vector of probabilities defining a multinomial distribution drawn from a Dirichlet process and \\(w_{\\mathbf{z}}\\) is the probability of observing cluster assignment \\(\\mathbf{z}\\). That is, \\(P(\\mathbf{z}=Z|\\alpha,\\phi)=w_Z\\) and so \\(\\mathbf{z}|\\alpha,\\phi \\sim DP(\\alpha, GP(\\phi))\\).\nAbove, \\(\\theta_j\\) is the parameter vector for the GP expert assigned to cluster \\(j\\), while \\(\\alpha\\) is the DP concentration parameter and \\(\\phi\\) is the parameter vector for the DP’s base distribution. Note that \\(\\phi\\) is purely a vector of lengthscales; the nugget is assumed to be zero. The priors on \\(\\theta\\), \\(\\alpha\\), and \\(\\phi\\) are described below.\n\\[\\theta_{j_k}\\stackrel{ind}{\\sim} Gamma(a_k,b_k)\\] \\[\\alpha\\sim Inv.Gam(1,1),\\text{   } \\phi_k\\stackrel{iid}{\\sim} LogN(0,1)\\] That is, each element \\(k\\) of \\(\\theta_j\\) (the dimension of \\(X\\) plus a noise parameter) is assigned a independent Gamma prior with fixed parameters \\(a_k\\) and \\(b_k\\). Thus, every GP expert has the same prior on its noise parameter and so on. Similarly, each element of \\(\\phi\\) receives an independent log-normal prior.\n\n\nDistribution Functions\nThe iMGPE algorithm is a Gibbs-sampling method, which means we must draw a new value of \\(\\phi\\) and \\(\\alpha\\) from their conditional distributions each iteration. The conditional distribution of \\(\\alpha\\) only depends on the number of data points \\(N\\) and the number of clusters \\(J\\), as seen here.\n\\[p(\\alpha|n, J,\\dots)\\propto \\alpha^{J-3/2}\\exp(-1/2\\alpha)\\Gamma(\\alpha)/\\Gamma(n+\\alpha)\\]\nThe conditional distribution for \\(\\phi\\) is more difficult to obtain, as the Dirichlet process is only defined through the following conditional probabilities.\n\\[p(z_i=j|z_{-i},\\phi,\\alpha)=\\frac{n-1}{n-1+\\alpha}\\frac{\\sum_{i'\\neq i,z_{i'}=j} K_{\\phi}(X_i,X_{i'})}{\\sum_{i'\\neq i} K_{\\phi}(X_i,X_{i'})}\\] \\[p(z_i\\neq z_{i'}\\text{ for all }i'\\neq i|z_{-i},\\alpha) =\\frac{\\alpha}{n-1+\\alpha} \\]\nTherefore, Rasmussen et al chose to sample from the pseudo-posterior of \\(\\phi\\), defined as the leave-one-out pseudo-likelihood (the product of the conditional probabilities of the \\(z_i\\)) and the prior on \\(\\phi\\). That is,\n\\[p^*(\\phi|\\mathbf{z},\\alpha)=\\prod_{i=1}^n p(z_i=j|\\phi,\\alpha)\\times p(\\phi)\\]\n\n\nPractical Experiments\nLast week, I had difficulties with a low acceptance rate in \\(\\phi\\)’s random walk sampler. After examining my code, I determined that the cause was numerical instability in my R function for calculating \\(\\log(p^*(\\phi|\\cdot))\\). Once I revised my posterior density function to replace values of ‘-Inf’ with \\(-1000\\), the sampler achieved an acceptance rate of \\(34\\%\\).\nWith these revisions, I reran the motorcycle example from before.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe trace plots for both \\(\\alpha\\) and \\(\\phi\\) now appear to be well mixed, with \\(\\phi\\) achieving an acceptance rate of \\(34\\%\\). As before, for every fifth iteration, I took the fitted values of the response at each point in time, resulting in a sample of 1000 fitted values for each point. I took the 2.5th and 97.5th quantiles of this sample to get a credible interval for the fitted value at each point. Below, I have plotted the true data as black dots, the mean predicted values as a red line, and the \\(95\\%\\) credible intervals as the gray ribbon.\n\n\n\n\n\n\n\n\n\nThe credible interval is still unusually wide, especially around the ‘dip’ at time 20.\nThis week I also prepared a visual to display ‘standard cluster memberships’. Below is shown a heatmap displaying the frequency of each pair of data points belonging to the same cluster. Points are ordered by time, so they appear in the same order from left to right as in the plot above. There are two distinct clusters near the beginning."
  },
  {
    "objectID": "index.html#summary-32",
    "href": "index.html#summary-32",
    "title": "Weekly Reports",
    "section": "Summary",
    "text": "Summary\nThis week I researched current ways of using functional inputs for clustering. There are two basic strategies I’ve found: to use a distance metric between two functions, or to convert the functions into linear combinations of basis functions and use the covariates for clustering.\nI also prepared a new method of fitting a GP with a categorical input by first fitting a linear model to the categorical input to simulate a mean function in the GP. I then compared all these methods on a new dataset that had two factor levels similar and the third different. The ranking of the different methods didn’t change, though the new mean function method performed best. I also plotted \\(95\\%\\) confidence bands of the estimated functions."
  },
  {
    "objectID": "imgpe.html#posterior-predictive-distribution",
    "href": "imgpe.html#posterior-predictive-distribution",
    "title": "The iMGPE Algorithm",
    "section": "Posterior Predictive Distribution",
    "text": "Posterior Predictive Distribution\nThe paper by Rasmussen and Ghahramani does not discuss what a posterior predictive distribution would look like. However, it can be deduced based on the posterior predictive distribution of a lone Gaussian process. With \\(p(n)\\) and \\(w_{z(k)}\\) defined the same as in the original data model, we determine that the distribution of a new point \\(y^*\\) with data \\(x^*\\) is as follows.\n\\[y^*\\sim \\sum_{k=1}^{p(n)} \\left[\\sum_{j=1}^{J_k} N(\\mu_j^*,\\sigma_j^*)  P(z_{y^*}=j)\\right] w_k\\] where \\(\\mu_j^*\\) and \\(\\Sigma_j^*\\) are found according to the kriging equations of a Gaussian process. That is,\n\\[\\mu_j^*=K(x^*,X^{(j)})^TK^{-1}y^{(j)}\\text{ and } \\sigma_j^*=K(x^*,x^*)-K(x^*,X^{(j)})^T K^{-1}K(x^*,X^{(j)})\\] where \\(K\\) is the covariance matrix based on parameters \\(\\theta_j\\) and \\(X^{(j)}\\subset X\\) and \\(y^{(j)} \\subset y\\) are the data and response values associated with cluster \\(j\\)."
  },
  {
    "objectID": "index.html#july-31-2025",
    "href": "index.html#july-31-2025",
    "title": "Weekly Reports",
    "section": "July 31, 2025",
    "text": "July 31, 2025\n\nSummary\nThis week I continued my research into how best to make posterior predictions using the iMGPE algorithm. I also present the fully functional distance dependent CRP-based version of the algorithm.\n\n\nPosterior Predictive Distribution\nLet \\(\\Omega=\\{\\alpha,\\phi,z,\\theta\\}\\) be the set of all model parameters. Then the predictive posterior distribution is\n\\[p(y^*)=\\int_{\\Omega} p(y^*|x^*,\\Omega)p(\\Omega|X,y) d\\Omega\\] \\[=\\int_{\\Omega} \\left[\\sum_{j=1}^J N(\\mu^*,\\sigma^{2*})P(z_{y^*}=j) \\right] p(\\theta|z,X,y)p(z|\\alpha,\\phi,X) p(\\alpha)p(\\phi)p(\\theta) d\\Omega\\]\nwhere the component in brackets is a weighted sum of each experts prediction and the component outside the brackets is the posterior density of the model parameters. The expert predictions are weighted by the probability of \\((x^*,y^*)\\) being assigned to each expert by our clustering process. Also \\[\\mu_j^*=K(x^*,X^{(j)})^TK^{-1}y^{(j)}\\text{ and } \\sigma_j^*=K(x^*,x^*)-K(x^*,X^{(j)})^T K^{-1}K(x^*,X^{(j)})\\] where \\(K\\) is the covariance matrix based on parameters \\(\\theta_j\\) and \\(X^{(j)}\\subset X\\) and \\(y^{(j)} \\subset y\\) are the data and response values associated with cluster \\(j\\).\nWe encounter two problems when attempting to calculate this directly. First, there is no closed form for the density \\(p(z|\\alpha,\\phi,X)\\). Second, \\(P(z_{y^*}=j)\\) depends on both the input data \\(x^*\\) and the response \\(y^*\\) according to the formulation laid down by Rasmussen et al. However, \\(y^*\\) is unknown, so we must estimate the probability without it.\nThe first problem is arguably moot as we “integrate” over \\(\\Omega\\) via the Markov Chain process, meaning that for a given iteration, all parameter values are given and the distribution of \\(y^*\\) is given simply by the weighted mean of the expert predictions. The second may still be an issue.\n\n\nBest Expert Prediction\nResearch into other mixture of experts prediction methods indicated that my current predictive approach is accurate, the problems discussed above notwithstanding. To get an idea of how my predictions could be improved, I went back to my experiments with ten ordered clusters. First, I plotted each experts prediction across the whole test data set. I found that individual experts predict well on points close to their training data but their accuracy decays rapidly as the points get farther away. Often, there is only one expert with anything to contribute to the understanding of a given point in the test set.\n\n\n\n\n\n\n\n\n\nGiven this, my next idea was to predict at new points by selecting the most likely expert, according to our cluster assignment method, for that point and use its estimate alone. This results in a much more accurate fitted function, though a much choppier one. This method is also mathematically unsatisfactory, due to its seemingly arbitrary nature. The best expert estimates for each GP method are shown below along with the true function line. The ML optimization method is in green, the STAN sampler is in blue, and the slice sampler is in red.\n\n\n\n\n\n\n\n\n\nAfter some contemplation, I hit upon a more mathematically sound idea. I could set a smaller value of \\(\\phi\\) to compel sharper transitions between experts. I learned in previous weeks that smaller values of \\(\\phi\\) increase the probability of a point being clustered with “like” points. Applying this to the test data, the influence of distant experts is weakened while that of nearby experts is strengthened. My previous code had used a \\(\\phi\\) value of \\(2.369\\), drawn at random from a previous run. Replacing that with \\(\\phi=1\\) effectively recreated the “best expert only” method.\n\n\n\n\n\n\n\n\n\nRight now, \\(\\phi\\) has a lognormal prior with \\(\\mu=0\\) and \\(\\sigma^2=1\\). Perhaps we can set a more informative or less heavy-tailed prior on \\(\\phi\\) to encourage sharper transitions between experts.\n\n\nDDCRP-iMGPE\nI fixed the posterior prediction code of the DDCRP-iMGPE algorithm and examined its output when trained for \\(5000\\) iterations on the simulated data set 1. This method tended to put all points into a single cluster, which is why \\(\\phi\\) grew so large - it has no significance in that context.\nA plot of the true and estimated functions is shown below. The credible interval for the estimate is not shown as the data was almost always in a single cluster and thus there is no uncertainty in the estimate.\n\n\n\n\n\n\n\n\n\nI plan to try this method on different data sets that cannot be captured by a single GP, to see if it will choose more clusters."
  },
  {
    "objectID": "index.html#april-3-2025",
    "href": "index.html#april-3-2025",
    "title": "Weekly Reports",
    "section": "April 3, 2025",
    "text": "April 3, 2025\n\nSummary\nThis week I elaborated upon the iMGPE model and further improved the algorithm. The trace plot for \\(\\phi\\) now appears to be converging. I should be able to convert the algorithm to Rcpp this week.\n\n\nData Model\nWe have an explanatory data matrix \\(\\mathbf{x}\\) and a scalar response \\(\\mathbf{y}\\), indicators \\(z_i\\), \\(i=1,\\dots,n\\), representing the cluster assignments for each point. Clusters are determined by a Dirichlet process with concentration parameter \\(\\alpha\\) and a Gaussian kernel with lengthscale parameter vector \\(\\phi\\). For each Gaussian process expert indexed by \\(j\\), we have a parameter vector \\(\\theta_j\\). Given all this, the likelihood of the data is a sum over exponentially many possible cluster assignments.\n\\[p(\\mathbf{y}|\\mathbf{x},\\theta)= \\sum_{\\mathbf{z}}\\left[ \\prod_j p(\\{y_i:z_i=j\\}| \\{x_i:z_i=j\\},\\theta_j) \\right]p(\\mathbf{z}|\\mathbf{x},\\alpha,\\phi)\\]\nIn the above likelihood, \\(p(\\{y_i:z_i=j\\}| \\{x_i:z_i=j\\},\\theta_j)\\) is the likelihood of the Gaussian process fit to cluster \\(j\\). The iMGPE algorithm performs Gibbs sampling of this likelihood. Lastly, \\(\\alpha\\), \\(\\phi\\), and all \\(\\theta_j\\) are assigned priors. We give \\(\\alpha\\) a vague inverse gamma prior, while each component of \\(\\phi\\) is assigned a vague gamma prior. The components of the \\(\\theta_j\\) each receive a gamma prior as well, so that, for example, the nugget parameters of every expert share the same prior.\n\n\nUpdated Results\nI adjusted the update step for \\(\\phi\\). It still uses a Random Walk Metropolis-Hastings sampler with with a normal proposal, but the variance of the proposal is set to the optimal value of \\(\\frac{2.38^2}{d}\\mathbf{H}^{-1}\\) where \\(d\\) is the number of dimensions and \\(\\mathbf{H}\\) is the Hessian matrix of the posterior density for \\(\\phi\\). I realized that I had forgotten to invert the Hessian in prior tests, which is why I was getting very small proposal variances.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe posterior distribution of \\(\\phi\\) has changed significantly, and now has a mean of about \\(88\\). Its acceptance probability is still very low, around \\(3\\%\\).\nFor every fifth iteration, I took the fitted values of the response at each point in time, resulting in a sample of 1000 fitted values for each point. I took the 2.5th and 97.5th quantiles of this sample to get a credible interval for the fitted value at each point. Below, I have plotted the true data as black dots, the mean predicted values as a red line, and the \\(95\\%\\) credible intervals as the gray ribbon."
  },
  {
    "objectID": "index.html#march-27-2025",
    "href": "index.html#march-27-2025",
    "title": "Weekly Reports",
    "section": "March 27, 2025",
    "text": "March 27, 2025\n\nSummary\nThis week I improved the update steps for the parameters \\(\\alpha\\) and \\(\\phi\\) in the Infinite Mixture of GP Experts function. I also altered the function output to report a credible interval for the value of each point in the data set.\nI have begun converting parts of the iMGPE algorithm to C++ with Rcpp. Performance gains are modest for now, but may increase in the future.\n\n\nChanges to DP Parameter Updates\nI rewrote the code for updating the Dirichlet process parameters \\(\\alpha\\) and \\(\\phi\\). First, I implemented a slice sampling update for \\(\\alpha\\) and attempted the same for \\(\\phi\\), though I wasn’t able to debug that in time. Instead, I adjusted the update step for \\(\\phi\\) by implementing a minimum standard deviation for the proposal density of its rejection sampling algorithm.\nCurrently, the proposal is a normal density with variance equal to the hessian of the density function for \\(\\phi\\). This was recommended by Rasmussen et al since it removes the need for a tunable parameter, but I have found that the Hessian produces extremely small proposal variances, leading to extremely small step sizes. After making the changes described, I fit the model to the motorcycle dataset with 5000 iterations. The trace plots for \\(\\alpha\\) and \\(\\phi\\) after these adjustments are shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe trace plot for \\(\\alpha\\) looks good, but the one for \\(\\phi\\) failed to converge. It may be that the average step size is still too small.\n\n\nUpdated Results\nThe distribution of cluster counts is shown below. It is roughly the same as in last meeting, concentrating around 6 to 8 clusters.\n\n\n\n\n\n\n\n\n\nFor every fifth iteration, I predicted the value of the response at each point in time. I took the 2.5th and 97.5th quantiles of this sample to get a credible interval for the fitted value at each point. Below, I have plotted the true data as black dots, the mean predicted values as a red line, and the \\(95\\%\\) credible intervals as the gray ribbon.\n\n\n\n\n\n\n\n\n\nThe credible interval bounds are much narrower than the prediction intervals plotted in the last meeting, especially for the early times."
  },
  {
    "objectID": "index.html#march-13-2025",
    "href": "index.html#march-13-2025",
    "title": "Weekly Reports",
    "section": "March 13, 2025",
    "text": "March 13, 2025\n\nSummary\nThis week I completed coding a version of the Infinite Mixture of GP Experts model proposed by Rasmussen and Ghahramani. I fit it to the motorcycle crash dataset, an example also used in the original paper. The algorithm is performing as intended, though the update steps for the Dirichlet process parameters may need to be adjusted.\n\n\nInitial Results\nI tested the algorithm first on the motorcycle crash dataset, a simple dataset of the acceleration of the head of a crash test dummy over time as its car hit a wall. There was thus one continuous input, time, and one continuous output, acceleration. Below is a plot of the motorcycle dataset.\n\n\n\n\n\n\n\n\n\nI fit a model to this dataset using the iMGPE algorithm, which ran for 5000 iterations. In each iteration, this algorithm generated a random assignment of data points to clusters using a modified Dirichlet process, fit a Gaussian process model to each cluster, and updated the cluster assignment parameters using MCMC. The posterior distributions of those parameters are discussed below, along with some other key features of the fitted model.\nThe cluster assignment probabilities were controlled by two parameters, \\(\\alpha\\) and \\(\\phi\\), where \\(\\alpha\\) is the Dirichlet process concentration parameter and \\(\\phi\\) is the lengthscale vector for a Gaussian kernel. In this example, \\(\\phi\\) is scalar since there is only one input variable. Their trace plots are shown here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBelow is a histogram of the number of clusters generated in each iteration. The algorithm generally used between six and seven clusters.\n\n\n\n\n\n\n\n\n\nAt each step of the algorithm, I drew a random sample from the posterior distributions of each data point, for a new set of 94 points. I discarded the first 2500 draws as burn-in and recorded the mean, median, and the 2.5th and 97.5th quantiles for each data point. These are plotted below, alongside the original data. The red line is the median and the grey ribbon covers the \\(95\\%\\) confidence band.\n\n\n\n\n\n\n\n\n\n\n\nConclusions\nThe trace plots for \\(\\alpha\\) and \\(\\phi\\) indicate some problems with their MH updates. The plot for \\(\\phi\\) clearly shows that new proposals are being rejected far too often. I will have to experiment with the acceptance probability to bring it into a reasonable range. The plot for \\(\\alpha\\) could indicate that the step size of their random walk is too small, another parameter that may need fine tuning.\nRasmussen and Ghahramani’s own experiment on this data set yielded a much smoother median line. My algorithm may be overfitting compared to theirs.\nFurthermore, it is not clear from Rasmussen’s paper if or how they intended to perform inference on new data given their model. Inference could be performed for any given set of experts but to draw from a posterior predictive distribution the same way the authors draw from the posterior would require the new data to be available during the model fitting process."
  },
  {
    "objectID": "index.html#february-27-2025",
    "href": "index.html#february-27-2025",
    "title": "Weekly Reports",
    "section": "February 27, 2025",
    "text": "February 27, 2025\n\nSummary\nThis week I worked on developing R code to run the iMGPE method by Rasmussen and Ghahramani. Most of the algorithm is complete, but I am not sure how to calculate a conditional probability necessary for the Gibbs sampling update of the indicator variables.\n\n\nTheory of Algorithm\nFor this model we have \\(y\\), a vector of \\(n\\) outputs, and \\(X\\), an \\(n\\times d\\) matrix of inputs. The data will be partitioned into \\(J\\) clusters and a GP expert fit to each cluster. Cluster assignments are represented by a vector of indicator variables \\(z={z_i: i=1,\\dots,n}\\). These values are controlled by a Dirichlet process with concentration parameter \\(\\alpha\\) and a kernel function parameterized by a lengthscale vector \\(\\phi\\). The parameters of each GP expert are represented by the vector \\(\\theta_j\\), with \\(\\theta\\) representing all GP parameters.\nTo begin, \\(y\\) and \\(X\\) are given and we place priors on \\(\\alpha\\), \\(\\phi\\), and \\(\\theta\\) and choose an initial cluster assignment for \\(z\\). The authors suggest a vague inverse Gamma prior for \\(\\alpha\\) and vague independent log-normal priors for \\(\\phi\\). Then, we run a Gibbs sampling sweep over each data point, updating their cluster assignments with Dirichlet process clustering. The conditional probability of point \\(i\\) being assigned to a cluster \\(j\\), is expressed below. This probability is in two parts, the second of which is input dependent and can be modified to accommodate qualitative inputs without much trouble. The first part factors into the conditional probabilities of one output given all other outputs in the expert.\n\\[p(z_i=j|z_{-i},X,y,\\theta,\\phi,\\alpha) \\propto p(y|z_i=j,z_{-i},X,\\theta)p(z_i=j|z_{-i}, x,\\phi,\\alpha)\\] \\[= p(y_i|y_{-i},x_j,\\theta_j)p(z_i=j|z_{-i}, x,\\phi,\\alpha)\\] \\[\\text{where } p(z_i=j|z_{-i},X,\\phi,\\alpha)=\\frac{n-1}{n-1+\\alpha}\\frac{\\sum_{i'\\neq i,z_{i'}=j} K_{\\phi}(X_i,X_{i'})}{\\sum_{i'\\neq i} K_{\\phi}(X_i,X_{i'})}\\]\nWhere \\(K_{\\phi}\\) is a Gaussian kernel function with lengthscales \\(\\phi\\). I am not sure how best to sample the conditional probabilities above, given that they depend on the covariance matrix between all points except \\(i\\) and given that the Gibbs sampling updates gradually change which points are assigned to which cluster. The authors remark that they can “reuse [covariance matrices] for consecutive Gibbs updates by performing rank one updates (since Gibbs sampling changes at most one indicator at a time),” but do not explain how to perform such updates.\nThen, we fit a Gaussian process to each cluster. I have been using the package ‘laGP’ to fit a standard squared exponential model. However, ‘laGP’ doesn’t let me set priors for the GP parameters, so I may need to use a different package. I do not plan to include qualitative inputs in this step to save on computation time.\nWe then update the parameters of our gating function, the DP concentration parameter \\(\\alpha\\) and the gating kernel lengthscales \\(\\phi\\). The posterior distribution of \\(\\alpha\\), below, is sampled using Adaptive Rejection Sampling.\n\\[p(\\alpha|n, J)\\propto \\alpha^{J-3/2}\\exp(-1/2\\alpha)\\Gamma(\\alpha)/\\Gamma(n+\\alpha)\\]\nFor \\(\\phi\\), we sample from the pseudo-posterior, which is the product of the conditional distributions of the indicator variables and the prior. We use vague independent log-normal priors for \\(\\phi\\), with parameters \\(\\mu=0\\) and \\(\\sigma^2=1\\).\n\\[p^*(\\phi|\\alpha, z, x) = \\left[\\prod_{i=1}^n p(z_i=j|z_{-i},\\phi,\\alpha)\\right] p(\\phi)\\]\nThis completes one iteration of the iMGPE algorithm. We repeat until all parameters have converged.\n\n\nFuture Work\nAs stated above, the algorithm is incomplete. Once I cross the last hurdle, I will be able to begin testing the accuracy and efficiency of this method on different datasets. I can experiment with different approaches to involving qualitative inputs in the gating function to see how they affect the speed and performance of the algorithm."
  },
  {
    "objectID": "index.html#february-13-2025",
    "href": "index.html#february-13-2025",
    "title": "Weekly Reports",
    "section": "February 13, 2025",
    "text": "February 13, 2025\n\nSummary\nThis week I researched improvements on the iMGPE method proposed by Rasmussen and Ghahramani. I found two papers that looked particularly useful, a 2005 paper by Meeds and Osindero and a 2010 paper by Sun and Xu, which extend the approach through generative modeling and variational inference respectively. More recent papers exist, but are largely focused on directions not relevant to us, such as multivariate responses and general non-stationary probabilistic regression.\nI have not yet found code for the iMGPE method. In the meantime, I have begun replicating the algorithm from Rasmussen and Ghahramani’s original paper.\n\n\nSummary of Alternative iMGPE\nThe 2005 paper “An Alternative Infinite Mixture of Gaussian Process Experts” by Meeds and Osindero presents an extension of the iMGPE approach using generative modeling. The generative approach to Mixture of Experts modeling assumes that the experts generate the inputs which generate the outputs, rather than conditioning the experts on the inputs. This technique can handle missing or incomplete data easily and allows for reverse-conditioning: assessing where in the input space a particular output is likely to have originated.\nThe generative model is most easily explained as a data generation algorithm comprised of a series of conditionals. To generate a set of \\(N\\) data points, we would take the following steps.\n\nSample the Dirichlet process concentration parameter \\(\\alpha_0\\) from a prior.\nPartition a set of \\(N\\) objects using a Dirichlet process, denoting the partitions with the indicator variables \\(\\{z_i\\}_{i=1}^N\\) taking values \\(r=1,\\dots,E\\).\nSample the gate hyperparameters \\(\\phi\\) from their priors.\nFor each partition, sample the input space parameters \\(\\psi_r\\) conditioned on \\(\\phi\\). These define the density in each input space.\nGiven the parameters for each group, sample the locations of the input points \\(X_r=\\{x_i:z_i=r\\}\\).\nFor each group, sample the hyperparameters \\(\\theta_r\\) of the GP associated with it.\nGiven \\(X_r\\) and \\(\\theta_r\\) for each group, formulate the GP output covariance matrix and sample the set of output values.\n\nInference for this model is accomplished through an MCMC algorithm to identify the expert partition and hyperparameters most likely to have generated the training inputs and output. Qualitative inputs could possibly be incorporated into the GP experts. This would require defining a generative multinomial (or similar) distribution for each qualitative input that allows correlation between them and the quantitative inputs, as well as priors for the hyperparameters of those distributions.\n\n\nSummary of Variational Inference\nThe paper “Variational Inference for Infinite Mixture of Gaussian Processes” by Sun and Xu further develops the generative mixture of experts model by employing a form of variational inference called mean field approximation to evaluate the model parameters. This is a method of approximating a probability distribution that can serve as a faster alternative to MCMC approximation.\nTake some data \\(X\\) generated by some latent variables \\(\\Omega=(\\alpha_0, \\phi)\\) through a hierarchical model. Suppose we have a joint prior on \\(\\Omega\\), \\(P(\\Omega)\\) and we want to approximate the posterior \\(P(\\Omega|X)\\). Standard variational inference defines a distribution \\(Q(\\Omega)\\) over \\(\\Omega\\) to be of a certain family of distributions similar to the posterior distribution. The similarity between them is measured through a dissimilarity function \\(d(P;Q)\\) and inference is performed by selecting \\(Q(\\Omega)\\) so as to minimize \\(d(P;Q)\\).\nFor mean field approximation, \\(Q(\\Omega)\\) is assumed to factorize over some partition of the unobserved variables \\(Z_1,\\dots,Z_M\\). It can be shown that the best distribution \\(q_m^*\\) for some factor \\(q_m\\), in terms of Kullback-Leibler divergence, is\n\\[q_m^*(\\Omega_m|X)= \\frac{e^{E_{q^*_{-m}}[\\ln p(\\Omega,X)]}}{\\int e^{E_{q^*_{-m}}[\\ln p(\\Omega,X)]} d\\Omega_m}\\]\nor equivalently,\n\\[\\ln q_m^*(\\Omega_m|X) = E_{q^*_{-m}}[\\ln p(\\Omega,X)] + \\text{constant}\\]\nwhere \\(E_{q^*_{-m}}[\\ln p(\\Omega,X)]\\) is the expectation of the logarithm of the joint distribution over all data and variables taken with respect to \\(q^*\\) over all variables not in the partition. This expectation can usually be determined to be of a known type of distribution. The factors can then be iteratively updated, much like the E-M algorithm.\nIn Sun and Xu’s case, they formulate the joint distribution of the inputs and outputs and then perform variational inference on each hyperparameter in turn. Their method also differs from the Alternative iMGPE by learning a support set for each GP expert that serves as the training data for that expert. These support sets are conditioned on the inputs and the expert partitions but are not synonymous with them.\n\n\nViability for Qualitative Inputs\nI am ambivalent towards the generative model proposed by Meeds and Osindero. It seems to be more popular than Rasmussen’s conditional model but I’m not sure its advantages are really useful in the case of WEPP emulation. Missing data is not a problem for us, nor is imputing inputs from outputs. Furthermore, it no longer makes sense to condition the Dirichlet process gating function on the inputs, meaning we can’t reduce the number of parameters by only incorporating the qualitative inputs into the gating function.\nVariational inference could be a faster alternative to MCMC, though the variational distributions have to be derived analytically, which could be challenging or even intractable depending on our model. Though Sun and Xu used a generative model, it is not needed to apply variational inference."
  },
  {
    "objectID": "index.html#february-6-2025",
    "href": "index.html#february-6-2025",
    "title": "Weekly Reports",
    "section": "February 6, 2025",
    "text": "February 6, 2025\n\nSummary\nThis week I studied the Mixture of Experts approach to modeling large datasets and considered its compatibility with qualitative inputs. A 2012 paper by Seniha Yuksel, et al, explained the fundamentals of ME modeling and surveyed the available variations. This paper is available at IEEE.\nI considered the approaches described by Yuksel and settled on one developed by Rasmussen and Ghahramani, called an Infinite Mixture of GP Experts (iMGPE), which uses the Dirichlet Process as a gating function to partition the dataset among a potentially infinite number of Gaussian process experts. Their paper is available for download at the NIPS website.\nLastly, I considered how these methods could be applied to my own research. Options for incorporating qualitative inputs in a Mixture of Experts are presented, with an eye to minimizing computational complexity.\n\n\nMixture of Experts Overview\nThe mixture of expert (ME) approach uses a set of expert models and a gating function to perform regression or classification on a dataset. The gating function makes a soft split of the input space, meaning that the partitioned regions may overlap. The experts are trained on the data, each one focusing on a partition. Parameter estimation for both the gating function and the experts is most often done through the Expectation-Maximization algorithm.\nAssume going forward that we have data \\(\\mathbf{x}\\) and response \\(\\mathbf{y}\\), that \\(n=1,\\dots ,N\\) is the number of data points, and \\(i=1,\\dots, I\\) is the number of experts. Let \\(\\theta=(\\theta_g,\\theta_e)\\) represent the parameters of the gating function and the experts. In this case, the probability of observing \\(\\mathbf{y}\\) given \\(\\mathbf{x}\\) is\n\\[P(\\mathbf{y}|\\mathbf{x},\\theta) = \\sum_{i=1}^I g_i(\\mathbf{x},\\theta_g)P(\\mathbf{y}|i, \\mathbf{x},\\theta_e)\\]\nFor regression, the gate \\(g_i(\\cdot)\\) is generally defined as the softmax function.\n\\[g_i(\\mathbf{x,v}) = \\frac{\\exp(\\beta_i(\\mathbf{x,v}))}{\\sum_{j=1}^I \\exp(\\beta_j(\\mathbf{x,v}))}\\] where \\(\\mathbf{v}\\) is the gate parameter and the functions of the gate parameters are linear: \\(\\beta_i(\\mathbf{x,v})=\\mathbf{v_i^T}[\\mathbf{x},1]\\). By introducing indicator variables \\(Z=\\{\\{z_i^{(n)}\\}_{n=1}^N\\}_{i=1}^I\\) representing the expert assignment for each observation, we can write the full log likelihood of the data and solve it.\n\\[l(\\mathbf{x},\\mathbf{y},Z,\\theta) = \\sum_{n=1}^n \\sum_{i=1}^I z_i^{(n)}\\left[ \\log g_i(\\mathbf{x^{(n)}}, \\theta_g) + \\log P_i(\\mathbf{y^{(n)}}) \\right]\\]\nNote that in the original ME formulation the parameters for the experts and the gating function are learned simultaneously and each expert is trained on all data, not just the data “assigned” to it.\n\n\nInfinite Mixture of GP Experts\nThe 2002 paper “Infinite Mixtures of Gaussian Process Experts” by Rasmussen and Ghahramani describes a modification of the ME approach that uses a Dirichlet process for the gating function and a potentially infinite number of Gaussian processes as experts, which they call iMGPE. The Dirichlet process works similarly to the clustering algorithm described on November 5, 2024, but is made input dependent through a squared exponential kernel parameterized by a lengthscale \\(\\phi\\), rather than by a Gaussian density.\nIt places Bayesian priors on the model parameters: an inverse gamma prior for the DP concentration parameter \\(\\alpha\\), inverse gamma priors on the spatial variance and error variance of each expert, with common hyperparameters \\((a_1,b_1)\\) and \\((a_2,b_2)\\), and independent log normal priors for the lengthscale parameters of each expert and the gating kernel parameter \\(\\phi\\).\nThis approach then iterates through the following MCMC algorithm.\n\nInitialize indicator variables \\(z_i^{(n)}\\) to a single value or set of values.\nUpdate each of the indicators with a Gibbs sampling sweep.\nDo Monte Carlo estimation of the parameters of each GP expert in turn.\nOptimize the hyperparameters \\((a_1,b_1)\\) and \\((a_2,b_2)\\) of the GP variances.\nSample the Dirichlet process concentration parameter, \\(\\alpha\\), using adaptive rejection sampling.\nSample the gating parameter \\(\\phi\\).\nRepeat from step 2 until the MCMC output has converged.\n\nThis method has several advantages over the standard ME approach. It allows the Dirichlet process to determine the appropriate number of experts to represent the data rather than specifying a certain number of experts beforehand. It also fits each GP expert only to a subset of the data, speeding up computation significantly.\n\n\nApplication to Qualitative Inputs\nMy current objective is to incorporate qualitative inputs into Gaussian process regression in the context of large data sets. The mixture of experts approach, particularly the variation described by Rasmussen, is a promising avenue of research. The primary tension between qualitative inputs and big data is that every qualitative input requires many more parameters be estimated to capture the relationship between levels, which complicates and slows down model fitting. The iMGPE reduces computation time while having plenty of room for qualitative inputs.\nAs a mixture of experts model does not much care what its component experts are, the most straightforward approach to incorporating qualitative inputs is to incorporate them into the GP experts through any previously described approach (EC, MC, LV). The viability of this option would only depend on the complexity of the chosen approach and the minimum number of experts you permit.\nA second option would be to utilize qualitative inputs in the gating function, the Dirichlet process, but not in the experts themselves. This would save significantly on computation as their parameters would only be estimated once per iteration instead of for each expert. Such an approach would be similar to the Naive Local Expert models I have already presented, but with two distinctions: the data partitions are learned rather than decided a priori and use all inputs, not just the qualitative ones. The downside is that we cannot easily interpret the relationship between qualitative inputs and the response."
  },
  {
    "objectID": "index.html#january-30-2025",
    "href": "index.html#january-30-2025",
    "title": "Weekly Reports",
    "section": "January 30, 2025",
    "text": "January 30, 2025\n\nSummary\nThis week I studied the latent variable approach to Gaussian process regression that was discussed on November 19. I downloaded the author’s code and ran it in MATLAB, replicating an example from the paper.\nI also considered how we might approach the task of GP regression with qualitative variables in situations with massive data or many inputs. I compared the known methods by the number of parameters needed and the flexibility of their correlation structures. I then discussed several ways to build upon the existing research.\n\n\nLatent Variable Method Example\nI return to the paper “A Latent Variable Approach to Gaussian Process Modeling with Qualitative and Quantitative Factors” by Zhang et al, available at https://www.tandfonline.com/doi/10.1080/00401706.2019.1638834\nI replicated one example from this paper related to beam bending: a metal beam with one of six different cross-sectional shapes is fixed at one end to a wall and downward pressure of \\(600\\) N is applied to the other end. The response is the amount of deformation in the beam. The sixth shape is known to behave significantly differently from the others, but this is not included in the model.\nI downloaded the author’s code and data and ran the example in MATLAB. It used a dataset with \\(60\\) observations of two quantitative variables and one qualitative variable with six levels. The RMSE was \\(8.46e-7\\) and the relative RMSE was \\(0.0623\\). The factor level latent variables were fit to 2D space as in the following plot:\n\n\n\n\n\n\n\n\n\n\n\n\nlevel\ncoords\n\n\n\n\n1\n(0,0)\n\n\n2\n(-0.0537,0)\n\n\n3\n(0.0121,-0.0001)\n\n\n4\n(-0.0290,0.0001)\n\n\n5\n(0.0403,-0.0007)\n\n\n6\n(0.7025,-0.1555)\n\n\n\nNote that the sixth factor level is far from the others, indicating that our model has successfully captured the underlying physical structure.\n\n\nQualitative GP with Big Data\nMost methods of fitting a GP with qualitative inputs are not designed for situations with large \\(n\\). While adding one more quantitative variable to a model rarely means more than one additional parameter to estimate, as with a separated squared exponential kernel function, adding one qualitative variable can easily add half a dozen or more parameters, depending on the number of levels it possesses. The exact number of parameters used varies significantly depending on the method of estimation chosen. Here, I compare some of the methods discussed on October 27 to the latent variable method.\nThe exchangeable covariance (EC) method uses just one parameter per factor variable. The unrestricted covariance (UC) method uses \\(\\sum_{j=1}^r m_j(m_j-1)/2\\) parameters, assuming \\(r\\) factors and \\(m_j\\) levels in the \\(r^{th}\\) factor. The multiplicative covariance (MC) method uses \\(\\sum_{j=1}^r m_j\\) parameters. The latent variable (LV) method uses \\(\\sum_{j=1}^r 2m_j-3\\) parameters. Thus, EC is best in terms of the number of parameters needed, followed by MC, but both are unable to model the full range of possible relationships between levels. The LV method, meanwhile, is superior to the UC method when at least one factor has four or more levels.\nIn an ideal situation we would be able to do the following:\nOne research direction available to us is to develop a statistical test to determine whether a qualitative variable can be safely excluded from a regression model. Similarly, we could develop a test to determine whether the correlation structure of a factor is significantly different from the exchangeable correlation structure.\nAnother is to investigate how any of the factor modeling methods described could be integrated into known big data techniques. We have attempted one form of integration with local expert models by stratifying by factor levels, but there may be better ways of combining these methods."
  },
  {
    "objectID": "index.html#january-23-2025",
    "href": "index.html#january-23-2025",
    "title": "Weekly Reports",
    "section": "January 23, 2025",
    "text": "January 23, 2025\n\nSummary\nThis week, I developed a tentative academic plan, which is attached separately. I also experimented with local expert methods on a simple dataset with no outliers. I compared using random forest models for each factor level to Gaussian process models and found that they performed equally well on a well-behaved dataset.\n\n\nGP vs RF on Simple Data\nI simulated a dataset consisting of one continuous variable x1 and one categorical variable x2. The categorical variable had three levels: \\({1,2,3}\\). The response was generated using a function proposed by Han, et al, where a different quadratic function of the continuous variable is used for each level of the categorical variable. I generated a training dataset with \\(900\\) observations, and two test datasets, each with 93. The x1 values in the training set and one of the test sets were generated from a \\(N(0,1)\\) distribution, while those in the second test set were evenly spaced between \\(-3\\) and \\(3\\). The training data is plotted below.\n\n\n\n\n\n\n\n\n\nI then stratified the training data by factor and compared fitting a GP to each group with fitting a random forest model to each. I compared the predictive accuracy of these models on both test sets, starting with the random set. For the random forest models, the RMSE was 0.68 and the MAE was 0.55. For the Gaussian process models, the RMSE was 0.30 and the MAE was 0.18. Comparing on the regularly spaced set, the RF models had an RMSE of 4.66 and an MAE of 2.35 and the GP models had an RMSE of 8.83 and an MAE of 4.38.\nThe x1 values of the random test set are clustered around zero, where most of the training data is, while those of the regular test set extend into regions with little training data. GPs seem to outperform RF models on predicting data points similar to the training data, but this relationship is reversed for data points outside the training data."
  },
  {
    "objectID": "index.html#january-17-2025",
    "href": "index.html#january-17-2025",
    "title": "Weekly Reports",
    "section": "January 17, 2025",
    "text": "January 17, 2025\n\nSummary\nThis week I continued my comparison of local expert Gaussian process models to random forest models. The random forest models previously outperformed the LE GP models, which we hypothesized to be because the RF models were fit to the whole training data set while the GP models were each fit to a single strata. However, when I stratified the data by qualitative variables and fit one RF model to each strata, the accuracy on the test data did not significantly change.\nComparing the predicted by actual plots for each method showed that the difference is mostly in how they respond to large (&gt;100) soil loss values, which make up about \\(2\\%\\) of observations. The RF method predicts all values as small while the GP method predicts all large values as small and some small values as large, resulting in a higher RMSE. There may simply not be enough large values in our dataset to accurately model them with a Gaussian process. Data augmentation to increase the number of large soil loss values in the training data may help.\nI also studied the factor level combination methods proposed by Cruz-Reyes and Pauger and Wagner, focusing on their design of prior correlation structures for factor effects and their relative advantages. Lastly, I considered how these methods could be adapted for use in our own WEPP research.\n\n\nStratified Random Forest Models\nI took the positive soil loss dataset and tested the performance of random forest models when using a local expert technique. I used the soil loss data from the years 2009 and 2010 as the training set and the data from 2011 as the test set. First, I took the training set and stratified it by ‘crop’. Then I fit a random forest model to each strata, predicting soil loss with the variables ‘rad’, ‘tmin’, ‘precip’, ‘max_prcp’, and ‘slp_wavg4’. Then I predicted soil loss on the test set and recorded the RMSE and mean absolute error of the predictions. I repeated this, stratifying on ‘till’ and then on both ‘crop’ and ‘till’.\n\n\n\nModel(Factor)\nRMSE\nMAE\n\n\n\n\nRF(crop)\n17.82\n3.75\n\n\nRF(till)\n17.47\n3.66\n\n\nRF(both)\n17.57\n3.75\n\n\n\nThe stratified random forest models did not perform significantly differently from the RF models that included crop or till. The performance gap between the local expert GP models and the RF models is not due to this stratification. For stratification by ‘crop’, plots of the predicted by actual response are shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe local expert random forest models have a much narrower range in predicted values than the local expert GPs.\n\n\nModel Structures for Factor Fusion\nHere, I describe the model and prior correlation structure presented by Pauger and Wagner in their 2017 paper. We have a linear model based on the dummy variable expression of a categorical covariate with \\(c+1\\) levels. Let \\(B_0(\\delta,\\tau^2)\\) be the prior correlation matrix of the regression coefficients \\(\\beta_1,\\dots ,\\beta_c\\) with respect to a baseline category. That is, \\(\\mathbf{\\beta}\\sim N(0,B_0(\\delta, \\tau^2))\\). It depends on a scale parameter \\(\\tau^2\\) and a vector of binary indices \\(\\delta\\) with one element for each pair of levels that may be fused. \\[B_0(\\delta,\\tau^2)=\\gamma \\tau^2 Q^{-1}(\\delta)= \\gamma\\tau^2\\left(\\begin{array}{cccc}\n\\sum_{j\\neq 1} \\kappa_{1j} & -\\kappa_{12} & \\dots & -\\kappa_{1c}\\\\\n-\\kappa_{21} & \\sum_{j\\neq 2} \\kappa_{2j} & \\dots & -\\kappa_{2c}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n-\\kappa_{c1} & -\\kappa_{c2} & \\dots & \\sum_{j\\neq c} \\kappa_{cj}\n\\end{array}\\right)\\]\nHere, \\(\\gamma=c/2\\) is a fixed constant. The \\(\\kappa_{kj}\\) (for \\(k&gt;j\\)) are defined as \\(\\delta_{kj}+r(1-\\delta_{kj})\\) where \\(r\\) is a fixed large number called the precision ratio and \\(\\delta_{kj}\\) is the indicator variable for whether levels \\(k\\) and \\(j\\) are separate. Then \\(\\kappa_{jk}=\\kappa_{kj}\\). Note that this means two levels of a factor will be highly correlated if \\(\\delta_{kj}=0\\) and weakly correlated if \\(\\delta_{kj}=1\\).\nThe precision ratio is so called because it is the ratio of the maximum prior precision of \\(\\beta_k\\) to the minimum prior precision. Lastly, hyperpriors are assigned to \\(\\tau^2\\) and \\(\\delta\\), an inverse Gamma prior on \\(\\tau^2\\) and iid Bernoulli priors on the elements of \\(\\delta\\). After fitting the model, the elements of \\(\\delta\\) tell you which pairs of factor levels were fused.\nCruz-Reyes, in her 2023 paper, starts with the same model structure but models \\(\\mathbf{\\beta}\\sim N(\\mathbf{0},\\sigma^2Q^{-1})\\) where \\(\\sigma^2\\) is a scale parameter and the \\(c\\times c\\) precision matrix \\(Q\\) is defined according to a parameter vector \\(\\rho\\). The vector \\(\\rho\\) has \\(c+\\frac{c(c-1)}{2}\\) elements, one for each factor level and one for each pair of factor levels (though some of the latter elements may be set to zero). Then \\(Q\\) is as follows.\n\\[Q(\\rho) = \\left(\\begin{array}{ccc}\n1+\\rho_1+\\sum_{j\\neq 1} |\\rho_{1j}| & \\dots & -\\rho_{1c}\\\\\n-\\rho_{21} & \\dots & -\\rho_{2c}\\\\\n\\vdots & \\ddots & \\vdots\\\\\n-\\rho_{c1} & \\dots & 1+\\rho_{c}+\\sum_{j\\neq c} |\\rho_{cj}|\n\\end{array}\\right)\\]\nIn \\(Q\\), the \\(\\rho_i&gt;0\\) for \\(i=1,\\dots, c\\) and \\(\\rho_{ij}=\\rho_{ji}\\). Lastly, Cruz-Reyes defines a custom prior for \\(\\rho\\) that accounts for these conditions. Both approaches combine variable selection with level fusion, as the merge of all levels of a categorical variable implies it has no effect on the response. Cruz-Reyes’s prior structure is arranged to allow for correlation between the levels of the variable, such as when represent distinct, possibly neighboring, geographic areas.\n\n\nFurther Research Options\nThe most obvious application to our soil erosion research is to use priors like the ones presented for the correlation matrix of a qualitative input in a Gaussian process model. Interpretation would be the same: a correlation of one between two levels of a factor would indicate that those levels can be clustered together. The optimal form of that prior would depend on the nature of the variable being modeled. The Cruz-Reyes prior permits spacial correlation between levels but requires more parameters than the Pauger-Wagner prior.\nA further avenue of research is how best to adapt these methods to situations with many qualitative variables, variables of many levels, or large \\(n\\). The existing methods depend on large numbers of parameters to model all the possible interactions. The question of how the model could be simplified or how the model fitting process could be sped up are worth exploring."
  },
  {
    "objectID": "index.html#december-19-2024",
    "href": "index.html#december-19-2024",
    "title": "Weekly Reports",
    "section": "December 19, 2024",
    "text": "December 19, 2024\n\nSummary\nThis week I fit a set of random forest models on the positive soil loss dataset, as a comparison with the local expert GP models I had fit previously. The local expert GPs underperformed the random forest models except for the GP stratified on ‘crop’.\nI also considered several approaches to identifying and consolidating significantly similar levels of categorical variables. A paper by Danna Cruz-Reyes suggested setting a Bayesian prior on the correlation matrix of the factor levels, acting as a penalty term to induce sparsity. It is designed for linear regression but may provide inspiration for Gaussian processes.\nThe paper by Cruz-Reyes is available at https://link.springer.com/chapter/10.1007/978-3-031-48415-5_11. It draws heavily on a paper by Daniela Pauger and Helga Wagner, available at https://projecteuclid.org/journals/bayesian-analysis/volume-14/issue-2/Bayesian-Effect-Fusion-for-Categorical-Predictors/10.1214/18-BA1096.full.\n\n\nRandom Forest Model Comparisons\nI have previously fit a number of GP models to the positive soil loss data set, using the data from 2009 and 2010 as a training set and the data from 2011 as a test set. I now fit three random forest models to the same training set, using the same numeric variables as well as ‘crop’, ‘till’, or both. I evaluate these models on the 2011 test set and display the results in the table below.\n\n\n\nModel\nRMSE\nMAE\n\n\n\n\nRF(crop)\n17.79\n3.73\n\n\nRF(till)\n17.56\n3.61\n\n\nRF(both)\n17.52\n3.63\n\n\nGP(crop)\n18.60\n4.26\n\n\nGP(till)\n26.44\n6.60\n\n\nGP(both)\n21.73\n5.70\n\n\n\nThe GP models stratified on ‘till’ and ‘crop’+‘till’ significantly underperform their random forest equivalents, while the GP model stratified on ‘crop’ performs close to the random forest including crop.\n\n\nCombining Factor Levels\nSimilar levels of a categorical variable may be identified at three stages: before, during, or after a model has been fit. Before, the distribution of response values within each category could be compared for statistical similarity. This could be based on mean values or other summaries, or test statistics such as KL divergence. It would be necessary to control for the other covariates for such statistics to have meaning.\nDuring, or as a part of, model fit, the most obvious solution is to convert factor inputs into dummy variables and apply any existing variable selection technique to the dummy variables. The downside of this approach is that dropping a dummy variable ‘combines’ that level with the reference level but cannot combine it with any other level. A better approach is presented by Cruz-Reyes in the section below, based on defining a multivariate normal prior for the effects of the factor levels. Numeric covariates could possibly be included in this process by modeling the covariances between factor levels as a function of the numeric variables.\nAfter fitting a model, there are a few possible ways of evaluating level similarity depending on the type of model that was fit. If fitting a local expert GP model, the levels could be compared on their lengthscale parameter values or on their predictive accuracy on other strata. If a GP trained on one level can accurately predict data from another level, the levels may not be significantly different. As another option, since GP regression has a multivariate normal posterior distribution, we could calculate confidence intervals for the expected value of the response conditional on each factor level and see if they overlap.\n\n\nSpacial Shrinkage Prior by Cruz-Reyes et al.\nThe paper “Spacial Shrinkage Prior: A Probabilistic Approach to Models for Categorical Variables with Many Levels” by Danna Cruz-Reyes describes a Bayesian method for combining levels of categorical variables within a linear regression model. Her technique is to encode the categorical variable with a parameter \\(\\mathbf{\\theta}=(\\theta_1, \\dots,\\theta_r)\\) representing the effects of its \\(r\\) levels.\nShe imagines the levels as nodes on a map with edges connecting each pair. She then puts a multivariate normal prior on \\(\\mathbf{\\theta}\\) with a covariance matrix depending on the random weights of the edges connecting nodes. Finally, she puts a hyperprior on the random edge weights that allows the weights to go to zero. If, upon fitting the model, they do, then the two categorical levels connected by that edge have been combined."
  },
  {
    "objectID": "index.html#december-10-2024",
    "href": "index.html#december-10-2024",
    "title": "Weekly Reports",
    "section": "December 10, 2024",
    "text": "December 10, 2024\n\nSummary\nThis week I performed a deeper exploratory analysis of the variables crop and till together. Notably, some combinations of crop and till did not occur in the stratified data set at all. Corn, for example, was never associated with zero tilling operations while tre was only associated with zero tilling operations. These imbalances may have significant effects on the performance of local expert methods.\nI also fit a local expert GP model by crop and till together. It outperformed the local expert model stratified on till but not the model stratified on crop. As with previous LE models, there were a small number of categories that contributed the majority of error.\n\n\nCrop by Till\nThe number of hillslopes with a particular combination of crop and tilling operation before 2012 is shown in the table below.\n\n\n\nseason\ncrop\\till\n0\n1\n4\n5\n6\n\n\n\n\nwinter\nalfalfa\n14\n10\n2\n25\n1\n\n\nwinter\nbromegr\n52\n0\n2\n11\n0\n\n\nwinter\nnone\n0\n88\n5\n258\n5\n\n\nspring\nalfalfa\n52\n0\n2\n11\n0\n\n\nspring\ncorn\n0\n98\n7\n283\n6\n\n\nspring\ntre\n14\n0\n0\n0\n0\n\n\n\nThe distribution of spring after 2012 is shown below. (The distribution for winter is unchanged.)\n\n\n\nseason\ncrop\\till\n0\n1\n4\n5\n6\n\n\n\n\nspring\nalfalfa\n52\n0\n0\n0\n0\n\n\nspring\ncorn\n0\n98\n9\n281\n6\n\n\nspring\nsoy\n0\n0\n0\n13\n0\n\n\nspring\ntre\n14\n0\n0\n0\n0\n\n\n\nFor each combination of crop and till, I recorded summary statistics of soil loss: the mean, median, and first and third quartiles. The results are shown below, from largest to smallest mean soil loss.\n\n\n\ncrop\ntill\nQ1\nmedian\nmean\nQ3\n\n\n\n\ncorn\n6\n0.2\n1.30\n15.44\n10.0\n\n\nsoy\n6\n0.2\n1.10\n13.07\n7.60\n\n\nalfalfa\n6\n0.2\n0.60\n12.04\n1.53\n\n\ncorn\n5\n0.2\n0.50\n6.99\n1.80\n\n\nsoy\n5\n0.2\n0.50\n6.54\n1.90\n\n\nalfalfa\n5\n0.1\n0.30\n5.29\n0.90\n\n\nnone\n5\n0.1\n0.30\n3.41\n0.90\n\n\nnone\n6\n0.1\n0.30\n3.34\n0.90\n\n\ncorn\n4\n0.1\n0.40\n3.10\n1.50\n\n\nnone\n4\n0.1\n0.30\n2.44\n0.80\n\n\nbromegr\n0\n0.1\n0.20\n1.62\n0.60\n\n\nbromegr\n5\n0.1\n0.20\n1.43\n0.43\n\n\ntre\n0\n0.1\n0.40\n1.36\n1.45\n\n\nsoy\n1\n0.1\n0.30\n1.36\n1.00\n\n\ncorn\n1\n0.1\n0.30\n1.24\n0.90\n\n\nalfalfa\n0\n0.1\n0.40\n1.22\n1.20\n\n\nbromegr\n4\n0.1\n0.30\n0.79\n1.10\n\n\nalfalfa\n1\n0.1\n0.20\n0.66\n0.43\n\n\nnone\n1\n0.1\n0.20\n0.60\n0.60\n\n\nalfalfa\n4\n0.1\n0.25\n0.48\n0.60\n\n\nbromegr\n1\n0.1\n0.20\n0.44\n0.50\n\n\n\n\n\nNLE by Crop and Till\nFrom my stratified data sample, I extracted only the dates between 2009 and 2010 which had nonzero soil loss, a total of 5938 observations. I combined crop and till into a single variable. There were 18 combinations present in the data set but three of them had too few observations in them to fit a model. After removing these, there were 5923 observations in the training data set. I then fit a local expert GP model by crop and till using the variables ‘rad’, ‘tmax’, ‘precip’, ‘max_prcp’, and ‘slp_wavg4’. The ML estimates of the component models are shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\ngroup\nrad\ntmax\nprecip\nmax_prcp\nslp_wavg4\nnugget\nsp.var\n\n\n\n\nalfalfa 0\n23378\n0.15\n3.07\n5e4\n3.52\n1.014\n1.494\n\n\nalfalfa 1\n1.88\n636.2\n3065\n5854\n0.634\n0.001\n1.371\n\n\nalfalfa 4\n5e4\n42585\n47815\n41995\n41908\n0.712\n0.124\n\n\nalfalfa 5\n34596\n41995\n5e4\n34577\n1.470\n9.254\n0.003\n\n\nbromegr 0\n2.45\n4.52\n29445\n5e4\n1.991\n8.204\n0.011\n\n\nbromegr 1\n2.07\n7.48\n12.17\n23.51\n10.15\n1e-8\n0.954\n\n\nbromegr 5\n39731\n5e4\n44022\n38292\n1.377\n4.407\n0.249\n\n\ncorn 1\n66.15\n50.62\n4.32\n50.25\n0.002\n0.122\n24.91\n\n\ncorn 4\n6.80\n3394\n4.12\n3058\n3.702\n0.027\n293.7\n\n\ncorn 5\n0.37\n7.40\n0.42\n6.59\n0.020\n0.002\n6283\n\n\ncorn 6\n5e4\n32206\n18.02\n5.25\n1.304\n0.406\n411.1\n\n\nnone 1\n0.95\n18351\n2.29\n5e4\n0.130\n0.323\n0.799\n\n\nnone 5\n17213\n10259\n0.95\n0.23\n0.215\n0.035\n39.80\n\n\nnone 6\n2.64\n6.63\n6.21\n1e-8\n5.712\n1e-8\n7.499\n\n\ntre 0\n44524\n1.02\n1.69\n5e4\n3.338\n0.445\n6.448\n\n\n\nThe model was tested on the positive soil loss data from 2011, with 1762 observations. The overall RMSE was \\(21.73\\) and the MAE was \\(5.70\\). The within group RMSEs on the test set were relatively low for all strata except for ‘corn 5’, ‘corn 6’, ‘none 5’, and ‘none 6’."
  },
  {
    "objectID": "index.html#december-3-2024",
    "href": "index.html#december-3-2024",
    "title": "Weekly Reports",
    "section": "December 3, 2024",
    "text": "December 3, 2024\n\nSummary\nThis week I fit a series of local expert GP models to the stratified data sample, fitting on soil loss based on ‘till’ and on average detachment based on the variables ‘crop’ and ‘till’. I found that the crop model fit to soil loss better than the till model, while the two models were about equal in performance when fit to average detachment.\nI also fit a set of linear regression models to the positive soil loss dataset to test for interactions between ‘crop’, ‘till’, and the numeric variables. Both had particularly strong interactions with slope and precip and weaker interactions with the other variables.\n\n\nGP of Soil Loss by Till\nFrom my stratified data sample, I extracted only the dates between 2009 and 2010 which had nonzero soil loss, a total of 5938 observations. I then fit a local expert GP model by till using the variables ‘rad’, ‘tmax’, ‘precip’, ‘max_prcp’, and ‘slp_wavg4’. The ML estimates of the component models are shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\ntill\nrad\ntmax\nprecip\nmax_prcp\nslp_wavg4\nnugget\nsp.var\n\n\n\n\n0\n2.51\n1.01\n3.71\n5e4\n3.84\n3.258\n0.482\n\n\n1\n83.05\n59.05\n5.71\n85.96\n0.002\n0.116\n25.67\n\n\n4\n1297\n12483\n0.594\n11744\n0.407\n0.052\n162.2\n\n\n5\n0.53\n5.58\n0.232\n14.53\n0.017\n0.002\n4822.4\n\n\n6\n131.3\n159.3\n6.62\n0.012\n5.521\n0.092\n928.8\n\n\n\nThe model was tested on the positive soil loss data from 2011, with 1769 observations. The overall RMSE was \\(26.44\\) and the MAE was \\(6.60\\). The within group RMSEs on the test set for Till = 0, 1, 4, 5, 6 were \\(3.60\\), \\(1.93\\), \\(12.30\\), \\(35.27\\), and \\(39.45\\). Till levels 5 and 6 had relatively high RMSEs while the rest had low, since those had a much greater range in soil loss values than the others.\n\n\nGP of Average Detachment by Crop and Till\nAgain from the stratified sample, I extracted the days with positive average detachment between 2009 and 2010, a total of \\(4662\\) observations. I then fit a local expert GP model by crop using the variables ‘rad’, ‘tmax’, ‘precip’, ‘max_prcp’, and ‘slp_wavg4’. The ML estimates of the component models are shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\ntill\nrad\ntmax\nprecip\nmax_prcp\nslp_wavg4\nnugget\nsp.var\n\n\n\n\nalfalfa\n90.70\n0.44\n42.34\n0.009\n414.7\n0.027\n0.014\n\n\nbromegr\n4.77\n11.53\n5e4\n24388\n35410\n0.495\n0.002\n\n\ncorn\n1.62\n0.53\n0.181\n1.92\n0.017\n0.003\n0.200\n\n\ntre\n0.48\n4.22\n8.50\n8.09\n9.228\n0.203\n0.0007\n\n\nnone\n146.6\n4.03\n608.3\n0.10\n0.668\n0.064\n0.007\n\n\n\nThe model was tested on the positive average detachment data from 2011, with \\(946\\) observations. The overall RMSE was \\(0.268\\) and the MAE was \\(0.095\\). The within group RMSEs on the test set for alfalfa, bromegrass, corn, tre, and none were \\(0.044\\), \\(0.055\\), \\(0.295\\), \\(0.020\\), and \\(0.278\\). I then fit a GP with the same inputs but stratified on ‘till’, with its ML estimates shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\ntill\nrad\ntmax\nprecip\nmax_prcp\nslp_wavg4\nnugget\nsp.var\n\n\n\n\n0\n1.89\n8.66\n3.97\n273.1\n454.8\n0.211\n0.001\n\n\n1\n1.78\n7.19\n1.08\n12.59\n1.574\n0.048\n0.003\n\n\n4\n32.45\n2.90\n6.12\n4.66\n0.748\n0.131\n0.010\n\n\n5\n0.26\n1.40\n0.347\n13.45\n0.028\n0.002\n0.209\n\n\n6\n0.91\n782.8\n7.22\n0.274\n0.183\n0.147\n0.040\n\n\n\nThe model was tested on the positive average detachment data from 2011, with \\(946\\) observations. The overall RMSE was \\(0.249\\) and the MAE was \\(0.084\\). The within group RMSEs on the test set for till = 0, 1, 4, 5, 6 were \\(0.041\\), \\(0.039\\), \\(0.125\\), \\(0.298\\), and \\(0.417\\).\n\n\nLinear Models with Interactions\nUsing the positive soil loss dataset, I fit a series of linear models to test for interactions between the categorical and numeric variables. The table below displays parameter estimates for the variables ‘crop’, ‘rad’, ‘tmin’, precip’, ‘max_prcp’, and ‘slp_wavg4’. It is filtered to only display the interaction terms that were statistically significant.\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.23\n2.26\n-0.99\n0.32\n\n\ncropbromegr\n-1.24\n4.50\n-0.27\n0.78\n\n\ncropcorn\n-10.10\n2.58\n-3.92\n0.00\n\n\ncropnone\n2.53\n3.30\n0.77\n0.44\n\n\ncropsoy\n-11.63\n2.85\n-4.09\n0.00\n\n\ncroptre\n-0.77\n5.83\n-0.13\n0.89\n\n\nrad\n0.00\n0.00\n0.49\n0.62\n\n\ntmin\n-0.16\n0.09\n-1.83\n0.07\n\n\nprecip\n0.09\n0.02\n3.80\n0.00\n\n\nmax_prcp\n0.01\n0.04\n0.32\n0.75\n\n\nslp_wavg4\n16.51\n13.16\n1.25\n0.21\n\n\ncropsoy:rad\n0.01\n0.00\n2.13\n0.03\n\n\ncropcorn:tmin\n0.19\n0.11\n1.81\n0.07\n\n\ncropnone:tmin\n0.39\n0.18\n2.17\n0.03\n\n\ncropcorn:precip\n0.08\n0.03\n3.19\n0.00\n\n\ncropsoy:precip\n0.07\n0.03\n2.21\n0.03\n\n\ncropcorn:max_prcp\n0.10\n0.04\n2.20\n0.03\n\n\ncropcorn:slp_wavg4\n162.59\n15.32\n10.62\n0.00\n\n\ncropsoy:slp_wavg4\n208.82\n17.65\n11.83\n0.00\n\n\n\n\n\nBelow are the model results for the variable ‘till’, filtered as before.\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.41\n2.05\n-1.18\n0.24\n\n\ntill1\n1.13\n2.50\n0.45\n0.65\n\n\ntill4\n0.45\n5.60\n0.08\n0.94\n\n\ntill5\n-13.61\n2.28\n-5.97\n0.00\n\n\ntill6\n-34.88\n6.28\n-5.55\n0.00\n\n\nrad\n0.00\n0.00\n0.34\n0.73\n\n\ntmin\n-0.04\n0.08\n-0.51\n0.61\n\n\nprecip\n0.06\n0.02\n2.92\n0.00\n\n\nmax_prcp\n0.00\n0.03\n0.07\n0.94\n\n\nslp_wavg4\n10.73\n12.05\n0.89\n0.37\n\n\ntill5:rad\n0.01\n0.00\n2.42\n0.02\n\n\ntill6:rad\n0.01\n0.01\n1.86\n0.06\n\n\ntill5:tmin\n0.15\n0.09\n1.70\n0.09\n\n\ntill5:precip\n0.12\n0.02\n4.89\n0.00\n\n\ntill6:precip\n0.70\n0.07\n10.40\n0.00\n\n\ntill5:max_prcp\n0.12\n0.04\n3.12\n0.00\n\n\ntill6:max_prcp\n0.31\n0.07\n4.49\n0.00\n\n\ntill5:slp_wavg4\n267.98\n14.36\n18.66\n0.00\n\n\ntill6:slp_wavg4\n184.26\n57.92\n3.18\n0.00\n\n\n\n\n\nLastly, here are the model results for the variable ‘cover’.\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-7.91\n0.79\n-10.02\n0.00\n\n\ncoverTRUE\n1.87\n3.04\n0.61\n0.54\n\n\nrad\n0.01\n0.00\n4.87\n0.00\n\n\ntmin\n0.01\n0.04\n0.36\n0.72\n\n\nprecip\n0.12\n0.01\n13.19\n0.00\n\n\nmax_prcp\n0.09\n0.01\n7.80\n0.00\n\n\nslp_wavg4\n106.57\n5.18\n20.56\n0.00\n\n\ncoverTRUE:rad\n-0.01\n0.01\n-0.91\n0.36\n\n\ncoverTRUE:tmin\n0.00\n0.16\n0.03\n0.98\n\n\ncoverTRUE:precip\n0.03\n0.03\n1.03\n0.31\n\n\ncoverTRUE:max_prcp\n-0.08\n0.07\n-1.06\n0.29\n\n\ncoverTRUE:slp_wavg4\n-59.80\n20.91\n-2.86\n0.00"
  },
  {
    "objectID": "index.html#november-19-2024",
    "href": "index.html#november-19-2024",
    "title": "Weekly Reports",
    "section": "November 19, 2024",
    "text": "November 19, 2024\n\nSummary\nThis week I studied an article by describing an approach to incorporating qualitative variables in Gaussian processes based on latent variables. By expressing each level of a quantitative variable as a value of a 2D latent numeric variable, they can model the underlying numeric process behind the variable directly. This method had significantly lower restricted RMSE than comparable qualitative methods.\nI also conducted further exploratory analyses of the stratified data set. I graphed soil loss by several input variables and colored the points according to the ‘crop’ and ‘till’ variables. The results confirmed that alfalfa and bromegrass generally had lower soil loss than corn and soy, even when controlling for inputs such as precipitation, temperature, and slope.\n\n\nLatent Variable Approach to GP Modeling\nIn their paper “A Latent Variable Approach to GP Modeling with Qualitative and Quantitative Variables”, the authors Zhang et al. argue that in any physical process qualitative variables are stand-ins for unobserved numeric variables. Therefore, they can be modeled by modeling the underlying latent variables they represent. For a GP with numeric inputs \\(x_1,\\dots, x_p\\) and a qualitative input \\(t\\), they suggest this modification of the squared exponential function. \\[K(X, X') =\\exp\\left(-\\sum_{i=1}^p \\frac{|x_i-x_i'|^2}{\\theta_i} -|z(t)-z(t')|^2 \\right)\\] Where \\(\\theta_i\\) are lengthscale parameters and \\(z(t)\\) is a function mapping each level of \\(t\\) to a 2D numeric quantity. The \\(z(t)\\) values can then be fit through MLE.\nA two dimensional latent variable is deemed superior to a one dimensional variable for the following reason. Suppose the qualitative factor has three levels with equal correlation between each pair of levels. To represent this with a one dimensional mapping is impossible, but with two or more dimensions it can be done. To prevent indeterminacy in ML estimation, they further restrict the first level of each qualitative variable to be mapped to \\((0,0)\\) and the second to be mapped to the horizontal axis, so for a variable with \\(m\\) levels, there are \\(2m-3\\) scalar values to estimate.\nThe authors compared their method to two other methods of qualitative GP analysis: multiplicative covariance and hypersphere covariance, which they called unrestricted covariance. I discussed these methods on October 22 of this report. The latent variable method outperformed both in terms of restricted RMSE, which is the RMSE divided by the sum of the squared differences between the predicted response and the true mean of the responses, on multiple real-life datasets.\n\n\nAdditional Exploratory Analysis\nI performed a more in-depth analysis of the relationships between the new categorical variables and the numeric variables. Shown below are plots of significant inputs by response with points colored according to crop or to tilling operations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI also fit a linear model to soil loss, including ‘crop’ and ‘till’ as inputs. I used ‘precip’, ‘rad’, ‘tmin’, and ‘slp_wavg4’ as my numeric inputs.\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.79\n0.69\n-8.41\n0.00\n\n\nprecip\n3.07\n0.20\n15.76\n0.00\n\n\nrad\n0.83\n0.22\n3.75\n0.00\n\n\ntmin\n0.12\n0.25\n0.49\n0.62\n\n\nslp_wavg4\n5.84\n0.22\n26.58\n0.00\n\n\ncropbromegr\n1.83\n1.30\n1.40\n0.16\n\n\ncropcorn\n6.47\n1.16\n5.56\n0.00\n\n\ncropnone\n4.77\n1.37\n3.48\n0.00\n\n\ncropsoy\n6.69\n1.20\n5.56\n0.00\n\n\ncroptre\n1.70\n1.31\n1.31\n0.19\n\n\ntill1\n1.48\n1.29\n1.14\n0.25\n\n\ntill4\n1.09\n1.81\n0.60\n0.55\n\n\ntill5\n7.63\n1.24\n6.16\n0.00\n\n\ntill6\n8.90\n1.68\n5.30\n0.00"
  },
  {
    "objectID": "index.html#november-12-2024",
    "href": "index.html#november-12-2024",
    "title": "Weekly Reports",
    "section": "November 12, 2024",
    "text": "November 12, 2024\n\nSummary\nThis week I reran the Dirichlet process cluster analysis on a dataset with unbalanced cluster sizes, having one large cluster, two medium, and one small. The small cluster was still fit fairly accurately, so the bias towards adding points to large clusters doesn’t seem to be very big.\nI experimented with random forest models to predict whether a observation had positive soil loss or not. I found that ‘crop’ was a much more significant variable than ‘till’, and ‘month’ was also significant. By undersampling the zero cases to form a new training set and refitting the model, I was able to somewhat improve the predictive accuracy on a test set.\nI also fit an NLE GP model to positive soil loss data from 2009 and 2010, separated by crop. Based on test data from 2011, Corn and No Crop were much harder to predict than the others, as there is much more variability in soil loss within those groups.\n\n\nSoil Loss Prediction with Random Forest\nI fit a random forest model to the stratified sample of hillslopes using the new categorical variables. I first fit a model including all the new variables and five variables that had proven significant in previous models. Second, I fit a model using only the top 5 variables. The table displays the importance of each variable in the models by the mean decrease in the Gini index (a measure of the purity of sets).\n\n\n\nVariable\nFull\nReduced\n\n\n\n\nprecip\n54803\n56420\n\n\nslp_wavg1\n1918\n-\n\n\ntmin\n7300\n8151\n\n\nrad\n3982\n3259\n\n\nmax_prcp\n1128\n-\n\n\ncrop\n2813\n3022\n\n\ntill\n338\n-\n\n\nmonth\n4309\n4120\n\n\ncover\n251\n-\n\n\n\nThe confusion matrix of the second model showed that it never predicted any observation in the training set as having nonzero soil loss. This occurred even though I tried to weight the nonzero cases in the training set. Its performance was not bad on the test set, however.\n\n\n\nModel\nTrue\\Pred\n0\n1\nClassError\n\n\n\n\nTrain\n0\n149877\n2911\n0.0191\n\n\n.\n1\n0\n0\nNA\n\n\nTest\n0\n16646\n330\n0.0194\n\n\n.\n1\n9\n279\n0.0313\n\n\n\nI decided to try fitting a random forest model to a more balanced training set by undersampling the zero cases. I took my training set and randomly selected \\(10\\%\\) of the zero cases and added them to all the nonzero cases in the training set. The resulting data had \\(15279\\) zero observations and \\(2593\\) nonzero observations. This model performed much better on predicting the nonzero observations and a bit worse on the zero observations.\n\n\n\nModel\nTrue\\Pred\n0\n1\nClassError\n\n\n\n\nTrain\n0\n14891\n388\n0.0254\n\n\n.\n1\n36\n2557\n0.0139\n\n\nTest\n0\n16556\n420\n0.0247\n\n\n.\n1\n4\n284\n0.0139\n\n\n\n\n\nGP NLE by Crop\nFrom my stratified data sample, I extracted only the dates between 2009 and 2010 which had nonzero soil loss, a total of 5938 observations. I then fit a local expert GP model by crop using the variables ‘rad’, ‘tmax’, ‘precip’, and ‘max_prcp’. The ML estimates of the component models are shown below.\n\n\n\ncrop\nrad\ntmax\nprecip\nmax_prcp\nnugget\nsp.var\n\n\n\n\nalfalfa\n32.19\n47.70\n26.98\n14.77\n9.318\n0.003\n\n\nbromegr\n4.14\n12.06\n39033\n50000\n11.534\n0.0003\n\n\ncorn\n39052\n2302\n0.689\n0.003\n0.169\n3249\n\n\ntre\n0.39\n3.04\n3.22\n4.95\n0.494\n6.672\n\n\nnone\n50000\n38244\n0.053\n0.43\n0.272\n15.345\n\n\n\nThe model was tested on the positive soil loss data from 2011, with 1769 observations. The overall RMSE was \\(18.60\\) and the MAE was \\(4.26\\). The within group RMSEs on the test set for Alfalfa, Bromegrass, Corn, Tre, and None were \\(3.33\\), \\(5.17\\), \\(20.60\\), \\(2.21\\), and \\(24.06\\). Corn and None had relatively high RMSEs while Alfalfa, Bromegrass, and Tre had low, since the first two had a much greater range in soil loss values than the others.\n\n\nDPMM with Unbalanced Cluster Sizes\nI repeated the Dirichlet Process Mixture Modeling example from last week, but with unbalanced cluster sizes. I drew \\(20\\) points from Cluster 1, \\(100\\) from cluster 2 and \\(60\\) from the other two, rather than \\(60\\) from each. The training data is plotted below.\n\n\n\n\n\n\n\n\n\nThe results of the DPMM algorithm with \\(\\alpha=0.01\\) are shown below, comparing the true cluster assignments to the estimated ones. The class imbalance did not seem to significantly affect the accuracy of the result, though Cluster 4 is still poorly handled. I also tried running the algorithm for 2000 iterations instead of 1000 but it didn’t meaningfully change the results.\n\n\n\nTrue\\Est\n1\n2\n3\n4\n\n\n\n\n1\n19\n0\n1\n0\n\n\n2\n0\n100\n0\n0\n\n\n3\n0\n0\n60\n0\n\n\n4\n0\n4\n19\n37"
  },
  {
    "objectID": "index.html#november-5-2024",
    "href": "index.html#november-5-2024",
    "title": "Weekly Reports",
    "section": "November 5, 2024",
    "text": "November 5, 2024\n\nSummary\nThis week I prepared an example of Dirichlet Process Mixture Modeling, which sorts data values into clusters by modeling each cluster as a multivariate Gaussian distribution. The advantage of this method is that you do not need to specify the number of clusters beforehand; it is estimated from the data.\nI also merged the management data with the weather data for the stratified sample of HUC12s. I tested the categorical variables for correlation with the response variables and prepared some boxplots displaying their relationships. These plots showed that soil loss was highest for main crops like corn and soy and for tilling with row cultivators. This seems reasonable since row cultivators can be used multiple times per season.\n\n\nDirichlet Process Mixture Modeling\nI borrowed the following example from . Dirichlet Process Mixture Modeling is a method of cluster analysis based on the Gaussian mixture models. The data is divided into clusters where the responses in each cluster are drawn from a different multivariate Gaussian distribution. The special characteristic of DPMM is that the number of clusters is estimated organically from the data and doesn’t need to be specified beforehand.\nThe cluster assignment process is based on an MCMC algorithm that generates a sequence of cluster assignments. For each iteration, the data points are assigned to clusters with a Chinese Restaurant Process. Imagine a Chinese restaurant with an infinite number of tables. Diners enter one by one to be seated. The first diner always sits at the first table. The second diner sits at the first table with probability \\(1/(1+\\alpha)\\) and at the second table with probability \\(\\alpha/(1+\\alpha)\\) where \\(\\alpha\\) is a positive real number. The \\(k^{th}\\) diner sits at an occupied table with probability proportional to the number of people already sitting there and at the next unoccupied table with probability proportional to \\(\\alpha\\).\nIn our case, those probabilities are adjusted by the posterior probability of the new point belonging to each cluster given prior assumptions about the distribution of points within a cluster. Once you have a sequence of such assignments, you note for each point the cluster to which it was most often assigned, which is its final assignment.\nSuppose we have a data vector \\(y\\) normally distributed according to a mean \\(\\mu\\) and a known measurement error \\(\\sigma_y^2\\), and that \\(\\mu\\) is distributed with a prior mean \\(\\mu_0\\) and precision \\(\\tau_0^2\\). Then, to be precise, the probability of a new point \\(y_i\\) being assigned to cluster \\(k\\) is \\[p(y_i\\in c_k)\\propto n_{-i,k}\\times \\text{pdf}(y_i|\\mu_k,\\tau_k)\\] \\[\\propto \\frac{n_{-i,k}}{n-1+\\alpha}\\Phi\\left(y_i| \\frac{\\bar y_kn_k \\tau_k+\\mu_0\\tau_0}{n_k\\tau_k+\\tau_0}, (n_k\\tau_k+\\tau_0)^{-1}+\\sigma_y^2 \\right)\\] where \\(n_{-i,k}\\) is the number of points in cluster \\(k\\) not counting \\(y_i\\), \\(n\\) is the total number of points assigned to clusters, and \\(\\bar y_k\\) and \\(\\tau_k\\) are the estimated mean and precision of cluster \\(k\\). Finally, \\(\\Phi()\\) is the pdf of the Normal distribution.\nFor our example we sixty points each from four bivariate normal distributions, which are our four clusters. The data set is plotted below.\n\n\n\n\n\n\n\n\n\nThe results of the DPMM algorithm with \\(\\alpha=0.01\\) are shown below, comparing the true cluster assignments to the estimated ones.\n\n\n\nTrue\\Est\n1\n2\n3\n4\n\n\n\n\n1\n60\n0\n0\n0\n\n\n2\n0\n0\n60\n0\n\n\n3\n1\n0\n0\n59\n\n\n4\n7\n40\n4\n9\n\n\n\n\n\n\n\n\n\n\n\n\nRepeating this process on the same data but with an \\(\\alpha\\) value of \\(0.1\\) are shown here. The cluster assignments are very similar,\n\n\n\nTrue\\Est\n1\n2\n3\n4\n\n\n\n\n1\n0\n0\n0\n60\n\n\n2\n0\n0\n60\n0\n\n\n3\n60\n0\n0\n0\n\n\n4\n18\n38\n4\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTilling Operations Description\nAfter studying, I can better define some of the equipment referenced in the tilling operations in the WEPP data set. A planter is used to cut open the soil and drop the seeds in. Chisel plows, field cultivators, and row cultivators are all used to break up the soil before or after the growing season. Chisel plows dig deepest, while cultivators are shallower. Row cultivators are used to break up the soil between planted rows, possibly for weed control. Tandem disks and double disk openers can be added to cultivators to further disturb the soil. Double disk openers are designed for handling stubble, field debris, or other trashy conditions.\n\n\nCategorical and Numeric Data Merge\nI added three variables to the stratified data set this week: Crop, Cover, Till, and Month. Crop records the kind of crop we think was growing on a hillslope at a particular time, Cover records whether the crop was a cover crop or not, Till records the set of tilling operations performed on that hillslope, and Month is the month the observation was recorded.\nI performed Kruskal-Wallis rank sum tests between each categorical variable and response to test for correlation between them. Crop was significantly correlated to soil loss, runoff, and average detachment with chi-squared statistics of \\(16916\\), \\(4409\\), and \\(16196\\) respectively. Cover was as well, with test statistics of \\(1391\\), \\(491\\), and \\(1852\\) respectively. Till also was, with test statistics of \\(805\\), \\(1010\\), and \\(472\\) respectively. The p-values were less than \\(2.2\\text{e-}16\\) for each case.\nHere, I have plotted boxplots of the response variables with respect to the categorical inputs ‘crop’ and ‘till’. It is easy to see that main crops (corn and soy) have higher soil loss than cover crops."
  },
  {
    "objectID": "index.html#october-29-2024",
    "href": "index.html#october-29-2024",
    "title": "Weekly Reports",
    "section": "October 29, 2024",
    "text": "October 29, 2024\n\nSummary\nThis week, I performed a deep analysis into the qualitative variables available in the WEPP stratified sample, specifically the crops present at each location and the tilling operations performed at that location. As a reminder, the stratified sample consists of three HUC12 watersheds, spread across Iowa, and all the hillslopes recorded within each watershed.\nEach crop listed in a hillslope’s management file is associated with a comment that may include a date, either in 1998 or 2013. There is a stark division between the crops labeled 1998 and those labeled 2013. There are also distinctions between the different watersheds, which are displayed through separate graphs. Several questions of how to interpret the crop data remain before it can be merged with the main dataset.\nI also performed analysis of the tilling operations data. There are six tilling operations in the sample, with four unique patterns of co-occurence. Notable differences between watersheds are explored through graphs of the operation frequency. This data could be merged with the main dataset now, possibly alongside a date or month variable, since the effect of tilling is time-dependent.\n\n\nWEPP Crops Deep Dive\nThe original analysis of the crop variables in WEPP considered the full stratified sample drawn in June which contained all hillslopes in three HUC12 watersheds. This sample contained the watershed \\(071000030704\\) with \\(227\\) hillslopes, \\(070801051103\\) with \\(113\\) hillslopes, and \\(102802010405\\) with \\(133\\) hillslopes. Separating the watersheds, we see the distribution of crop variants differs by location.\nFor location \\(071000030704\\), we see the following.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor location \\(070801051103\\), we see:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor location \\(102802010405\\), we see:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA detailed analysis of the crop data for the stratified sample of HUC12s found a sharp delineation between crops labeled in 1998 and crops labeled in 2013. For crops labeled February 1998, only corn, alfalfa, brome grass, and Tre_2932 are present. The table below displays the frequency of each combination of these crops, where a 1 in the first four columns means that that crop is present and a 0 means it is absent.\n\n\n\nCorn\nAlfalfa\nBrome\nTre2932\nN\n\n\n\n\n0\n1\n0\n1\n13\n\n\n0\n1\n1\n1\n1\n\n\n0\n1\n1\n0\n65\n\n\n1\n0\n0\n0\n356\n\n\n1\n1\n0\n0\n31\n\n\n1\n1\n1\n0\n7\n\n\n\nFor crops labeled April 2013, if they exist they are always a corn variant and a soy variant. The WEPP User Summary adds some clarity to this situation, explaining that “A different type of residue on a field besides the current crop growth being simulated also needs to be assigned a crop number.” In other words, crops marked as present may not actually be growing on the field if the remains from past growth are present.\nThe logical next step is to merge the crop data with the daily weather and soil loss data, but three major complications exist to implementing this at this point. First, we must decide when each crop type is on the field. A reasonable estimation is that main Crops run from May 1 to October 31 while cover crops run from November 1 to April 30. Second, many hillslopes list two main crops or two cover crops present at the same time. One solution might be to say that if two main crops or two cover crops coexist, they alternate years (e.g. the farmer plants corn one year, soy the next, corn the next, and so on). Lastly, we must decide whether Thre_2932 is a main crop or a cover crop. It never coexists with corn or soy so a main crop may be more likely, but a definitive answer is needed. Alternatively, we can just omit the \\(14\\) hillslopes containing Tre_2932 from our analysis.\n\n\nTilling Operations Analysis\nThere are six tilling operations found within the stratified WEPP data set. They are listed below, alongside their descriptions as taken from the management files.\n\nPLNTFC: Planter, no-till with fluted coulter.\nFCSTACDP: Field cultivator, secondary tillage, after duckfoot points. Maximum depth of 10 cm (4 inches).\nTAND0002: Tandem disk.\nPLDDO: Planter, double disk openers. Tillage depth of 2 inches.\nCULTMUSW: Cultivator, row, multiple sweeps per row.\nCHISSTSP: A chisel plow, straight with spike points.\n\nSome distinct patters emerge in how these tilling operations co-occur. PLNTFC occurs alone in 98 hillslopes. The other five operations occur together in 294 hillslopes. The other five minus CULTMUSW occur in 9 hillslopes. All six occur together in 6 hillslopes. No tilling operations are listed for 66 hillslopes\nThe results for \\(071000030704\\) are below.\n\n\n\n\n\n\n\n\n\nThe results for \\(070801051103\\) are below.\n\n\n\n\n\n\n\n\n\nThe results for \\(102802010405\\) are below."
  },
  {
    "objectID": "index.html#october-22-2024",
    "href": "index.html#october-22-2024",
    "title": "Weekly Reports",
    "section": "October 22, 2024",
    "text": "October 22, 2024\n\nSummary\nThis week I reviewed a number of papers discussing how to incorporate qualitative variables into GPs. Existing solutions focus on defining a correlation function for qualitative variables, separate from that for quantitative variables. These correlation functions often include a large number of estimated parameters to capture the correlations between factor levels. For this reason, most methods increase the computational complexity of model fitting. How best to fit qualitative variables while minimizing computation time is a less explored question.\nI also summarized the crop qualitative variables in the WEPP dataset, and described some of the other qualitative variables in that dataset.\n\n\nQualitative Input Lit Review\nA literature review of existing work on incorporating qualitative variables into Gaussian process models shows that the existing methods generally approach the issue by defining a kernel (covariance) function for qualitative variables that differs from the kernel function for quantitative variables. This is a contrast to my previous approach, which focused on defining a distance function for qualitative variables that could be fed into the same covariance function as the quantitative variables. The covariance-based approach allows for much greater flexibility in how correlations between levels of a factor are defined, but has some weaknesses of its own.\nThe methods discussed below derive from three main articles: by X. Deng, C. Devon Lin, et al.  by Peter Quian, C. F. Jeff Wu, and Huaiqing Wu. by Peter Quian, Qiang Shu, and Shiyu Zhu.\nIf your distance function on qualitative variables doesn’t depend on any parameters, then the distance matrix between your observed data points can be calculated once and reused in the GP estimation process, saving much time. This is lost when you have parameters to estimate in your distance matrix as you have to calculate it again every time. However, calculating distance between the levels of a nominal variable is hardly useful without prior knowledge of how the levels differ. The only sensible choice is to let \\(d(x_j,x_{j'})=0\\) when \\(j=j'\\) and \\(d(x_j,x_{j'})=c\\) when \\(j\\neq j'\\) for some constant \\(c\\). This is equivalent to using the Gower dissimilarity as the distance metric.\nAlternative distance metric, cosine dissimilarity, is based on encoding qualitative inputs with dummy variables and taking the angle between two normalized vectors representing the qualitative inputs values of two points. This returns a single distance representing the collective difference between two observations across all qualitative variables.\nThe methods I’ve studied are exchangeable correlation (EC) functions, multiplicative correlation (MC) functions, and hypersphere-based correlation (HC) functions. In EC, MC, and HC, one kernel function is used for quantitative variables and another kernel function is used for qualitative variables. The qualitative kernel functions for EC and MC are \\(EC: k_Z(z_j,z_{j'})= c\\) if \\(j\\neq j'\\) where \\(0&lt;c&lt;1\\) and \\(MC: k_Z(z_j,z_{j'})= \\exp(-(\\theta_j+\\theta_{j'}))\\) where \\(\\theta_j,\\theta_{j'} &gt;0\\) respectively. The HC method projects the correlation matrix for each qualitative variable onto a hypersphere and expresses the corresponding elements as the sum of sines and cosines of estimated parameters.\nA major difference between these three methods is the number of parameters they need to estimate. Supposing we have \\(k\\) qualitative inputs with \\(m_k\\) levels each, the EC method has \\(k\\) parameters, the MC method has \\(\\sum_k m_k\\) parameters and the HC method has \\(\\sum_k m_k(m_k-1)\\) parameters. Also noteworthy is that the HC method can accommodate negative correlations between different levels of a factor, which is possible if, for example, \\(y=\\cos(x)\\) given factor level \\(1\\) and \\(y=-cos(x)\\) given factor level \\(2\\). Given that we are working with a very large dataset, it is of interest to minimize the complexity of the approach we take.\n\n\nWEPP Qualitative Variables\nThe WEPP dataset contains a large number of qualitative variables, mostly representing the crops present and the tilling operations performed on each hillslope. There are ten crops present, or five if you combine different variants of corn and soy. The five are alfalfa, bromegrass, corn, soy, and something called ‘Tre_2932’. Multiple crops can be present on a hillslope at the same time and the slopes seem to have been measured twice: once in 1998 and once in 2013. Graphs of the number of different crops found on the same hillslope and of the prevalence of each crop are shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMost qualitative variables in this dataset have only two levels, but there are a large number of them. It is reasonable to expect interactions between factors and between factors and numeric variables. There may also be negative correlations between some factor levels, though it is hard to predict this one way or another.\n\n\nDirichlet Process Example\nThe Dirichlet process is a stochastic process of probability distributions specified by a base distribution \\(F_0\\) and a concentration parameter \\(\\alpha\\). When used as a Bayesian model to estimate the distribution generating data, \\(F_0\\) is our prior guess at the distribution and \\(\\alpha\\) controls the relative weight of the prior versus the data. When the number of data points is much greater than \\(\\alpha\\), the posterior approaches the empirical CDF of the data. We do not directly calculate the posterior though, but sample from it by generating a Dirichlet process of posterior CDFs and averaging them together.\nGiven \\(30\\) draws from a \\(N(3,1)\\) distribution and a \\(N(1,1)\\) prior, I estimated the data distribution with a Dirichlet process with \\(50\\) runs, obtaining the results shown below.\n\n\n\n\n\n\n\n\n\nSince our prior distribution is light-tailed, there is not enough data at the extremes to overrule the prior. This can be mitigated by choosing a heavy-tailed prior, such as a t-distribution with \\(df=2\\) and \\(ncp=1\\). The estimation result is shown below."
  },
  {
    "objectID": "index.html#october-15-2024",
    "href": "index.html#october-15-2024",
    "title": "Weekly Reports",
    "section": "October 15, 2024",
    "text": "October 15, 2024\n\nSummary\nThis week, I explored several papers discussing methods of handling mixed data in GPs. These methods centered around defining covariance functions for the qualitative variables, rather than using the same covariance function for qualitative and quantitative variables and defining separate distance functions.\nI tried clustering data with the PAM (Partitioning Around Medoids) method, specifying more or fewer clusters than optimal. Then I used a local expert method, fitting a GP to the data in each cluster, to train a model and make inference on an out-of-sample set.\nI also updated the simulation study of a GP with an interaction term, using the same input data each time and comparing models with interaction terms to models without. There was not a consistent relationship between the relevance of the interaction term and the size of the interaction component of the response.\n\n\nAdditive GP with Mixed Data\nI studied the additive GP model with mixed data proposed by . Starting with a response variable \\(Y\\) based on quantitative variables \\(X_j\\), \\(j=1,\\dots p,\\), and qualitative variables \\(Z_k\\), \\(k=1,\\dots,q\\), they define the model as a sum of Gaussian processes. \\[Y(X,Z)=\\mu+ GP_1(Z_1,X) +\\dots +GP_q(Z_q,X)\\] so \\(q\\) GPs are fit to \\(Y\\), each including one qualitative variable and all the quantitative variables. Then each GP follows a MVN distribution \\(N(0,\\Sigma)\\), where the kernel function of \\(\\Sigma\\) given two inputs \\(\\textbf{w}_1= (\\textbf{x}_1, \\textbf{z}_1)\\) and \\(\\textbf{w}_2=(\\textbf{x}_2, \\textbf{z}_2)\\) is \\[K(\\textbf{w}_1, \\textbf{w}_2)= \\tau^2 k_X(\\textbf{x}_1, \\textbf{x}_2|\\theta) \\prod_{k=1}^q  k_{Z_q}(\\textbf{z}_{1q}, \\textbf{z}_{2q}|\\theta)\\] where \\(k_X(\\cdot)\\) represents the covariance function between the quantitative variables and \\(k_{Z_q}(\\cdot)\\) represents the covariance function for the qth qualitative variable.\nThe authors choose to use separate covariance or kernel functions, not just separate distance functions like I had. Since the overall kernel function \\(K\\) is the product of the individual covariance functions, a value of zero for any of them could cause the entire covariance to become zero. Thus, they opted for an additive model that splits the qualitative variables among an equal number of GPs.\nFor the covariance function of a qualitative variable, the authors proposed three options. First, let \\(k_{Z_q}(\\textbf{z}_{iq}, \\textbf{z}_{jq}|\\theta_z)=\\theta_z\\), \\(0&lt;\\theta_z&lt;1\\) for \\(i\\neq j\\), called the exchangeable method. Second \\(k_{Z_q}(\\textbf{z}_{iq}, \\textbf{z}_{jq}|\\theta)=\\exp(-(\\theta_{iq}+ \\theta_{jq}))\\), \\(\\theta_{iq}&gt;0, \\theta_{jq}&gt;0\\) for \\(i\\neq j\\), called the multiplicative method. Third, a covariance function based on a hypersphere decomposition of the covariance matrix for each factor.\nThe multiplicative method is most similar to my previous approach of using the same kernel but different distance functions for qualitative and quantitative variables. However, the third method is preferred by the authors as the show that it has a higher RMSE on average. Nevertheless, I think the second method or a variation of it will be most suitable for the WEPP project. The hypersphere method favored by the authors has many parameters and needs a lot of computation time, which runs against our interests, while the exchangeable method assumes different levels of a factor always have the same ‘distance’ between them, which may not be justified.\n\n\nGP with Gower/PAM Clustering\nI tried clustering our previous training dataset with a 3-level factor using the Partitioning around Medoids method. When I specified \\(3\\) clusters, it perfectly captured the three levels of the factor input, so I also clustered the data into \\(2\\) and \\(4\\) clusters. For each set of clusters, I fit a GP to each cluster and predicted on the corresponding cluster in the training data.\nFor two clusters, the estimated lengthscales for the continuous input were \\(1.33\\) and \\(1.72\\) respectively for clusters 1 and 2. The cluster structure relative to the factor input is shown in this table.\n\n\n\nFactor\nC1\nC2\n\n\n\n\n1\n300\n0\n\n\n2\n0\n300\n\n\n3\n167\n133\n\n\n\nThe RMSE of this model on the test set was \\(9.91\\) and its estimated functions with \\(95\\%\\) confidence bands are plotted below. As indicated in the table, factor level 3 is treated the same as 1 for values below zero and the same as 2 for values roughly above zero.\n\n\n\n\n\n\n\n\n\nFor four clusters, the estimated lengthscales for the continuous input were \\(1.60\\), \\(1.38\\), \\(1.04\\), and \\(2.54\\) respectively for clusters 1 to 4. The cluster structure relative to the factor input is shown in this table.\n\n\n\nFactor\nC1\nC2\nC3\nC4\n\n\n\n\n1\n300\n0\n0\n0\n\n\n2\n0\n180\n120\n0\n\n\n3\n0\n0\n0\n300\n\n\n\nThe RMSE of this model on the test set was \\(9.06\\) and its estimated functions with \\(95\\%\\) confidence bands are plotted below. This method succeeded in estimating the functions fairly well, though the uncertainty on Factor 2 varies according to what cluster it is in."
  },
  {
    "objectID": "index.html#october-8-2024",
    "href": "index.html#october-8-2024",
    "title": "Weekly Reports",
    "section": "October 8, 2024",
    "text": "October 8, 2024\n\nSummary\nThis week, I tried clustering our previous datasets with the PAM (Partitioning Around Medoids) method. I was able to perfectly retrieve the original cluster structure of the factor input for both the one-factor and the two-factor datasets.\nI also tried generating a dataset with two continuous inputs and fitting a GP using just those two inputs and again with a third input that was the product of the other two. I repeated this with several datasets with different amounts of interaction between the inputs. My hope was that the relevance of the interaction parameter (represented by the lengthscale) would increase as the amount of interaction increased. This occurred, but the relative relevance of the interaction term was not consistent, hampering interpretability. A new approach may be needed.\nLastly, I finished coding a GP model with a custom distance function. I fit a GP with the old single-factor dataset from September 17 using the Gower distance (Euclidean distance for numeric variables and 0-1 distance for categorical variables). I then predicted on the test set of evenly spaced numeric values. The model performed very well, even better than the NLE model with a GP fit to each factor level independently.\n\n\nGP with Interaction Variable\nTo test how a Gaussian process handles interactions between inputs, I generated a dataset based on two continuous variables. I fit a GP to the data using the following kernel function.\n\\[K(x,x')= \\exp\\left(-\\left[\\frac{d(x_1,x_1')}{\\theta_1} + \\frac{d(x_2,x_2')}{\\theta_2} +\\frac{d(x_1x_2,x_1'x_2')}{\\theta_3}\\right] \\right)\\] In practice, all I needed to do was add a third variable that was the product of the first two. I then scaled all three inputs, to make their lengthscales comparable. The response depended on the inputs by the equation \\[y_i = 1.3x_{1i}+1.8x_{2i}+\\alpha\\exp(x_{1i}+x_{2i})+\\epsilon_i\\] where \\(\\epsilon_i~N(0,0.1)\\). I generated five such datasets with \\(\\alpha\\) equal to \\(0.25\\), \\(0.5\\), \\(1\\), \\(1.5\\) and \\(2\\). The MLE parameter estimates of these models were as follows:\n\n\n\nInteract\nAlpha\nX1\nX2\nX1X2\nNugget\n\n\n\n\nY\n0.25\n0.83\n49.9\n40.4\n0.0001\n\n\nN\n0.25\n0.69\n42.4\n–\n0.0001\n\n\nY\n0.5\n0.75\n49.9\n40.6\n0.0001\n\n\nN\n0.5\n0.64\n7.36\n–\n0.0001\n\n\nY\n1\n0.71\n50\n40.5\n0.0001\n\n\nN\n1\n0.57\n6.72\n–\n0.0001\n\n\nY\n1.5\n0.64\n50\n10.4\n0.0001\n\n\nN\n1.5\n0.56\n6.60\n–\n0.0001\n\n\nY\n2\n0.67\n33.4\n41.7\n0.0001\n\n\nN\n2\n0.55\n6.56\n–\n0.0001\n\n\n\nThe lengthscale of the interaction term does decrease as \\(\\alpha\\) increases, suggesting increasing relevance for that input. However, the lengthscales of the other inputs do not follow a consistent pattern, so it is difficult to judge the relevance of the interaction term by comparing it to the other terms. An interaction term in the kernel function thus may not be an effective way of measuring variable interaction within a GP.\n\n\nGP with Gower Dist\nI fit a custom Gaussian process model to a subset of the single-factor dataset with two levels of the factor similar, composed of \\(100\\) observations drawn from each factor level for a total of \\(300\\) observations. This GP used the Gower dissimilarity to quantify the difference between observations on the categorical input. If two observations matched on the factor input the distance was zero, if they differed the distance was one.\nThe lengthscales are \\(37.39\\) and \\(21.00\\) for the numeric and factor inputs respectively, and the nugget was \\(0.001\\). The RMSE for this model was \\(2.20\\), far better than any other method. The \\(95\\%\\) confidence bands for the predicted functions are plotted below."
  },
  {
    "objectID": "index.html#october-1-2024",
    "href": "index.html#october-1-2024",
    "title": "Weekly Reports",
    "section": "October 1, 2024",
    "text": "October 1, 2024\n\nSummary\nThis week I considered how we might design interpretable interactions between covariates, based on a remark by Dr. Niemi. This could be done either as part of the clustering process in a local experts method or as a modification of the Gaussian process model. I show that the Gaussian process allows for interactions between terms, but does not have any clear interpretation of them.\nI also tested my best performing methods for GPs with categorical inputs on a new dataset with two categorical inputs instead of one. The linear mean function method suffered, as the data did not follow a consistent pattern even with the factor effects removed, but the NLE method stayed effective.\nI worked on developing a GP function that calls a customizable distance function, but was not able to get it to run consistently in time for this meeting.\n\n\nCovariate Interactions in a GP\nDr. Niemi mentioned last week that he was interested in whether there was a way to make use of the fact that the similarity between levels of the factor input varied according to the value of the numeric input in last week’s dataset. I considered two ways to approach that question, first as a question of clustering/partitioning and second as a question of model fitting.\nCurrently I treat each level of the categorical input as a cluster, with no involvement from the numeric input, but this need not be the case. Clustering algorithms such as those described by Costa, Papatsouma, and Markos (see Sep. 17) partition the data based on all inputs, allowing for interactions between them. If we want to study or visualize the interactions, we could cluster in two steps: first partition the data based on the numeric inputs, then partition each cluster based on the categorical inputs. Alternatively, we could cluster the data by the numeric inputs, then cluster it again by the categorical inputs and see how the two sets of clusters overlap.\nAnother approach is to fit a Gaussian process model and study how its inputs interact within the fitted model. This is hard to do in practice because while a GP accommodates interaction between inputs with a squared exponential kernel, there is no clean way to visualize interactions between them. The expected value of a new observation is based on the correlations between it and each other observation. Changing one input will change these correlations, and the expected value, to a degree that depends on the value of the other inputs.\nTo demonstrate this, consider a squared exponential Gaussian process fit to a dataset with inputs \\(X_i=(x_{i,con},x_{i,cat}), i=1,\\dots,n\\) and response \\(Y\\), where \\(x_{con}\\) is a continuous variable and \\(x_{cat}\\) is a categorical variable. The covariance between two points \\[K(X_1, X_2)=\\exp\\left(-\\left[\\frac{||x_{1,con}-x_{2,con}||^2}{\\theta_{con}} + \\frac{||x_{1,cat}-x_{2,cat}||^2}{\\theta_{cat}} \\right] \\right)\\] \\[=\\exp\\left(-\\frac{||x_{1,con}-x_{2,con}||^2}{\\theta_{con}} \\right) \\exp\\left( -\\frac{||x_{1,cat}-x_{2,cat}||^2}{\\theta_{cat}} \\right) =k_{con}(X_1,X_2)k_{cat}(X_1,X_2)\\]\nThe covariance is then a product of functions of the squared distances between inputs. The size of the change in covariance resulting from a change in one input will thus depend on the size of the contributions of the other inputs.\nAs is, the GP provides no insight on the general nature interactions between its inputs. However, it may be possible to modify the model to draw out those interactions. Adding an interaction term inside the kernel function (or distance function) could be a way to achieve this.\n\n\nGP with Two Factor Inputs\nTo extend my previous research into categorical methods, I constructed a new dataset with two factor inputs each with two levels and one numeric input and generated a training set of 800 observations and a test set of 200. I applied three methods of addressing categorical inputs with GPs: to ignore them (Method 1), to partition the data based on the factors and fit a different GP to each combination of levels (Method 2), and to fit a linear model using the factor inputs and then fit a GP using the residuals of the linear model (Method 3). The training data is plotted below.\n\n\n\n\n\n\n\n\n\nFor Method 1, the prediction RMSE was \\(13.7\\). The maximum likelihood estimates of the parameters were \\(107.66\\) for the lengthscale of x1 and \\(0.223\\) for the nugget. For Method 2, the prediction RMSE was \\(0.72\\). The maximum likelihood estimates of the parameters were \\(2.80\\), \\(3.40\\), \\(1.32\\), and \\(1.94\\) for the lengthscales of x1 given \\((x2,x3) = \\{(1,1), (1,2), (2,1), (2,2)\\}\\). For Method 3, the prediction RMSE was \\(11.01\\). Its lengthscale MSE was \\(110.80\\) and its nugget was \\(0.203\\). The predicted vs residual plots for each method looked normal, and the estimated functions with \\(95\\%\\) confidence bands are plotted below."
  },
  {
    "objectID": "index.html#september-24-2024",
    "href": "index.html#september-24-2024",
    "title": "Weekly Reports",
    "section": "September 24, 2024",
    "text": "September 24, 2024\n\nSummary\nThis week I researched current ways of using functional inputs for clustering. There are two basic strategies I’ve found: to use a distance metric between two functions, or to convert the functions into linear combinations of basis functions and use the covariates for clustering.\nI also prepared a new method of fitting a GP with a categorical input by first fitting a linear model to the categorical input to simulate a mean function in the GP. I then compared all these methods on a new dataset that had two factor levels similar and the third different. The ranking of the different methods didn’t change, though the new mean function method performed best. I also plotted \\(95\\%\\) confidence bands of the estimated functions.\n\n\nDistances for Functional Data\nThere are a number of ways of handling functional inputs for clustering or regression purposes. One is to use a distance metric between two curves such as the Frechet distance as a basis for clustering. Another is to convert the functional input into a linear combination of basis functions or principal components and use the linear coefficients as inputs.\n suggest a clustering method using Frechet distance and show that it is competitive for functional inputs that are irregularly measured. propose a variation of basis function-based dimension reduction for cases when the response is categorical where the basis functions are chosen adaptively to maximize classification accuracy and are restricted to piecewise linear functions to improve their interpretability.\n\n\nGP with Categorical Mean Fn\nAs a fourth method to handle categorical inputs, I tried fitting a GP with a categorical mean function by first fitting a linear model with the factor variable as the predictor. Then I fit a GP using the residuals of the linear model as my response. The RMSE of this method was \\(3.15\\). The first plot below shows the predicted values versus the residuals while the second shows the \\(95\\%\\) confidence bands for the estimated function for each level.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategorical Data with Similar Levels\nI repeated the four methods described before on a new dataset where levels 1 and 2 of the factor were much closer together while level 3 was different. The new training data is plotted below.\n\n\n\n\n\n\n\n\n\nTo fit a model and perform inference, I compared the four methods used on the last data set. First, simply ignoring the categorical variable (Method 1). Second, partitioning the data set by the categorical input and fitting a different GP to each section (Method 2). Third, encoding the categorical input as dummy variables and including those in the GP (Method 3). Fourth, fitting a linear model with the factor input followed by a GP with the continuous input (Method 4).\nFor Method 1, the prediction RMSE was \\(14.49\\). The maximum likelihood estimates of the parameters were \\(3.74\\) for the lengthscale of x1 and \\(0.288\\) for the nugget. For Method 2, the prediction RMSE was \\(9.09\\). The maximum likelihood estimates of the parameters were \\(1.60\\), \\(1.28\\), and \\(2.54\\) for the lengthscales of x1 given \\(x2 = {1,2,3}\\). For Method 3, the prediction RMSE was \\(16.98\\). I was not able to get MLE estimates for its parameters. For Method 4, the prediction RMSE was \\(6.67\\) and the MLE of its lengthscale was \\(1.29\\).\nFor each method, the predicted versus residual plot on the test data is shown alongside the \\(95\\%\\) confidence band of the predicted function. Note that though the residuals for Methods 2 and 3 look to be strictly positive, the median residual for Method 2 is \\(0.04\\) and the median residual for Method 3 is \\(1.01\\). They simply have large positive errors that obscure the distribution of errors."
  },
  {
    "objectID": "index.html#august-7-2025",
    "href": "index.html#august-7-2025",
    "title": "Weekly Reports",
    "section": "August 7, 2025",
    "text": "August 7, 2025\n\nSummary\nThis week I attempted to mathematically derive the posterior predictive distribution and joint predictive distribution of the iMGPE model.\n\n\nPredictive Posterior\nLet \\(\\Omega=\\{\\alpha,\\phi,z,\\theta\\}\\) be the set of all model parameters. Suppose we fit the model to input data \\(X\\) and response \\(y\\) and now wish to predict some new response value \\(y^*\\) given the corresponding input data point \\(x^*\\). Then the predictive posterior distribution is\n\\[p(y^*|x^*,X,y)=\\int_{\\Omega} p(y^*|x^*,\\Omega)p(\\Omega|X,y) d\\Omega\\]\nwhere \\(p(y^*|x^*,\\Omega)\\) is the pdf of \\(y^*\\) given the model parameters and the new data \\(x^*\\) and \\(p(\\Omega|X,y)\\) is the posterior distribution of the model parameters given our training data. The pdf of \\(y^*\\) would then be a weighted sum of the expert predictions.\n\\[p(y^*|x^*,\\Omega) =\\sum_{j=1}^J N(\\mu_j^*,\\sigma_j^{2*})P(z_{y^*}=j)\\] The expert predictions are weighted by the probability of \\((x^*,y^*)\\) being assigned to each expert by our clustering process. Also \\[\\mu_j^*=K(x^*,X^{(j)})^TK^{-1}y^{(j)}\\text{ and } \\sigma_j^*=K(x^*,x^*)-K(x^*,X^{(j)})^T K^{-1}K(x^*,X^{(j)})\\] where \\(K\\) is the covariance matrix based on parameters \\(\\theta_j\\) and \\(X^{(j)}\\subset X\\) and \\(y^{(j)} \\subset y\\) are the data and response values associated with cluster \\(j\\).\nOur posterior distribution is proportional to the likelihood times the priors. However, it cannot be calculated directly since the component \\(p(z|\\alpha,\\phi)\\) does not have a known closed form.\n\\[p(\\Omega|X,y)\\propto p(y|z,\\theta)p(\\theta|z)p(z|\\alpha,\\phi)\\times p(\\theta)p(\\alpha)p(\\phi)\\]\nThe joint distribution of \\(y^*\\) and \\(y\\) would then be the product of predictive posterior distribution and the marginal distribution of \\(y\\).\n\\[p(y^*,y)=p(y^*|y)p(y)\\]\n\n\nBest Expert Prediction\nLast week, I found that predicting at a new point using only the expert to which it was most likely to be assigned could produce a better estimate than weighting the predictions of multiple experts, most of whom have no information on the target point. This week, I implemented best expert prediction into the iMGPE-EM algorithm with ML and the iMGPE algorithm with slice sampling. I ran each for \\(2000\\) iterations on Simulated Data Set 1 and compared the results of their predictions. Plotted below are the fitted medians and \\(95\\%\\) credible intervals of each method.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeveral noteworthy features stand out. First, the predictive accuracy of both methods improves over weighted expert prediction while the uncertainty of these predictions worsens. Second, the ML method struggles to capture the “hump” at \\(x=4\\), most likely indicating that our cluster assignment probabilities are not reliably identifying the “best” expert for those points. Third, the median line and credible interval boundaries are visibly jagged, though not to an extreme extent.\n\n\nAdjusted Prior on Phi\nAs the parameter \\(\\phi\\) controls cluster assignment probabilities, with smaller values skewing the probabilities in favor of the nearest clusters, the best expert prediction method can also be understood as the result of treating \\(\\phi\\) as going to zero during prediction. Thus, I decided to see if the gains in predictive accuracy from best expert prediction could be replicated by adjusting the prior on \\(\\phi\\) to prefer lower values.\nBy default, \\(\\phi\\) uses a log-normal prior with \\(\\mu=0\\) and \\(\\sigma^2=1\\), which has heavy tails. I substituted a light tailed \\(Gamma(1,1)\\) prior, and tested all three GP methods on simulated Data Set 1. The ML and slice sampling methods were run for \\(2000\\) iterations and the STAN method was run for \\(500\\) iterations. The fitted means for each method are plotted below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese plots show only slight improvement over using the default prior on \\(\\phi\\) in the case of slice sampling and no improvement for ML or STAN. It may be worth trying a prior on \\(\\phi\\) that is even more concentrated near zero, but I suspect the more influential factor is actually \\(\\alpha\\). A smaller \\(\\alpha\\) value leads to fewer clusters, which improves accuracy, as the function above can be captured well by even a single GP. Notably, the distribution of \\(\\alpha\\) differs significantly between the three GP methods, with STAN having a median \\(\\alpha\\) value of \\(0.82\\) and an average of \\(4.59\\) clusters, slice sampling having a median \\(\\alpha\\) value of \\(1.62\\) and an average of \\(7.83\\) clusters, and ML optimization having a median \\(\\alpha\\) value of \\(24.65\\) and an average of \\(40.6\\) clusters."
  },
  {
    "objectID": "index.html#august-21-2025",
    "href": "index.html#august-21-2025",
    "title": "Weekly Reports",
    "section": "",
    "text": "This week I studied how the iMGPE algorithm behaves when the value of \\(\\alpha\\) is held constant. I also updated my description of the posterior predictive distribution.\n\n\n\nLet \\(\\omega=\\{\\alpha,\\phi,z,\\theta\\}\\) be the set of all model parameters. Suppose we fit the model to input data \\(x\\) and response \\(y\\) and now wish to predict some new response value \\(y^*\\) given the corresponding input data point \\(x^*\\). Then the predictive posterior distribution is\n\\[p_{x,x^*}(y^*|y)=\\int_{\\Omega} p_{x^*}(y^*|\\omega)p_x(\\omega|y) d\\omega\\]\nwhere \\(p_{x^*}(y^*|\\omega)\\) is the pdf of \\(y^*\\) given the model parameters and the new data \\(x^*\\) and \\(p_x(\\omega|y)\\) is the posterior distribution of the model parameters given our training data. The pdf of \\(y^*\\) would then be a weighted sum of the expert predictions.\n\\[p(y^*|x^*,\\Omega) =\\sum_{j=1}^J N(\\mu_j^*,\\sigma_j^{2*})P(z_{y^*}=j)\\] The expert predictions are weighted by the probability of \\((x^*,y^*)\\) being assigned to each expert by our clustering process. Also \\[\\mu_j^*=K_{\\theta_j}(x^*,X^{(j)})^TK_{\\theta_j}^{-1}y^{(j)}\\text{ and } \\sigma_j^*=K_{\\theta_j}(x^*,x^*)-K_{\\theta_j}(x^*,X^{(j)})^T  K_{\\theta_j}^{-1} K_{\\theta_j}(x^*,X^{(j)})\\] where \\(K_{\\theta_j}\\) is the covariance matrix based on parameters \\(\\theta_j\\) and \\(X^{(j)}\\subset X\\) and \\(y^{(j)} \\subset y\\) are the data and response values associated with cluster \\(j\\).\nOur posterior distribution is proportional to the likelihood times the priors. However, it cannot be calculated directly since the component \\(p(z|\\alpha,\\phi)\\) does not have a known closed form.\n\\[p(\\Omega|X,y)\\propto p(y|z,\\theta)p(z|\\alpha,\\phi)\\times p(\\theta)p(\\alpha)p(\\phi)\\]\n\n\n\nTo better understand the influence of the parameter \\(\\alpha\\) on the iMGPE algorithm, I ran the algorithm while holding \\(\\alpha\\) fixed at values of \\(1\\), \\(5\\), and \\(25\\) across iterations, while allowing other paramters to vary. This was repeated for the MLE method and the slice sampling methods for fitting Gaussian process experts.\nBelow are the trace plots of \\(\\phi\\) for the MLE method with \\(\\alpha\\) fixed at values of \\(1\\), \\(5\\), and \\(25\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBelow are plotted credible intervals for the fitted functions for each level of \\(\\alpha\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is apparent that accuracy begins to dip as \\(\\alpha\\) increases, likely because larger \\(alpha\\) results in more clusters. The average number of clusters for each level was \\(7.9\\), \\(20.3\\), and \\(42.7\\).\nHere are the results for the slice sampling method, with \\(\\alpha\\) again held fixed at values of \\(1\\), \\(5\\), and \\(25\\). The trace plots of \\(\\phi\\) do not change significantly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHowever, the fitted functions and credible intervals do grow less accurate as \\(\\alpha\\) and the number of clusters increases.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the slice method, the average number of clusters given an \\(\\alpha\\) values of \\(1\\), \\(5\\), and \\(25\\) are \\(7.6\\), \\(9.3\\), and \\(11.2\\).\nPrior to this experiment, I suspected that the biggest influence on the predictive accuracy of the iMGPE algorithm was the number of clusters it generated, with fewer clusters resulting in greater accuracy. Then \\(\\alpha\\) would be the single most influential parameter, as it has a large influence on the number of clusters. This has been partially borne out, as the number of clusters does increase and accuracy does decrease as \\(\\alpha\\) increases.\nHowever, the GP fitting method also seems to have a major impact on both the number of clusters and predictive accuracy. For \\(\\alpha=25\\) the MLE method resulted in almost four times as many clusters on average as the slice method. Additionally, the MLE method was much more accurate that the slice sampling method even for \\(\\alpha=1\\), when both methods had roughly the same average number of clusters. Further investigation is needed to understand how accuracy is affected by the model parameters and structure and how it can be maximized."
  },
  {
    "objectID": "index.html#august-28-2025",
    "href": "index.html#august-28-2025",
    "title": "Weekly Reports",
    "section": "August 28, 2025",
    "text": "August 28, 2025\n\nSummary\nThis week I studied how the iMGPE algorithm behaves when the value of \\(\\alpha\\) is held constant. I also updated my description of the posterior predictive distribution.\n\n\nPosterior Predictive\nLet \\(\\omega=\\{\\alpha,\\phi,z,\\theta\\}\\) be the set of all model parameters. Suppose we fit the model to input data \\(x\\) and response \\(y\\) and now wish to predict some new response value \\(y^*\\) given the corresponding input data point \\(x^*\\). Then the predictive posterior distribution is\n\\[p_{x,x^*}(y^*|y)=\\int_{\\Omega} p_{x^*}(y^*|\\omega)p_x(\\omega|y) d\\omega\\]\nwhere \\(p_{x^*}(y^*|\\omega)\\) is the pdf of \\(y^*\\) given the model parameters and the new data \\(x^*\\) and \\(p_x(\\omega|y)\\) is the posterior distribution of the model parameters given our training data. The pdf of \\(y^*\\) would then be a weighted sum of the expert predictions.\n\\[p(y^*|x^*,\\Omega) =\\sum_{j=1}^J N(\\mu_j^*,\\sigma_j^{2*})P(z_{y^*}=j)\\] The expert predictions are weighted by the probability of \\((x^*,y^*)\\) being assigned to each expert by our clustering process. Also \\[\\mu_j^*=K_{\\theta_j}(x^*,X^{(j)})^TK_{\\theta_j}^{-1}y^{(j)}\\text{ and } \\sigma_j^*=K_{\\theta_j}(x^*,x^*)-K_{\\theta_j}(x^*,X^{(j)})^T  K_{\\theta_j}^{-1} K_{\\theta_j}(x^*,X^{(j)})\\] where \\(K_{\\theta_j}\\) is the covariance matrix based on parameters \\(\\theta_j\\) and \\(X^{(j)}\\subset X\\) and \\(y^{(j)} \\subset y\\) are the data and response values associated with cluster \\(j\\).\nOur posterior distribution is proportional to the likelihood times the priors. However, it cannot be calculated directly since the component \\(p(z|\\alpha,\\phi)\\) does not have a known closed form.\n\\[p(\\Omega|X,y)\\propto p(y|z,\\theta)p(z|\\alpha,\\phi)\\times p(\\theta)p(\\alpha)p(\\phi)\\]\n\n\nConstant Alpha\nTo better understand the influence of the parameter \\(\\alpha\\) on the iMGPE and iMGPE-EM algorithms, I ran the algorithm while holding \\(\\alpha\\) fixed at values of \\(1\\), \\(5\\), and \\(25\\) across iterations, while allowing other parameters to vary. This was repeated for the MLE method and the slice sampling methods for fitting Gaussian process experts.\nBelow are the trace plots of \\(\\phi\\) for the MLE method with \\(\\alpha\\) fixed at values of \\(1\\), \\(5\\), and \\(25\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBelow are plotted credible intervals for the fitted functions for each level of \\(\\alpha\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is apparent that accuracy begins to dip as \\(\\alpha\\) increases, likely because larger \\(\\alpha\\) results in more clusters. The average number of clusters for each level was \\(7.9\\), \\(20.3\\), and \\(42.7\\).\nHere are the results for the slice sampling method, with \\(\\alpha\\) again held fixed at values of \\(1\\), \\(5\\), and \\(25\\). The trace plots of \\(\\phi\\) do not change significantly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHowever, the fitted functions and credible intervals do grow less accurate as \\(\\alpha\\) and the number of clusters increases.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the slice method, the average number of clusters given an \\(\\alpha\\) values of \\(1\\), \\(5\\), and \\(25\\) are \\(7.6\\), \\(9.3\\), and \\(11.2\\).\nPrior to this experiment, I suspected that the biggest influence on the predictive accuracy of the iMGPE algorithm was the number of clusters it generated, with fewer clusters resulting in greater accuracy. Then \\(\\alpha\\) would be the single most influential parameter, as it has a large influence on the number of clusters. This has been partially borne out, as the number of clusters does increase and accuracy does decrease as \\(\\alpha\\) increases.\nHowever, the GP fitting method also seems to have a major impact on both the number of clusters and predictive accuracy. For \\(\\alpha=25\\) the MLE method resulted in almost four times as many clusters on average as the slice method. Additionally, the MLE method was much more accurate that the slice sampling method even for \\(\\alpha=1\\), when both methods had roughly the same average number of clusters. Further investigation is needed to understand how accuracy is affected by the model parameters and structure and how it can be maximized."
  },
  {
    "objectID": "plan.html",
    "href": "plan.html",
    "title": "Research Plan",
    "section": "",
    "text": "Chapter Concepts\n1: Compare the fully Bayesian version of iMGPE described by Rasmussen et al. with the partially Bayesian version I have used where the parameters of the GP experts are fit with maximum likelihood estimation. Compare their theoretical properties and evaluate the accuracy and precision of the estimates of each.\n2: Replace the Modified Chinese Restaurant Process presented by the authors with another, better defined, clustering step such as the Distance Dependent CRP presented by Blei and Frazier. Compare the theoretical tractability and theoretical properties of each distribution.\n3: Modify the clustering step or the GP expert step of the iMGPE algorithm to accept categorical variables as covariates. Experiment with different approaches to find the most reliable and computationally efficient method of incorporating categorical covariate information.\n4: Modify the iMGPE algorithm to handle very large data sets (large \\(n\\)). In theory, iMGPE should already handle this by partitioning the data. The authors supplement this with the rather brute force suggestion of simply forbidding the construction of clusters of size greater than some threshold. We could study the effectiveness of the current algorithm and explore alternative methods of restricting maximum cluster size or requiring a minimum number of clusters. Another direction is to include big data methods such as Vecchia approximation in the fitting of GP experts, so that we can handle very large clusters.\n5: Incorporate deep GP experts into the iMGPE algorithm. I am of two minds regarding the utility of this modification. The practical benefit of deep GPs is that multiple layers of GPs can fit to non-Gaussian models, handling data features like outliers, discontinuities, and heteroskedasticity better than a single GP. The iMGPE algorithm was already supposed to help with these same issues by partitioning the data in each iteration into clusters so that the data in each cluster is appropriate for a single GP. On the other hand, it is not certain that iMGPE is as effective at this as deep GPs are or that we could not improve both methods by combining them.\n6: Modify the iMGPE algorithm to include a variable selection step. This is often done in a Bayesian way by using a vector of Bernoulli parameters \\(\\pi_{1:p}= (\\pi_1,\\dots, \\pi_p)\\sim \\prod_{j=1}^p Bern(\\gamma_j)\\) to represent which of \\(p\\) variables are selected. My first thought was to draw a new instance of \\(\\pi_{1:p}\\) for each cluster so that we could let different variables be relevant in different parts of the data. However, as clusters do not carry over between iterations of the algorithm, we would need to make sure the conditional distributions for the \\(\\pi_{1:p}\\) are dependent on the input values.\n7: Modify the iMGPE algorithm to accept functional inputs or outputs. Identify or develop a reasonable method of measuring the distance between two functions and of clustering similar functions. Consider whether functional inputs should be handled differently for the purposes of clustering and GP expert sampling."
  },
  {
    "objectID": "index.html#september-11-2025",
    "href": "index.html#september-11-2025",
    "title": "Weekly Reports",
    "section": "September 11, 2025",
    "text": "September 11, 2025\n\nSummary\nThis week I studied the Monte Carlo Expectation Maximization algorithm and compared it to my variant of iMGPE with MLE updates on \\(\\theta\\). The two algorithms are actually very similar, if not identical, but serve different theoretical purposes.\nI also lay out several new research concepts on the “Research Plans” page.\n\n\niMGPE-EM and MCEM\nWhile the authors of the iMGPE algorithm formulated it in purely Bayesian terms, I had coded a variation of it that performed MLE updates on the GP parameters while keeping to sampling steps for all other parameters. I call this method iMGPE-EM, as we will find that it is an EM algorithm. This raises the question of whether such an approach has any interesting properties or any similarities to more established methods.\nThe most similar existing algorithm to this Monte Carlo plus Maximum Likelihood approach is the Monte Carlo Expectation Maximization algorithm or MCEM. The Expectation Maximization algorithm is an iterative process to find MLE or MAP estimates of data parameters. It works by assuming that your response \\(Y\\) and your observed data \\(X\\) can be supplemented by some unobserved data (or latent variables) \\(Z\\) that, in conjunction with \\(X\\), helps to explain \\(Y\\) via a vector of unknown parameters \\(\\theta\\). The algorithm then iterates between an expectation step, in which we define an objective function as a conditional expectation, and a maximization step, in which we maximize this function over \\(\\theta\\). Given \\(\\theta^{(k-1)}\\) as the previous value for \\(\\theta\\), the k-th update steps are to define \\(Q(\\theta|\\theta^{(k-1)})\\) and then maximize it.\nThe E step:\n\\[Q(\\theta | \\theta^{(k-1)}) = E\\left[\\log(p(\\theta|Z,Y)p(Z|\\theta^{(k-1)},Y))\\right]\\]\nThe M step:\n\\[\\theta^{(k)} = \\text{argmax}_{\\theta} \\text{ } Q(\\theta | \\theta^{(k-1)})\\]\nThe MCEM algorithm is a variation of EM where the conditional expectation of the E step is replaced with a Monte Carlo average. At each iteration, the researcher draws a random sample from \\(p(Z|\\theta^{(k-1)},Y)\\) and defines\n\\[\\hat Q(\\theta| \\theta^{(k-1)}) =\\sum_{b=1}^B w_b\\log(p(\\theta|Z_b,Y)p(Z_b|\\theta^{(k-1)},Y))\\]\nbased on weights \\(w_b\\) (for iid sampling, we usually take all \\(w_b=1/B\\)) and maximize it as normal.\nThere are some major differences between MCEM and iMGPE-EM. MCEM produces a sequence of parameter values that asymptotically approach the MLE. MCEM is not really a Bayesian method as \\(\\theta\\) is not treated as a random variable, has no prior, and is estimated via a point value rather than a posterior distribution. But iMGPE-EM (in the context of the iMGPE algorithm) is meant to produce samples from the model’s joint posterior distribution. It is partially Bayesian as the non-GP parameters do have priors and posteriors.\nHowever, there are also significant similarities. If we took the iMGPE model, defining the spatial data \\(X\\) as our observed data, the cluster assignment structure \\(z\\) as our unobserved data (perhaps in combination with \\(\\alpha\\) and \\(\\phi\\)), and the GP expert parameters \\(\\theta\\) as our parameters of interest, then the resulting MCEM algorithm would be very similar to our iMGPE-EM algorithm. At each iteration of MCEM, we draw a sample of size \\(B=1\\) from the conditional distributions of \\(\\alpha\\), \\(\\phi\\), and \\(z\\) and then maximize \\(\\log(p(\\theta|Z,y)p(Z|\\theta^{(k-1)},y))\\) This is equivalent to one iteration of iMGPE-EM, which maximizes \\(p(\\theta|Z,y)\\).\nThe iMGPE-EM method can be considered an MCEM algorithm, but that leaves the question of whether approaching the model in this way is really useful or desirable. The MCEM algorithm is intended to find MLEs for some parameter vector of interest. Here, we are less interested in the parameters of the GP experts and more in the posterior and posterior predictive distributions for \\(y\\).\n\n\nDD-CRP iMGPE Implementation\nI have previously developed code for a version of the iMGPE algorithm that uses a Distance Dependent Chinese Restaurant Process for its clustering step. I trained it for \\(5000\\) iterations on the simulated data set 1 and examined its output. This method tended to put all points into a single cluster, causing its \\(\\alpha\\) parameter to be very small and its \\(\\phi\\) parameter to grow without converging. Presumably, \\(\\phi\\) increased out of control since it has no significance in the context of a single cluster.\nA plot of the true and estimated functions is shown below. The credible interval for the estimate is not shown as the data was almost always in a single cluster and thus there is no uncertainty in the estimate."
  },
  {
    "objectID": "index.html#september-18-2025",
    "href": "index.html#september-18-2025",
    "title": "Weekly Reports",
    "section": "",
    "text": "This week I concluded that iMGPE algorithm with ML estimation of the Gaussian process experts is an EM algorithm that I named iMGPE-EM, while iMGPE refers to the fully Bayesian implementation described by Rasmussen and Ghahramani. I have updated the terminology used in the past few weeks accordingly.\nI researched ways of incorporating dependence on covariates into Bayesian nonparametric models and consider the advantages and disadvantages of the Distance Dependent Chinese Restaurant Process.\n\n\n\nAfter carefully reviewing the literature on the MCEM algorithm, referring largely to the paper “A Review of Monte Carlo-based Versions of the EM Algorithm” by William Ruth, I have concluded that the iMGPE algorithm with MLE estimation of the GP experts is indeed a special case of the MCEM algorithm applied to Rasmussen et al’s model. Henceforth, the iMGPE with MLE algorithm will be referred to as iMGPE-EM.\nAs described in last week’s analysis, iMGPE-EM can be construed as an MCEM algorithm that generates 1 sample of the latent variables for each iteration to estimate its objective function. The objective function for MCEM is \\(\\log(p(\\theta|Z,y)p(Z|\\theta^{(k-1)},y))\\) while that of iMGPE-EM is \\(p(\\theta|Z,y)\\). I realized, upon examination, that since \\(\\theta^{(k-1)}\\) and \\(y\\) are fixed quantities, \\(p(Z|\\theta^{(k-1)},y)\\) is also fixed and the maximization of these two functions is equivalent.\nI briefly considered looking into fully Bayesian implementations of the MCEM algorithm, and found that such implementations were called variational inference or variational Bayes. Variational inference, in fact, a variational inference expansion of iMGPE has already come up in my research. In the report for February 13, 2025 I discussed the paper “Variational Inference for Infinite Mixture of Gaussian Processes” by Sun and Xu which presents a generative model for iMGPE and uses a variational inference method called mean field approximation to estimate the model parameters.\n\n\n\nThis week I examined a paper by Nicholas Foti and Sinead Williamson titled A Survey of Non-Exchangeable Priors for Bayesian Nonparametric Models. The authors note that the construction of a Bayesian nonparametric model for exchangeable data generally works as follows. Given exchangeable data \\(Z_{1:n}\\) controlled by a potentially infinite vector of parameters \\(\\theta\\), de Finetti’s Theorem tells us that\n\\[P(Z_1=z_1, Z_2=z_2,\\dots, Z_n=z_n) = \\int \\prod_{i=1}^n Q_{\\theta}(Z_i=z_i) p(d\\theta)\\]\nWhere \\(\\{Q_{\\theta}, \\theta\\in \\Theta\\}\\) is a family of conditional distributions and \\(p(d\\theta)\\) is a probability measure over \\(\\theta\\) (which could be a pdf or pmf) called the de Finetti mixing measure.\nFor example, taking the de Finetti mixing measure to be the Dirichlet distribution, then it generates a finite vector of locations and weights \\(\\theta\\), which defines a multinomial distribution \\(Q_{\\theta}\\) which generates new observations of \\(Z\\), which in this case is a random partition over \\(n\\) points. In practice we often integrate over the de Finetti mixing measure, dealing only with the predictive distribution \\(p(Z_{n+1}|Z_1,\\dots, Z_n)\\). Here, \\[p(Z_{n+1}=\\theta_k| Z_{1:n})= \\begin{cases}\n\\frac{n_k}{n+\\alpha} \\text{ if } n_k&gt;0 \\\\\n\\frac{\\alpha}{n+\\alpha} \\text{ otherwise}\n\\end{cases}\\]\nWhere \\(n_k\\) is the number of points assigned to cluster \\(k\\).\nDependence on covariates can be injected into this process at several different points. It can be added to the de Finetti mixture measure, or to the conditional distributions that generate observations or directly into the predictive distribution of new data points in \\(Z\\).\nThe authors discuss the Distance Dependent CRP as an example of covariate dependence through the predictive distribution. They note that it is very flexible, allowing for the modeling of many relationships between data points, but lacks the property of marginal invariance, meaning that marginalizing over a given data point results in a different distribution than if that point was not included in the model at all.\nMarginal invariance is discussed by Blei and Frazier who argue that this property is reasonable in some cases but inappropriate for others. Knowing that two people have a common friend affects our belief in them sharing shopping habits, for example, even if we don’t observe the habits of the common friend. While this property is not particularly useful for spatial-temporal WEPP data, it is not clear either that it is harmful. A lack of marginal invariance does not affect predictive accuracy, which is our primary concern."
  },
  {
    "objectID": "index.html#september-25-2025",
    "href": "index.html#september-25-2025",
    "title": "Weekly Reports",
    "section": "September 25, 2025",
    "text": "September 25, 2025\n\nSummary\nThis week I concluded that iMGPE algorithm with ML estimation of the Gaussian process experts is an EM algorithm that I named iMGPE-EM, while iMGPE refers to the fully Bayesian implementation described by Rasmussen and Ghahramani. I have updated the terminology used in the past few weeks accordingly.\nI researched ways of incorporating dependence on covariates into Bayesian nonparametric models and consider the advantages and disadvantages of the Distance Dependent Chinese Restaurant Process.\n\n\niMGPE-EM and MCEM\nAfter carefully reviewing the literature on the MCEM algorithm, referring largely to the paper “A Review of Monte Carlo-based Versions of the EM Algorithm” by William Ruth, I have concluded that the iMGPE algorithm with MLE estimation of the GP experts is indeed a special case of the MCEM algorithm applied to Rasmussen et al’s model. Henceforth, the iMGPE with MLE algorithm will be referred to as iMGPE-EM.\nAs described in last week’s analysis, iMGPE-EM can be construed as an MCEM algorithm that generates 1 sample of the latent variables for each iteration to estimate its objective function. The objective function for MCEM is \\(\\log(p(\\theta|Z,y)p(Z|\\theta^{(k-1)},y))\\) while that of iMGPE-EM is \\(p(\\theta|Z,y)\\). I realized, upon examination, that since \\(\\theta^{(k-1)}\\) and \\(y\\) are fixed quantities, \\(p(Z|\\theta^{(k-1)},y)\\) is also fixed and the maximization of these two functions is equivalent.\nI briefly considered looking into fully Bayesian implementations of the MCEM algorithm, and found that such implementations were called variational inference or variational Bayes. Variational inference, in fact, a variational inference expansion of iMGPE has already come up in my research. In the report for February 13, 2025 I discussed the paper “Variational Inference for Infinite Mixture of Gaussian Processes” by Sun and Xu which presents a generative model for iMGPE and uses a variational inference method called mean field approximation to estimate the model parameters.\n\n\nClustering Papers\nThis week I examined a paper by Nicholas Foti and Sinead Williamson titled A Survey of Non-Exchangeable Priors for Bayesian Nonparametric Models. The authors note that the construction of a Bayesian nonparametric model for exchangeable data generally works as follows. Given exchangeable data \\(Z_{1:n}\\) controlled by a potentially infinite vector of parameters \\(\\theta\\), de Finetti’s Theorem tells us that\n\\[P(Z_1=z_1, Z_2=z_2,\\dots, Z_n=z_n) = \\int \\prod_{i=1}^n Q_{\\theta}(Z_i=z_i) p(d\\theta)\\]\nWhere \\(\\{Q_{\\theta}, \\theta\\in \\Theta\\}\\) is a family of conditional distributions and \\(p(d\\theta)\\) is a distribution over \\(\\Theta\\) called the de Finetti mixing measure.\nFor example, taking the de Finetti mixing measure to be the Dirichlet distribution, then it generates a finite vector of locations and weights \\(\\theta\\), which defines a multinomial distribution \\(Q_{\\theta}\\) which generates new observations of \\(Z\\), which in this case is a random partition over \\(n\\) points. In practice we often integrate over the de Finetti mixing measure, dealing only with the predictive distribution \\(p(Z_{n+1}|Z_1,\\dots, Z_n)\\). Here, \\[p(Z_{n+1}=\\theta_k| Z_{1:n})= \\begin{cases}\n\\frac{n_k}{n+\\alpha} \\text{ if } n_k&gt;0 \\\\\n\\frac{\\alpha}{n+\\alpha} \\text{ otherwise}\n\\end{cases}\\]\nWhere \\(n_k\\) is the number of points assigned to cluster \\(k\\).\nDependence on covariates can be injected into this process at several different points. It can be added to the de Finetti mixture measure, or to the marginal distributions \\(Q_{\\theta},\\) or directly into the predictive distribution of new data points in \\(Z\\). Note that the de Finetti mixture measure is a discrete distribution, so it is defined by the locations of its point masses and the weights or probabilities associated with each location. Covariates can be included to induce dependence either among the weights or among the locations of the point masses.\nThe authors recommend different dependence strategies depending on how you expect to see covariates influence your data. If the values of some parameters are expected to change with a covariate, then the locations of the de Finetti mixture measure can be made dependent while its weights are held constant. If you expect the proportions of the latent components to vary across the covariate space, then holding the locations fixed while the weights are made dependent is appropriate. Lastly, if you expect that observations will be similar to observations at nearby covariates, then introducing dependence through the marginal distributions \\(Q_{\\theta}\\) or the predictive distributions \\(p(Z_{n+1}|Z_{1:n})\\) is the best way to accomplish this.\nThe authors discuss the Distance Dependent CRP as an example of covariate dependence through the predictive distribution. They note that it is very flexible, allowing for the modeling of many relationships between data points, but lacks the property of marginal invariance, meaning that marginalizing over a given data point results in a different distribution than if that point was not included in the model at all.\nMarginal invariance is discussed by Blei and Frazier who argue that this property is reasonable in some cases but inappropriate for others. Knowing that two people have a common friend affects our belief in them sharing shopping habits, for example, even if we don’t observe the habits of the common friend. While this property is not particularly useful for spatial-temporal WEPP data, it is not clear either that it is harmful. A lack of marginal invariance does not affect predictive accuracy, which is our primary concern."
  },
  {
    "objectID": "index.html#october-9-2025",
    "href": "index.html#october-9-2025",
    "title": "Weekly Reports",
    "section": "October 9, 2025",
    "text": "October 9, 2025\n\nSummary\nThis week I experimented with the properties of distance-dependent Chinese restaurant process by itself and when combined with the iMGPE algorithm.\n\n\niMGPE-DDEM\nThis week, I studied the iMGPE algorithm with distance-dependent CRP clustering and maximum likelihood estimation of each GP expert, or iMGPE-DDEM. I tested it on two new simulated data sets and Simulated Data 1 in order to see how well it identifies the ‘true’ cluster structure of a data set. The new data sets are plotted below, along with their mean functions, and are both step functions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI fit each data set using \\(1000\\) iterations (a small number but it will turn out not to matter). The estimated mean function for each is plotted below in red, while the true mean function is plotted in blue.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt was not possible to include credible intervals for the mean because the model rapidly converged to putting all data points into a single cluster for every data set. Hence, there was no variation in the predictive posterior after the first few iterations. The posterior distribution of \\(\\alpha\\) was centered around \\(0.3\\) for all fitted models, indicating that the algorithm had settled on a single cluster. This was clearly not desirable behavior for the two step function data sets, so I began investigating the distance-dependent CRP in isolation to see how it behaved.\n\n\nDDCRP Clustering\nI then chose to study the DDCRP clustering algorithm in isolation. Using a Gaussian decay function of \\(f(x,x')=\\exp(-(x-x')/\\theta)\\) with \\(\\theta=1\\) and concentration parameter \\(\\alpha=1\\), I ran just the clustering algorithm on the three simulated data sets for \\(100\\) iterations each.\nFor all data sets, DDCRP converged quickly to a ‘steady state’ with a fairly, or even exactly, constant number of clusters. The first data set was fit almost entirely in a single cluster. The second was also fit into clusters encompassing the whole breadth of the data set. Only the third was divided into two groups of clusters in a visually sensible way. Plotted below are the three data sets with points colored by their final cluster assignments.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI experimented with different values of \\(\\alpha\\) and \\(\\phi\\) but the results were disappointing. While larger \\(\\alpha\\) values and smaller \\(\\phi\\) values were associated with slightly more small clusters (\\(&lt;10\\) points), no difference was made in the overall structure.\n\n\nConclusions\nIt seems as though data will be clustered together so long as they are contiguous in the x-dimension. Only very high values of \\(\\alpha\\) drive up the number of clusters generated and even then there will often still be one cluster that contains most of the points in the data set. In addition, the DDCRP tends to quickly reach an ‘optimal’ cluster structure and deviate little from that in all subsequent iterations.\nThis has both good and bad consequences for prediction. In the data sets we have been working with so far, a single Gaussian process can fit the data fairly well, so putting all points in a single cluster may actually be an improvement over having multiple clusters. However, the DDCRP tends toward large clusters even when this is clearly not ideal, as with Data Set 2. In addition, one of the advantages we hoped to gain from clustering the data was that large data sets would be broken up into manageable pieces, reducing computation time. The DDCRP’s preference for large clusters results in little computational improvement.\nChanging parameter values had surprisingly little effect on the overall behavior of the algorithm, making this a difficult issue to resolve. Single clustering might be made less likely as we employ higher dimensional data. However, it may be necessary to look for clustering methods with better properties."
  },
  {
    "objectID": "dictionary.html#simulated-data-set-2",
    "href": "dictionary.html#simulated-data-set-2",
    "title": "Data Dictionary",
    "section": "Simulated Data Set 2",
    "text": "Simulated Data Set 2\nColumn \\(x\\) is a random draw from a uniform distribution on the interval \\((0,5)\\). Column \\(ytrue\\) is generated from the \\(x\\) values by a step function: \\[f(x)= \\begin{cases}\n0\\text{ if } x\\leq 1.5\\\\\n2\\text{ if } 1.5&lt;x\\leq 3\\\\\n5\\text{ if } x &gt; 3\n\\end{cases}\\]\nColumn \\(y\\) is the \\(ytrue\\) column plus \\(N(\\mu=0,\\sigma^2=0.04)\\) random noise."
  },
  {
    "objectID": "dictionary.html#simulated-data-set-3",
    "href": "dictionary.html#simulated-data-set-3",
    "title": "Data Dictionary",
    "section": "Simulated Data Set 3",
    "text": "Simulated Data Set 3\nColumn \\(x\\) consists of \\(50\\) points generated from a \\(N(\\mu=-3,\\sigma^2=0.5)\\) distribution and \\(50\\) points generated from a \\(N(\\mu=3,\\sigma^2=0.5)\\) distribution. Column \\(ytrue\\) is generated from the \\(x\\) values by a step function: \\[f(x)=\\begin{cases}\n1\\text{ if } x&lt;= 0\\\\\n5\\text{ if } x&gt;0\n\\end{cases}\\]\nColumn \\(y\\) is the \\(ytrue\\) column plus \\(N(\\mu=0,\\sigma^2=0.04)\\) random noise."
  },
  {
    "objectID": "index.html#october-16-2025",
    "href": "index.html#october-16-2025",
    "title": "Weekly Reports",
    "section": "October 16, 2025",
    "text": "October 16, 2025\n\nSummary\nThis week I studied the DDCRP and the iMGPE to understand their underlying clustering distributions. Both, I found, use arbitrary modifications of the posterior conditionals that prevent us from discerning what base distribution might have given rise to them.\nI then compared the iMGPE model to two closely related models with better theoretical tractability to highlight the trade-offs associated with our known model options.\n\n\nLimits of DDCRP Theory\nI studied the description of the Distance Dependent Chinese Restaurant Process given by Blei and Frazier to understand the model from which their process originated, but ultimately concluded that it had not been built with an original model in mind. A standard CRP is an algorithm for generating a partition over a number of points. It can be seen as the marginal distribution of a Dirichlet process, which is defined by a base measure (or distribution) \\(G\\) and a parameter \\(\\alpha\\). To fully define a CRP then, you must define the base distribution that parameterizes its underlying Dirichlet process and associate each cluster generated by the CRP with one draw from the base distribution.\nAssuming the points are exchangeable, the posterior probabilities of assigning a new point \\(y\\) to a current or new cluster \\(C\\) are as follows:\n\\[p(y\\to C_i)= \\begin{cases}\n\\frac{n_i}{n+\\alpha}\\text{ for existing clusters } C_i\\\\\n\\frac{\\alpha}{n+\\alpha}\\text{ for a new cluster}\n\\end{cases}\\]\nwhere \\(n_i\\) is the number of points in cluster \\(C_i\\). These probabilities are what is used, in computation, to generate a draw from a CRP.\nWhat Blei and Frazier did for the Distance Dependent Chinese Restaurant Process, and what Rasmussen and Ghahramani did for the Modified Chinese Restaurant Process, is modify these posterior probabilities directly to be dependent on some known covariates \\(X\\). This makes the resulting distribution non-exchangeable.\nConsequently, we cannot “go backwards” from this algorithm to the Dirichlet process that underlies it. As the original probabilities were derived under an assumption of exchangeability, the underlying process may not be a Dirichlet process at all.\n\n\niMGPE vs DPGMM vs SDP\nTo better understand the iMGPE model, I compared it to two other models: the Dirichlet Process Gaussian Mixture Model (DPGMM) and the Spatial Dirichlet Process (SDP). I found that the iMGPE is a modified version of the DPGMM. It makes the mixture components covariate dependent but no longer has an identifiable Dirichlet process prior on its components. The Spatial DP also accommodates covariate dependence but is much more computationally intensive, as it fits a GP over the entire data set.\nThe Dirichlet Process Gaussian Mixture Model is an infinite mixture of Gaussian process components with a Dirichlet process prior. The Dirichlet process partitions the data into clusters and a GP is fit to each cluster. The hierarchical form of this model for data \\(y\\) and vector of indicator variables \\(z\\) is:\n\\[p(y|z,\\mu,\\Sigma)=\\prod_{j=1}^{J_z} N_{C_j}(\\mu_j,\\Sigma_j)\\] \\[\\mu_j,\\Sigma_j\\stackrel{ind}{\\sim} G \\text{ for } j=1,\\dots,J_z\\] \\[z\\sim DP(\\alpha G)\\] where \\(C_j\\) is the number of points in cluster \\(j\\) given \\(z\\) and \\(J_z\\) is the number of clusters.\nI also considered the spatial Dirichlet process by Gelfand et al as a spatially dependent clustering process. In a spatial Dirichlet process, the base measure of the DP is a spatially dependent multivariate normal distribution. A draw from this DP is a discrete dist over many different realizations of the base measure over the set of locations. Then draw a vector \\(\\theta\\) from this discrete dist, which is the values of one specific draw from the base measure at your locations. Then your response \\(Y\\) is the mean vector \\(\\theta\\) plus error with fixed variance. Note that you need multiple observations of \\(Y\\) at each location so that you can observe multiple draws from the distribution of \\(\\theta\\) in order to estimate the parameters. The hierarchical distribution is\n\\[y_t|\\theta_t,\\tau^2 \\stackrel{ind}{\\sim} N(\\theta_t, \\tau^2I_n), t=1,\\dots,T\\] \\[\\theta_t\\stackrel{iid}{\\sim} G^{(n)}, t=1,\\dots,T\\] \\[G^{(n)}|\\alpha,\\sigma^2,\\phi \\sim DP(\\alpha G_0^{(n)})\\] \\[G_0^{(n)}(\\cdot|\\sigma^2,\\phi) = N_n(\\cdot|0,\\sigma^2H_n(\\phi))\\] where \\(t\\) indexes the observations at each location and \\(G_0^{(n)}\\) is a Gaussian process with covariance function parameterized by \\(\\phi\\) over our observed locations.\nIn conclusion, we will probably need to make some kind of trade-off to proceed. With the iMGPE we have a dependent clustering process in our model but cannot express the distribution of that process. With the spatial DP, we have a fully enumerated, covariate dependent process, but we will struggle to compute this model for large data sets. We may have to devise some new DP variant that generates dependent partitions or we may have to come to terms with an incomplete model specification.\n\n\n\nModel\nSpatial Dependence\nKnown Dists\nViable for Large N\n\n\n\n\niMGPE\nY\nN\nY\n\n\nDPGMM\nN\nY\nY\n\n\nSPD\nY\nY\nN"
  },
  {
    "objectID": "index.html#october-30-2025",
    "href": "index.html#october-30-2025",
    "title": "Weekly Reports",
    "section": "October 30, 2025",
    "text": "October 30, 2025\n\nSummary\nThis week, I completed a simulation study comparing the iMGPE-EM algorithm to the iMGPE-Slice algorithm on 20 randomly generated data sets. I found that the iMGPE-EM algorithm tended to converge to one of two different states, with distinct parameter values and cluster counts. The algorithm appears to be numerically unstable.\nI also studied Brook’s lemma as a means of describing the probability mass function of the cluster assignment vector \\(z\\). The resulting expression does not simplify well and I am not sure if the pmf can be identified in any useful form.\n\n\nBrook’s Lemma\nLet \\(Z\\) be the vector of randomly generated indicator variables representing cluster assignments, with \\(z\\) and \\(z'\\) being two possible values of this vector. Given the parameter values \\(\\alpha\\) and \\(\\phi\\), Brook’s lemma states that\n\\[\\frac{p(Z=z|\\alpha,\\phi)}{p(Z=z'|\\alpha,\\phi)} = \\frac{P(Z_1=z_1|z_{-1})P(Z_2 = z_2|z_{-2})\\dots P(Z_n=z_n| z_{-n})}{P(Z_1=z_1'|z'_{-1})P(Z_2 = z_2'|z'_{-2})\\dots P(Z_n=z_n'| z'_{-n})}\\]\nFrom the conditional distributions described on the “iMGPE” page, we have\n\\[P(z_i=j|z_{-i},y_i,\\dots)\\propto \\begin{cases}\n\\frac{n-1}{n+\\alpha-1}\\frac{\\sum_{i'\\neq i,z_{i'}=j} K_{\\phi}(X_i,X_{i'})}{\\sum_{i'\\neq i} K_{\\phi}(X_i,X_{i'})} \\text{ for existing clusters}\\\\\n\\frac{\\alpha}{n+\\alpha-1} \\text{ for new clusters}\n\\end{cases}\\]\nNote that the probability takes two possible forms, depending on whether \\(z_i\\) is in a cluster by itself (and thus a “new” cluster given \\(z_{-i}\\)) or in a cluster with other points. Also, the \\(n+\\alpha-1\\) component will cancel from all terms. Let \\(s\\subset z\\) and \\(s'\\subset z'\\) be the subsets of points in clusters by themselves and \\(|s|\\) be the cardinality of those sets. Then, Brook’s lemma becomes\n\\[\\frac{p(Z=z|\\alpha,\\phi)}{p(Z=z'|\\alpha,\\phi)} = \\frac{\\prod_{i\\notin s} \\left(\\sum_{i'\\neq i,z_{i'}=z_i} K_{\\phi}(x_i,x_{i'}) / \\sum_{i'\\neq i}K_{\\phi}(x_i,x_{i'})\\right)}{\\prod_{i\\notin s'} \\left(\\sum_{i'\\neq i,z'_{i'}=z'_i} K_{\\phi}(x_i,x_{i'})/ \\sum_{i'\\neq i} K_{\\phi}(x_i,x_{i'}) \\right)} (n-1)^{|z|-|z'|+|s'|-|s|}\\alpha^{|s|-|s'|}\\]\nI do not think this can be simplified any further without additional assumptions. The \\(\\sum_{i'\\neq i} K_{\\phi}(x_i,x_{i'})\\) term will cancel whenever \\(z_i\\) and \\(z'_i\\) are both not in singleton clusters. We could simplify this expression significantly if we assume that all clusters have at least two points, but this seems like an artificial limitation.\n\n\nSimulation Study\nTo more accurately assess the differences in performance between my iMGPE fitting methods, I conducted a simulation study across 20 randomly generated data sets of 100 points each. I fit each data set using the iMGPE-EM algorithm and the iMGPE-Slice algorithm using \\(2000\\) iterations each.\nEach data set was generated in three steps. First, I drew 100 points from a \\(Unif(0,5)\\) distribution as our \\(X\\) input variable. Second, I drew three values \\(a\\), \\(b\\), and \\(c\\) from the intervals \\([0,3]\\), \\([-1,1]\\), and \\((0,2]\\). These serve as parameters that define the mean function \\(g(\\cdot)\\) for the data set. \\[g(x)=(a+bx)I(x\\leq 2) + \\cos(c\\pi x)I(x &gt; 2)\\] Given this, I generated \\(Y_i=g(X_i)+\\epsilon_i\\) where the \\(\\epsilon_i\\) are independent \\(N(0,0.2^2)\\) variables. This results in a number of data sets with discontinuities around 2.\nThe predictive RMSE for the two methods are compared below, where the RMSE has been divided by the range of its data set. You can see that the data sets fall into two groups: those where iMGPE-EM significantly outperformed iMGPE-Slice and those where it only slightly outperformed iMGPE-Slice.\n\n\n\n\n\n\n\n\n\nThe same groups are evident in a plot of the average number of clusters for each method as well as a plot of the average width of a \\(95\\%\\) pointwise credible interval for the mean. Sometimes iMGPE-EM converges to a state with \\(\\alpha \\approx 0.3\\), one to two clusters, and a predicted mean function that hugs the true mean. Other times, it converges to a state with \\(\\alpha\\approx 30\\), 40 to 50 clusters, and a predicted mean function nothing like the true mean. iMGPE-Slice always ends up in the second group.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe two groups do not appear to be an artifact of the data itself. I saw no distinguishing characteristics between the data sets in each group. More likely, the numerical stability of the algorithm is at fault. Given a starting value of \\(\\alpha=1\\), the algorithm can converge to either \\(0.3\\) or \\(30\\) with roughly equal probability.\nI will be rerunning the simulation with alternate starting values of \\(\\alpha\\) and \\(\\phi\\) to see if that is the source of the issue.\n\n\nFuture Work: Categorical Inputs\nMy plan for including categorical variables in the iMGPE model is to investigate the following research question. Do we need to incorporate categorical variables into the GP experts themselves or is it sufficient to incorporate them into the clustering distribution?\nThe rationale is that existing methods to incorporate categorical variables into Gaussian processes require the estimation of many additional parameters, increasing computational complexity. This would be exacerbated in an infinite mixture model, where we are fitting many GP experts every iteration. Thus, we could save time by only including them in the clustering step and fitting the GP experts only on the continuous variables.\nTO evaluate this, I need to code a version of iMGPE that accepts categorical inputs in the clustering step, and one that accepts them in both the clustering step and the GP experts. Progress is steady here."
  },
  {
    "objectID": "index.html#november-6-2025",
    "href": "index.html#november-6-2025",
    "title": "Weekly Reports",
    "section": "November 6, 2025",
    "text": "November 6, 2025\n\nSummary\nThis week I experimented with fitting models in parallel and evaluating their convergence with the Geweke diagnostic. Many of my models were not converging, so I will need to redo the analysis with more iterations.\nI developed a new theory as to why so many of the estimated models approach a state where \\(\\alpha\\) is very large and the data is partitioned into 40 to 50 clusters of 2 or 3 points each. The MCRP tends to generate more clusters than the regular CRP, causing the Gibbs update for \\(\\alpha\\) to overestimate its value, creating even more clusters and runaway inflation in \\(\\alpha\\) and the number of clusters. This is borne out in simulations of the conditional distributions of \\(z\\) and \\(\\alpha\\).\nI also began experimenting with different ways to include categorical inputs in an iMGPE model. Two different methods are tested, with one appearing clearly superior.\n\n\nSimulation Study\nThis week I experimented with parallel processing to fit iMGPE models more quickly. The university’s Nova dashboard allows me to reserve a maximum of 32 cores for an 8 hour session. As the slice sampling models take around 100 minutes to fit and require 8 cores each, I can fit, at most, 16 iMGPE-Slice models in one 8 hour session with four running in parallel at once.\nI never got all 16 to run successfully, however. I experienced repeated troubles with models failing to save and dropped connections between clusters. Still, I was able to add another 20 iMGPE-Slice models to my simulation study, with an equal number of iMGPE-EM models.\nI tested convergence of the chains for \\(\\alpha\\) and \\(\\phi\\) using the Geweke diagnostic, which compares the first \\(10\\%\\) of the chain after burn-in to the last \\(50\\%\\) to test whether they came from the same distribution. For both model types, around half of the estimated models were flagged as not converged by the Geweke diagnostic. Rerunning the models to convergence will take more time, but in the meantime I have revised the plots of model comparisons on RMSE and number of clusters for the 22 iMGPE-EM models that did converge for \\(\\alpha\\) and their matching iMGPE-Slice models.\nThe improved plots of RMSE and number of clusters by model type is shown below. As before, the RMSE values have been scaled by the range of their respective data sets. Red represents EM models and blue represents slice sampler models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe same two groups are visible on the iMGPE-EM side, continuing the pattern noted before. My analysis of why this is occurring follows.\n\n\nCause of Bifurcation of Data Sets\nLast week and this week, we noticed that the fitted models fell into one of two groups: some approached a state of large \\(\\alpha\\) and many clusters, while others approached a state of small \\(\\alpha\\) and one or two clusters. I tried fitting iMGPE-EM models to the first 20 data sets with \\(\\alpha\\) started at a value of \\(0.1\\), hypothesizing that the posterior distribution for \\(\\alpha\\) was bimodal. However, all models converged to the same states as before.\nMy next hypothesis was that for the first group of data sets, MCRP generates more clusters on average than CRP, causing the Gibbs update step to overestimate the value of \\(\\alpha\\), which generates even more clusters in the next iteration which leads to an even higher value of \\(\\alpha\\). In the other group, the MCRP might generate fewer clusters than CRP, so the sampler would underestimate the value of \\(\\alpha\\), causing the algorithm to converge to a single cluster.\nThe first part of this hypothesis was born out when I studied prior distribution of cluster sizes given data and fixed parameters and compared it to the distribution of \\(\\alpha\\) given \\(\\phi\\) and the number of clusters \\(k\\). Ideally, there should be symmetry between the Gibbs sampling steps for \\(z\\) and \\(\\alpha\\). To be specific, we would expect symmetry between the expected number of clusters given \\(\\alpha\\) and the expected value of \\(\\alpha\\) given the number of clusters.\nRecall that \\(p(\\alpha|\\dots)\\) is only conditional on the number of data points \\(n\\) and the number of clusters \\(k\\). Let \\(E(k|\\alpha=a)=\\bar k_a\\) Then we should have \\(E[\\alpha | k=\\bar k_a]=a\\). But when we draw samples from \\(p(z|\\alpha,\\phi)\\) and \\(p(\\alpha|n,k)\\), we see that this does not hold.\nFor each of last week’s \\(20\\) data sets, I drew \\(1000\\) samples of \\(z\\) from the MCRP for each of three values of \\(\\alpha\\), holding \\(\\phi\\) at \\(1\\). I got the same median number of clusters (and nearly the same mean number of clusters) for all of the data sets. For each of the \\(1000\\) simulated values of \\(k\\), I generated a sample of \\(\\alpha\\) and inspected the mean of the \\(1000\\) alpha values.\n\n\n\nInitial \\(\\alpha\\)\n\\(E(k|\\alpha)\\)\n\\(E(\\alpha|k)\\)\n\n\n\n\n0.2\n1.3\n0.39\n\n\n0.5\n2.4\n0.56\n\n\n1\n4.9\n1.04\n\n\n5\n27.5\n10.96\n\n\n25\n48.1\n30.63\n\n\n\nIt is apparent that for \\(\\alpha &gt; 1\\), \\(E(\\alpha |\\bar k)\\) is much larger than the value of \\(\\alpha\\) that generated the samples of \\(k\\). This leaves the question of why the iMGPE-EM models sometimes degenerate to a single cluster. My best guess for now is that the conditioning on \\(y\\) that occurs in the Gibbs sampling step for \\(z\\) can alter or even invert this relationship.\n\n\nCategorical Inputs\nI generated a simulated data set with one continuous input \\(x_1\\) and one categorical input \\(x_2\\) with three levels. The response took values according to the following function:\n\\[y=f(x_1,x_2)=\\begin{cases} x_1^2\\text{ if } x_2=1\\\\\n4-x_1\\text{ if }x_2=2\\\\\n(2-x_1)^2\\text{ if } x_2=3\n\\end{cases}\\]\nI compared two different methods of handling categorical inputs. The first method is to include categorical inputs in clustering, but not in the GP experts. This relies only on the distances between each pair of points on each input. I am using the Gower distance for categorical inputs, that records a 1 if two points are the same on that input and a 0 if they are different.\nThe second method is to convert categorical inputs to dummy variables, treat them as continuous inputs, and include the dummy variables in both clustering and the GP experts.\nI fit one iMGPE-EM model with each method, running each for 2000 iterations. A plot of the estimated median and \\(95\\%\\) credible intervals for each group for the first method are shown below.\n\n\n\n\n\n\n\n\n\nThe estimated median and \\(95\\%\\) credible intervals for the second method, using dummy variables, is shown here.\n\n\n\n\n\n\n\n\n\nIncluding the categorical inputs in both the CRP and the GP experts produced much better results. This is not unexpected, especially as the clusters in this example are very distinct."
  },
  {
    "objectID": "index.html#november-13-2025",
    "href": "index.html#november-13-2025",
    "title": "Weekly Reports",
    "section": "November 13, 2025",
    "text": "November 13, 2025\n\nSummary\nThis week I derived a pdf for the cluster assignments under MCRP using Brook’s lemma. I also investigated EM methods for infinite mixtures and found no direct analogue to iMGPE-EM. It is more like an EM algorithm than anything else, but I don’t think it is a proper MCEM. I continued investigating the role of \\(\\alpha\\) in the MCRP and suspect my implementation may be flawed."
  },
  {
    "objectID": "index.html#brooks-lemma",
    "href": "index.html#brooks-lemma",
    "title": "Weekly Reports",
    "section": "Brook’s Lemma",
    "text": "Brook’s Lemma\nThis week I applied a version of Brook’s lemma to learn the distribution of the cluster indicators under MCRP given the MCRP parameters. Brook’s lemma says that given a fixed cluster arrangement \\(z'\\) for \\(n\\) observations we have\n\\[p(z|\\alpha,\\phi)=\\prod_{i=1}^n \\frac{p(z_i|z'_{1:i-1},z_{i+1:n})}{p(z'_i|z_{1:i-1},z'_{i+1:n})} p(z'|\\phi,\\alpha)\\]\nFor simplicity, I took \\(z'=(1,\\dots,n)\\) so that every observation is in a separate cluster. Then \\(p(z'_i|z_{1:i-1},z'_{i+1:n})\\propto \\alpha\\) for all \\(i\\) and \\(p(z'|\\phi,\\alpha)\\propto \\alpha^n\\). We can glean further insights about \\(p(z_i|z'_{1:i-1},z_{i+1:n})\\) if we compare a hypothetical cluster assignment vector to \\(z'\\):\n\\[\\begin{cases} z=\\{1,1,3,1,3,6,7,7,7\\} \\\\\nz'=\\{1,2,3,4,5,6,7,8,9\\}\n\\end{cases}\\]\nI have number the clusters in \\(z\\) according to the index of the first member in this arbitrary sequence of observations. Note that \\(p(z_i|z'_{1:i-1},z_{i+1:n})\\propto \\alpha\\) only when \\(z_i\\) is in a singleton cluster. For example, here, only \\(p(z_6|z'_{1:5},z_{7:9})\\propto\\alpha\\). Otherwise, the probabilities of \\(z_i=j\\) will be proportional to \\((n-1)(\\sum_{i'\\neq i,z^*_{i'}=j}K_{\\phi}(X_i,X_{i'})/\\sum_{i'\\neq i}K_{\\phi} (X_i,X_{i'}))\\) where \\(z^*\\equiv \\{z'_{1:i-1},z_{i+1:n}\\}\\) and \\(K_{\\phi}(\\cdot,\\cdot)\\) is a kernel or distance function parameterized by \\(\\phi\\), such as the squared Gaussian kernel: \\(K_{\\phi}(X_i,X_j)=\\exp(-(X_i-X_j)^2/\\phi)\\).\nLet \\(S_i\\) be the event that the \\(i^{th}\\) observation is clustered by itself in \\(z\\) and let \\(S_i'\\) be its complement. Then\n\\[p(z|\\alpha,\\phi)=\\left[\\prod_{i=1}^n p(z_i|z'_{1:i-1},z_{i+1:n}) \\right]\\left(\\frac{n+\\alpha-1}{\\alpha} \\right)^n \\left(\\frac{\\alpha}{n+\\alpha-1} \\right)^n\\] \\[=(n+\\alpha-1)^{-n}\\prod_{i=1}^n\\left((n-1)\\frac{\\sum_{i'\\neq i,z^*_{i'}=j}K_{\\phi}(X_i,X_{i'})}{\\sum_{i'\\neq i}K_{\\phi} (X_i,X_{i'})}I(S_i') + \\alpha I(S_i) \\right)\\]"
  },
  {
    "objectID": "index.html#mcem-and-imgpe-em",
    "href": "index.html#mcem-and-imgpe-em",
    "title": "Weekly Reports",
    "section": "MCEM and iMGPE-EM",
    "text": "MCEM and iMGPE-EM\nThere does not appear to be a direct analogue to iMGPE-EM. I uncovered several methods of adapting EM to fit an infinite mixture model but none involved optimizing the parameters of the cluster experts directly. The approaches I found instead optimized the parameters of the expert’s prior distribution. In our model, that would be the prior on \\(\\theta\\) itself.\nOne method of doing this is with a truncated approximation of the Dirichlet process prior. This approach is taken by Kimura et al. in the 2011 paper “Expectation-Maximization Algorithms for Dirichlet Process Mixtures”. Instead of using a DP prior with support over an infinite number of components, we can truncate the support of the DP to some upper bound \\(K\\), chosen to be much larger than the anticipated number of clusters. Once the model is fit, we can discard any components with a weight below some threshold (the authors use \\(0.0001\\)) leaving only those that are significant.\nAnother approach is proposed by Blei and Jordan in their 2004 paper “Variational Inference for Dirichlet Process Mixtures”. They use variational inference to optimize the hyperparameters of the component distributions. I have discussed this method in the case of infinite mixtures of Gaussian processes in my report on a paper by Sun and Xu on February 13, 2025.\nI still believe that iMGPE-EM is an EM algorithm, or at least is more like EM than anything else, but it does not appear to match any existing EM method. All the EM algorithms I could find chose to optimize a parameter vector of fixed and known length, unlike iMGPE-EM which optimizes a parameter vector of variable length."
  },
  {
    "objectID": "index.html#investigations-on-alpha",
    "href": "index.html#investigations-on-alpha",
    "title": "Weekly Reports",
    "section": "Investigations on Alpha",
    "text": "Investigations on Alpha\nI have continued to study how \\(\\alpha\\) affects the number of clusters generated by the MCRP and the iMGPE algorithms more generally but do not have concrete results yet. I have confirmed that the code for iMGPE-Slice has not changed since my last experiment with fixed values of \\(\\alpha\\) on August 28, 2025.\nI also investigated the Monte Carlo sampling procedure I used last week to construct samples of the cluster assignment vector. The MCMC chain of the number of clusters generated generally converged within 100 or so iterations. The mean number of clusters is slightly larger if we discard the first 500 iterations as burn-in but the overall pattern does not change. That is, if \\(E(k|\\alpha=x)=k_x\\) then we see that \\(E(\\alpha|k_x) &gt; x\\).\nThis does not seem like it should be happening. I suspect that there is an error in either my implementation of the MCRP or in the conditional probability of \\(\\alpha\\) that I use for slice sampling. I am reviewing the derivation of these components to see if I can find any inaccuracies."
  },
  {
    "objectID": "index.html#november-20-2025",
    "href": "index.html#november-20-2025",
    "title": "Weekly Reports",
    "section": "November 20, 2025",
    "text": "November 20, 2025\n\nSummary\nThis week, I continued investigating the pmf proposal I derived from Brook’s lemma for the joint distribution of cluster assignments under MCRP. Based on my results, I think a proper joint distribution does not exist.\nI have also experimented with holding \\(\\alpha\\) constant for the iMGPE-Slice algorithm, this time assessing convergence of parameters and predictive point estimates with the Geweke criterion.\n\n\nBrook’s Lemma\nLast week, I derived a proposal for the joint probability mass function of the vector of cluster indicator variables \\(z\\) generated by the Modified Chinese Restaurant Process using Brook’s Lemma. Let \\(S_i\\) be the event that the \\(i^{th}\\) observation is clustered by itself in \\(z\\) and let \\(S_i'\\) be its complement. Let \\(z^*_i \\equiv \\{1,\\dots,i-1,z_{i+1:n}\\}\\). Assuming an unknown normalizing constant on \\(p(z'=1:n|\\alpha,\\phi)\\), we know the pmf up to its normalizing constant.\n\\[p(z|\\alpha,\\phi)\\propto (n+\\alpha-1)^{-n}\\prod_{i=1}^n\\left((n-1)\\frac{\\sum_{i'\\neq i,z^*_{i'}=j}K_{\\phi}(X_i,X_{i'})}{\\sum_{i'\\neq i}K_{\\phi} (X_i,X_{i'})}I(S_i') + \\alpha I(S_i) \\right)\\]\nThe next step is to determine whether this formula sums to one across all possible cluster assignments. In the case of \\(n=2\\), there are two possible cluster assignments: \\(z=(1,1)\\) and \\(z=(1,2)\\). I maintain the convention that each cluster is indexed according to its leftmost member to preserve uniqueness.\nLet \\(K_{ij}=K_{\\phi}(X_i,X_j)\\) for simplicity. Then the two cases are\n\\[p(z=(1,1)|\\alpha,\\phi) \\propto (2+\\alpha-1)^{-2}\\left(1\\frac{K_{12}}{K_{12}} \\right)=\\frac{1}{(1+\\alpha)^2}\\]\n\\[p(z=(1,2)|\\alpha,\\phi)\\propto (2+\\alpha-1)^{-2} \\alpha^2 =\\frac{\\alpha^2}{(1+\\alpha)^2}\\]\nThese add to \\((1+\\alpha^2)/(1+\\alpha)^2\\) rather than one, so the normalizing constant must be \\((1+\\alpha)^2/(1+\\alpha^2)\\) if our formula is a valid pmf.\nNow consider \\(n=3\\). There are five unique cluster assignments: \\((1,1,1)\\), \\((1,1,3)\\), \\((1,2,1)\\), \\((1,2,2)\\), and \\((1,2,3)\\). Their unnormalized probabilities are\n\\[p(z=(1,1,1))\\propto (3+\\alpha-1)^{-3}\\left(2 \\right)^3 \\frac{K_{13}}{K_{13}+K_{23}}= \\frac{8(K_{13}/(K_{13}+K_{23}))}{(2+\\alpha)^3}\\]\n\\[p(z=(1,1,3))\\propto (2+\\alpha)^{-3}\\left(\\frac{2K_{12}}{K_{12}+K_{13}} \\right) \\left(\\frac{2K_{12}}{K_{12}+K_{23}} \\right)\\alpha\\] \\[p(z=(1,2,1))\\propto (2+\\alpha)^{-3}\\left(\\frac{2K_{13}}{K_{12}+K_{13}} \\right)\\alpha \\left(\\frac{2K_{13}}{K_{13}+K_{23}} \\right)\\] \\[p(z=(1,2,2))\\propto (2+\\alpha)^{-3}\\alpha \\left(\\frac{2K_{23}}{K_{12}+K_{23}} \\right) \\left(\\frac{2K_{23}}{K_{13}+K_{23}} \\right)\\] \\[p(z=(1,2,3))\\propto (2+\\alpha)^{-3}\\alpha^3=\\frac{\\alpha^3}{(2+\\alpha)^3}\\]\nThese five do not sum to one and the normalizing constant we got for \\(n=2\\) will not normalize them. Consequently, I think \\(p(z|\\alpha,\\phi)\\) is not a valid probability mass function and the MCRP may not have a valid joint distribution over its cluster indicators.\n\n\nFixed Alpha & Convergence\nI repeated my previous experiments with holding \\(\\alpha\\) constant while running the iMGPE algorithm, but monitored convergence to verify the results. The quantities monitored were \\(\\phi\\), the number of clusters, and the expected value of each point in the test set. I ran the iMGPE-Slice algorithm for 4000 iterations for each \\(\\alpha\\). I held \\(\\alpha\\) constant for values of 1, 5, and 25.\nI evaluated the convergence of the last 2000 iterations using the Geweke criterion, which produces a standard normal test statistic under the null hypothesis that the chain has converged. For \\(\\alpha=1\\), \\(99\\%\\) of the predicted values had test statistics within \\(1.96\\), well within what we would expect. For \\(\\alpha=5\\), only \\(70\\%\\) of the test statistics for predicted values were within \\(1.96\\), indicating possible non-convergence. For \\(\\alpha=25\\), \\(100\\%\\) of the test statistics were under the threshold. In summary, the chains of predicted values for \\(\\alpha=1\\) and \\(\\alpha=25\\) seem to have converged, while the chain for \\(\\alpha=5\\) may not have. I did not have time to run \\(\\alpha=5\\) for more iterations.\nBelow are the estimated median functions with \\(95\\%\\) credible intervals for each \\(\\alpha\\). The grey ribbon at the bottom of each plot shows the width of the credible interval.\n\n\n\n\n\n\n\n\n\nAs before, the quality of the median estimate degrades as \\(\\alpha\\) increases. The credible intervals, interestingly, get narrower as performance degrades. I suspect this is because as \\(\\alpha\\) increases, the GP experts have less data to work with and their predictions converge to to their common prior with mean zero.\nBelow, the trace plots for \\(\\phi\\), number of clusters, and the predictive point estimate for \\(x=0.01\\) are shown for each \\(\\alpha\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe mean number of clusters given \\(\\alpha\\) stayed roughly the same between this week’s results and August 28’s results. The Geweke test statistics for \\(\\phi\\) were \\(-1.35\\), \\(1.39\\), and \\(1.32\\) for \\(\\alpha=1,5,25\\), indicating that they all converged. The Geweke test statistics for the number of clusters were \\(-1.23\\), \\(-0.81\\), and \\(0.66\\) for \\(\\alpha=1,5,25\\) indicating they all converged."
  },
  {
    "objectID": "index.html#brooks-lemma-1",
    "href": "index.html#brooks-lemma-1",
    "title": "Weekly Reports",
    "section": "Brook’s Lemma",
    "text": "Brook’s Lemma\nThis week I applied a version of Brook’s lemma to learn the distribution of the cluster indicators under MCRP given the MCRP parameters. Brook’s lemma says that given a fixed cluster arrangement \\(z'\\) for \\(n\\) observations we have\n\\[p(z|\\alpha,\\phi)=\\prod_{i=1}^n \\frac{p(z_i|z'_{1:i-1},z_{i+1:n})}{p(z'_i|z_{1:i-1},z'_{i+1:n})} p(z'|\\phi,\\alpha)\\]\nFor simplicity, I took \\(z'=(1,\\dots,n)\\) so that every observation is in a separate cluster. Then \\(p(z'_i|z_{1:i-1},z'_{i+1:n})\\propto \\alpha\\) for all \\(i\\) and \\(p(z'|\\phi,\\alpha)\\propto \\alpha^n\\). We can glean further insights about \\(p(z_i|z'_{1:i-1},z_{i+1:n})\\) if we compare a hypothetical cluster assignment vector to \\(z'\\):\n\\[\\begin{cases} z=\\{1,1,3,1,3,6,7,7,7\\} \\\\\nz'=\\{1,2,3,4,5,6,7,8,9\\}\n\\end{cases}\\]\nI have number the clusters in \\(z\\) according to the index of the first member in this arbitrary sequence of observations. Note that \\(p(z_i|z'_{1:i-1},z_{i+1:n})\\propto \\alpha\\) only when \\(z_i\\) is in a singleton cluster. For example, here, only \\(p(z_6|z'_{1:5},z_{7:9})\\propto\\alpha\\). Otherwise, the probabilities of \\(z_i=j\\) will be proportional to \\((n-1)(\\sum_{i'\\neq i,z^*_{i'}=j}K_{\\phi}(X_i,X_{i'})/\\sum_{i'\\neq i}K_{\\phi} (X_i,X_{i'}))\\) where \\(z^*\\equiv \\{z'_{1:i-1},z_{i+1:n}\\}\\) and \\(K_{\\phi}(\\cdot,\\cdot)\\) is a kernel or distance function parameterized by \\(\\phi\\), such as the squared Gaussian kernel: \\(K_{\\phi}(X_i,X_j)=\\exp(-(X_i-X_j)^2/\\phi)\\).\nLet \\(S_i\\) be the event that the \\(i^{th}\\) observation is clustered by itself in \\(z\\) and let \\(S_i'\\) be its complement. Then\n\\[p(z|\\alpha,\\phi)=\\left[\\prod_{i=1}^n p(z_i|z'_{1:i-1},z_{i+1:n}) \\right]\\left(\\frac{n+\\alpha-1}{\\alpha} \\right)^n \\left(\\frac{\\alpha}{n+\\alpha-1} \\right)^n\\] \\[=(n+\\alpha-1)^{-n}\\prod_{i=1}^n\\left((n-1)\\frac{\\sum_{i'\\neq i,z^*_{i'}=j}K_{\\phi}(X_i,X_{i'})}{\\sum_{i'\\neq i}K_{\\phi} (X_i,X_{i'})}I(S_i') + \\alpha I(S_i) \\right)\\]"
  },
  {
    "objectID": "index.html#december-4-2025",
    "href": "index.html#december-4-2025",
    "title": "Weekly Reports",
    "section": "December 4, 2025",
    "text": "December 4, 2025\n\nSummary\nThis week I continued my study of the pmf of the cluster assignment vector for the Modified Chinese Restaurant Process. I am confident that a valid normalizing constant can always be constructed from \\(n\\), \\(X\\), \\(\\alpha\\), and \\(\\phi\\) but I have not been able to identify a general form for the normalizing constant, limiting its utility.\nI have also confirmed that the conditional likelihood I had used in the Gibbs sampling step for \\(\\alpha\\) was the likelihood for a standard CRP, which is why previous experiments showed that it overestimated the value of \\(\\alpha\\) given a number of clusters. I uncovered a simple linear relationship between \\(p_{CRP}(\\alpha|n,k)\\) and \\(p_{MCRP}(\\alpha|n,k)\\).\n\n\nMCRP PMF\nBased on Brook’s lemma, an unnormalized expression for the pmf of the cluster assignment vector for the Modified CRP is below. Let \\(S_i\\) be the event that the \\(i^{th}\\) observation is clustered by itself in \\(z\\) and let \\(S_i^C\\) be its complement. Let \\(z^*_i \\equiv \\{1,\\dots,i-1,z_{i+1:n}\\}\\).\n\\[p(z|\\alpha,\\phi)\\propto (n+\\alpha-1)^{-n}\\prod_{i=1}^n\\left((n-1)\\frac{\\sum_{i'\\neq i,z^*_{i'}=j}K_{\\phi}(X_i,X_{i'})}{\\sum_{i'\\neq i}K_{\\phi} (X_i,X_{i'})}I(S_i^C) + \\alpha I(S_i) \\right)\\]\nThe normalizing constant for \\(n=2\\) is \\((\\alpha+1)^2/(\\alpha^2+1)\\). Let \\(K_{ij}=K_{\\phi}(X_i,X_j)\\), \\(KK_i= \\sum_{j\\neq i} K_{ij}\\) and let \\(D_K=\\prod_{i=1}^n KK_i\\). Then the normalizing constant for \\(n=3\\) is\n\\[(2+\\alpha)^3D_K\\left[8D_K+4\\alpha\\left(K_{12}^2KK_3 + K_{13}^2KK_2 +K_{23}^2 KK_3 \\right) +\\alpha^3 D_K \\right]^{-1}\\]\nI also constructed the unnormalized pmf expressions for \\(n=4\\). It was not necessary to write out the expressions for every cluster arrangement, but there are 15 possible cluster assignments and I deduced the normalizing constant to be the following expression.\n\\[(3+\\alpha)^3D_K\\left[P_K +Q_K+R_K +\\alpha^4D_K \\right]^{-1}\\] where \\[P_K= 3^4\\left((K_{13}+K_{34})K_{14}KK_1KK_2 +K_{12}^2 K_{34}^2 +K_{13}^2K_{24}^2 +K_{14}^2K_{23}^2 \\right)\\] \\[Q_K= 3^3\\alpha\\big((K_{12}+K_{13})(K_{12}+K_{23})K_{13}KK_4 +(K_{12}+K_{14})(K_{12}+K_{24})K_{14}KK_3\\] \\[+(K_{13}+K_{14})(K_{13}+K_{34})K_{14}KK_2 +(K_{23}+K_{24})(K_{23}+K_{34})K_{24}KK_1\\big)\\] \\[R_K= 3^2\\alpha^2\\big(K_{12}^2KK_3KK_4 +K_{13}^2KK_2KK_4 +K_{14}^2KK_2KK_3\\] \\[+K_{23}^2KK_1KK_4 +K_{24}^2KK_1KK_3 +K_{34}^2KK_1KK_2 \\big)\\]\nIt is evident that a valid normalizing constant can be constructed for any \\(n\\) that is a function only of \\(n\\), \\(X\\), \\(\\alpha\\), and \\(\\phi\\). However, I have not been able to determine a general formula for the normalizing constant. The joint distribution of the cluster indicator variables does exist, but as far as I can tell it is intractable when \\(n\\) is not very small.\n\n\nLikelihood of Alpha\nThe conditional probability of \\(\\alpha|n,k\\) that I had been using was derived for an infinite Gaussian process mixture model with a Dirichlet process prior, that is, for an unmodified Chinese restaurant process. Without a general form for the pmf of \\(k|\\alpha\\) it isn’t feasible to sample from the true conditional distribution so I will try estimating it another way.\nFor each of twenty initial values of \\(\\alpha\\), I simulated a Markov chain of 5000 cluster assignments from the MCRP and recorded the median number of clusters \\(k\\) after discarding 2500 iterations as burn-in. I then drew 10,000 slice samples from \\(p(\\alpha|n,k)\\) for each of the twenty median cluster counts. The simulated quantities are plotted against the initial \\(\\alpha\\) value below, along with a fitted regression line.\n\n\n\n\n\n\n\n\n\nInterestingly, both plots display a very distinct relationship. The initial value of \\(\\alpha\\) is quadratically related to the median number of clusters generated by the MCRP and linearly related to the expected value of \\(\\alpha^*\\) given that median cluster count. Specifically, we estimate \\(k=-1.73+7.46\\alpha-0.35\\alpha^2\\) and \\(a^*=-1.22+2.50\\alpha\\) with \\(R^2\\) values of \\(0.9983\\) and \\(0.9945\\) respectively.\nThis looked like the \\(\\alpha\\) values generated by the conditional for the unmodified CRP were about \\(2.5\\) times as large as the true \\(\\alpha\\) values used in the MCRP. Thus, I decided to try running the same sampling process for \\(k\\) and \\(\\alpha\\) and just apply a location-scale transformation to \\(\\alpha\\) in its sampling step. That is, setting \\(a^*=-1.22+2.50\\alpha\\), it is easy to derive \\(p(a^*|n,k)\\) from \\(p(a|n,k)\\) and slice sample from it.\nThe plot below displays a scatterplot of initial and sampled \\(\\alpha\\), a fitted regression line in blue, and a \\(y=x\\) line in red.\n\n\n\n\n\n\n\n\n\nAs expected, our starting values for \\(\\alpha\\) and those generated by the conditional probability are now approximately equal.\nI am working on a simulation study of iMGPE methods with both new approaches to sampling \\(\\alpha\\). One is the location-scale transformation of the standard CRP conditional density and the other is a direct calculation of the pmf using the formula derived from Brook’s lemma. I intend to have it ready by next week."
  },
  {
    "objectID": "index.html#december-11-2025",
    "href": "index.html#december-11-2025",
    "title": "Weekly Reports",
    "section": "December 11, 2025",
    "text": "December 11, 2025\n\nSummary\nThis week I wrote a formal proof that a valid joint distribution of cluster assignment indicators exists for the Modified Chinese Restaurant process described by Rasmussen and Ghahramani.\nI investigated the feasibility of sampling from the conditional density of \\(\\alpha\\) given this joint distribution and find that it is not possible to sample from the true density as it depends on the normalizing constant of the MCRP, which is unknown.\nLastly, I consider some ways we can move forward without needing to sample from the conditional distribution of \\(\\alpha\\).\n\n\nProof of Valid Joint Distribution\nHere, I present a formal proof that given the Modified Chinese Restaurant Process used by Rasmussen and Ghahramani, a valid joint distribution over cluster indicators can be derived from Brook’s lemma.\nLet \\(X=(X_1,\\dots,X_n)\\) be a vector of covariate information for \\(n\\) data points, where each \\(X_i\\) may also be a vector. Let \\(z\\) be a vector of indicator variables of length \\(n\\) indicating the cluster assignments of the points. Assume \\(z\\) is generated by the Modified CRP with the following probabilities of point \\(z_i\\) being assigned to cluster \\(j\\) given all other points.\n\\[P(z_i=j|z_{-i},\\dots)\\propto \\begin{cases} \\begin{align}\n&\\frac{n-1}{n+\\alpha-1}\\frac{\\sum_{i'\\neq i,z_{i'}=j} K_{\\phi}(X_i,X_{i'})}{\\sum_{i'\\neq i} K_{\\phi}(X_i,X_{i'})} & \\text{ for } j=1,\\dots,J^{-i}\\\\\n&\\frac{\\alpha}{n+\\alpha-1} & \\text{ for } j=J^{-i}+1\n\\end{align}\\end{cases}\\]\nAssume \\(\\alpha &gt;0\\) and \\(n\\geq 2\\). Assume the kernel function \\(K_{\\phi}:\\mathbb{X}^2\\to (0,\\infty)\\). That is, the kernel function returns a finite positive scalar value always. Then the ratio of kernel components in the conditional probability above is always in the interval \\((0,1]\\), since the sum in the numerator is over a subset of the values in the denominator. Note that if the numerator was equal to \\(0\\), that would imply that \\(z_i\\) is in a cluster by itself, so the indicator function on \\(S_i^C\\) would cancel that term anyways.\nNow we can apply Brook’s lemma. For a given cluster assignment \\(z\\), let \\(S_i\\) be the event that the \\(i^{th}\\) observation is clustered by itself in \\(z\\) and let \\(S_i^C\\) be its complement. Let \\(z^*_i \\equiv \\{1,\\dots,i-1,z_{i+1:n}\\}\\). Assume \\(p(z'=1:n|\\alpha,\\phi)\\) is proportional to \\(\\left(\\frac{\\alpha}{n+\\alpha-1}\\right)^n\\) with an unknown normalizing constant. Then we know the pmf up to its normalizing constant.\n\\[p(z|\\alpha,\\phi)\\propto (n+\\alpha-1)^{-n}\\prod_{i=1}^n\\left((n-1)\\frac{\\sum_{i'\\neq i,z^*_{i'}=j}K_{\\phi}(X_i,X_{i'})}{\\sum_{i'\\neq i}K_{\\phi} (X_i,X_{i'})}I(S_i^C) + \\alpha I(S_i) \\right)\\]\nBy what we have established before, we know that \\(p(z|\\alpha,\\phi)\\) is always finite and non-negative.\nThere are only a finite number of possible cluster arrangements, so there is always a finite, non-negative normalizing constant constructed as the inverse sum of \\(p^*(z|\\alpha,\\phi)\\) over all possible \\(z\\). Thus, there is a valid joint distribution for the cluster assignment indicators for any value of \\(n\\).\n\n\nSampling from the Posterior of Alpha\nWe would like to be able to sample from \\(p(\\alpha|z)\\propto p(z|\\alpha,\\phi)p(\\alpha)\\). To do this we need only the parts of \\(p(z|\\alpha,\\phi)\\) that depend on \\(\\alpha\\), which unfortunately includes its normalizing constant.\nThe general form of the normalizing constant is shown here. Let \\(S_i\\) be the event that the \\(i^{th}\\) observation is clustered by itself in \\(z\\) and let \\(S_i^C\\) be its complement. Let \\(z^*_i \\equiv \\{1,\\dots,i-1,z_{i+1:n}\\}\\).\n\\[C_n=(n+\\alpha-1)^n \\left[\\sum_z \\prod_{i=1}^n \\left((n-1)\\frac{\\sum_{i'\\neq i,z^*_{i'}=j}K_{\\phi}(X_i,X_{i'})}{\\sum_{i'\\neq i}K_{\\phi} (X_i,X_{i'})}I(S_i^C) + \\alpha I(S_i) \\right) \\right]^{-1}\\]\nNote that the \\((n+\\alpha-1)^n\\) cancels with the \\((n+\\alpha-1)^{-n}\\) component of \\(p^*(z|\\alpha,\\phi)\\). As the normalizing constant depends on \\(\\alpha\\), we cannot sample from \\(p(\\alpha|z)\\) directly.\nI hypothesized at first that the normalizing constant can be ignored, my reasoning being as follows. The cluster assignments we sum over can be grouped by the number of lone points they contain. All assignments with no lone points combine into an \\((n-1)^n H_K\\) term where \\(H_K\\) is a function of the kernel function and \\(\\phi\\) only. Since \\((n-1)^n\\) is enormous even for reasonably small values of \\(n\\), this term will dominate the sum. Thus, the normalizing constant can be approximated by only the inverse of this first term of the sum, and since that term doesn’t depend on \\(\\alpha\\), the normalizing constant could potentially be dropped.\nThen let \\(|S_i|\\) be the number of lone clusters, so \\(p(\\alpha|z)\\propto \\alpha^{|S_i|}p(\\alpha)\\) approximately.\nHowever, when I tested this hypothesis, the results were not promising. For each of three fixed values of \\(\\alpha\\), I generated a Markov chain of length \\(5000\\) of cluster assignment vectors with a burn-in period of \\(1000\\). For each of the cluster assignment vectors after burn-in, I drew a value of \\(\\alpha\\) from \\(p(\\alpha|z)\\propto \\alpha^{|S_i|}p(\\alpha)\\). For each fixed value of \\(\\alpha\\), the mean number of single clusters and the mean of the draws from \\(p(\\alpha|z)\\) are shown in the table below.\n\n\n\n\\(\\alpha\\)\n\\(E(k|\\alpha)\\)\n\\(E(\\alpha|k)\\)\n\n\n\n\n1\n3\n1.66\n\n\n5\n3\n1.38\n\n\n25\n5\n0.96\n\n\n\nThe sampled \\(\\alpha\\) values do not match the original values of \\(\\alpha\\) at all. It seems that this “approximation” of \\(p(\\alpha|z)\\) is not close.\n\n\nSimilar Work\nI searched papers citing the iMGPE article for any work done on exploring the distribution of the Modified CRP or its theoretical properties. The most relevant paper I could find was “Convergence Rates for Gaussian Mixtures of Experts” by Nhat Ho, Chiao-Yu Yang, and Michael Jordan, writing in 2022. Their paper proves the rate of convergence of MLEs for infinite mixtures of Gaussian experts with covariate-free clustering distributions. The authors remark that they chose to focus on the covariate-free context because the math is much more complicated when the cluster assignment distribution is covariate dependent.\nThe authors prove that if the experts are “algebraically independent” then the MLEs will converge at a rate of \\(O(n^{1/4})\\).\n\n\nFurther Research\nGiven that it may not be possible to sample from the conditional distribution of \\(\\alpha\\), we will have to find some workaround to continue using the iMGPE algorithm. The simplest solution would be to fix \\(\\alpha\\) instead of estimating it.\nWe could also use a covariate-independent Dirichlet process prior, that is, a standard Chinese restaurant process, though the resulting model would no longer be an iMGPE model but a basic DP mixture of GPs. If we are willing to do that, we could go further and use a different clustering distribution entirely."
  },
  {
    "objectID": "index.html#alpha-ekalpha-ealphak",
    "href": "index.html#alpha-ekalpha-ealphak",
    "title": "Weekly Reports",
    "section": "\\(\\alpha\\) | \\(E(k|\\alpha)\\) | \\(E(\\alpha|k)\\)",
    "text": "\\(\\alpha\\) | \\(E(k|\\alpha)\\) | \\(E(\\alpha|k)\\)\n1 | 3 | 1.66 5 | 3 | 1.38 25 | 5 | 0.96\nThe sampled \\(\\alpha\\) values do not match the original values of \\(\\alpha\\) at all. It seems that this “approximation” of \\(p(\\alpha|z)\\) is not close.\n\nSimilar Work\nI searched papers citing the iMGPE article for any work done on exploring the distribution of the Modified CRP or its theoretical properties. The most relevant paper I could find was “Convergence Rates for Gaussian Mixtures of Experts” by Nhat Ho, Chiao-Yu Yang, and Michael Jordan, writing in 2022. Their paper proves the rate of convergence of MLEs for infinite mixtures of Gaussian experts with covariate-free clustering distributions. The authors remark that they chose to focus on the covariate-free context because the math is much more complicated when the cluster assignment distribution is covariate dependent.\n\n\nFurther Research\nGiven that it may not be possible to sample from the conditional distribution of \\(\\alpha\\), we will have to find some workaround to continue using the iMGPE algorithm. The simplest solution would be to fix \\(\\alpha\\) instead of estimating it.\nWe could also use a covariate-independent Dirichlet process prior, that is, a standard Chinese restaurant process, though the resulting model would no longer be an iMGPE model but a basic DP mixture of GPs. If we are willing to do that, we could go further and use a different clustering distribution entirely."
  },
  {
    "objectID": "index.html#january-15-2025",
    "href": "index.html#january-15-2025",
    "title": "Weekly Reports",
    "section": "",
    "text": "Over break, I studied what the conditional density of \\(\\alpha\\) looks like for small values of \\(n\\), when the normalizing constant of \\(p(z|\\alpha)\\) can be computed exactly. I also investigated the conditional density of \\(\\alpha\\) under the distance dependent CRP and concluded that, like the MCRP, it is not tractable for large \\(n\\).\nLastly, I considered what qualities we need in a clustering algorithm for analysis of large data sets and analyze how the iMGPE algorithm could be improved to improve its predictive performance.\n\n\n\nThe distribution of the cluster assignment vector \\(z\\) is a pmf over all possible partitions of \\(n\\) data points. Indexing each cluster by the index of its leftmost data point, I can recursively construct a list of all possible partitions over \\(n\\) elements, given such a list for \\(n-1\\) elements. It works by taking each entry of the previous list and appending all possible \\(n^{th}\\) elements. For example, taking \\(z=(1,1,3,4)\\), we could append a 1, 3, 4, or 5 in the fifth position.\nThe number of possible partitions for low values of \\(n\\) are plotted below.\n\n\n\n\n\n\n\n\n\nI then explored the shape of the conditional distribution of \\(\\alpha\\) for the small value of \\(n=7\\), when the normalizing constant can be computed exactly. I used \\(p(\\alpha|z)\\propto p(z|\\alpha)p(\\alpha)\\) where \\(p(z|\\alpha)\\) is as I calculated before and \\(p(\\alpha)\\) is a Gamma(1,1) prior. This conditional distribution is plotted below for six different values of \\(z\\).\n\n\n\n\n\n\n\n\n\nNote how the shape of the distribution does not change between the first two plots or between the next two, but does change between the first, third, and fifth plots.From this, we can confirm visually that the distribution of \\(\\alpha\\) depends only on the number of singleton clusters in \\(z\\) and not on the total number of clusters. The distributions look like gamma distributions, particularly in how they may have either a hump over a particular mode value of \\(\\alpha\\) or an infinite peak at zero.\n\n\n\nThe description of the Distance Dependent CRP by Blei and Frazier does not try to learn the \\(\\alpha\\) parameter but instead holds it constant. I believe their variation on the Chinese Restaurant Process also does not produce a computationally tractable joint density over the cluster assignment vector. Rather, like the MCRP, the density’s normalizing constant can only be found as the inverse sum of an exponentially increasing number of terms.\nTo demonstrate this, we can apply Brook’s lemma to the DD-CRP. Note that the DD-CRP defines a point assignment vector \\(c\\) where each point in the data set is assigned to another point or to itself. Thus, for \\(n=2\\), there are four potential point assignments: (1,1), (1,2), (2,1), and (2,2). The state (1,1), for example, indicates that point 1 is assigned to itself and point 2 is also assigned to point 1. All points that can be connected through these assignments are in the same cluster. Thus, only (1,2) represents a state in which there are two clusters. The rest all represent a single cluster.\nLet \\(c\\) be a cluster assignment vector and let \\(c'=1:n\\). Assume that \\(p(c'|\\alpha)\\propto \\alpha^n\\). Then, according to Brook’s lemma, the probability of \\(c\\) given \\(\\alpha\\) is\n\\[p(c|\\alpha)\\propto \\prod_{i=1}^n \\left[\\frac{P(c_i=j|c_{1:i-1}',c_{i+1:n})} {P(c_i'=j|c_{1:i-1},c_{i+1:n}')} \\right]p(c'|\\alpha) =\\prod_{i=1}^n f(d_{ic_i})I(c_i\\neq i) +\\alpha I(c_i=i)\\]\nThe normalizing constant for this distribution is the inverse sum of \\(n^n\\) terms, as that is the number of possible cluster assignments. The normalizing constant when \\(n=2\\) is \\((\\alpha^2+ 2\\alpha f(d_{12}) + f(d_{12})^2)^{-1}\\), for example. Since this constant involves \\(\\alpha\\) and cannot feasibly be determined for reasonably sized \\(n\\), we cannot directly sample from \\(p(\\alpha|c)\\) under the DD-CRP.\n\n\n\nAs things currently stand, there are two feasible paths before us. We can modify the iMGPE algorithm presented by Rasmussen et al by fixing \\(\\alpha\\) instead of learning it. This would be a relatively small change that allows us to continue pursuing our current research options: the incorporation of categorical inputs, deep GPs, or variable selection.\nWe could also abandon the MCRP and seek to find or devise a new clustering mechanism to replace it. If we do so, we have the opportunity to reconsider what properties we actually need in our clustering mechanism.\nThe situation that originally drew us to the iMGPE algorithm was that we have a very large data set such that fitting a single Gaussian process to all of it is infeasible. The iMGPE algorithm was a way to partition the data into smaller clusters so we could fit multiple GPs, one to each cluster. Randomly generating a new partition at each step of a Markov chain allowed us to capture our uncertainty in the partition.\nFor our purposes, we do not necessarily need to assume a “true” underlying cluster structure in our model. We don’t necessarily care whether the partitions generated by the Markov chain are “accurate” or “informative” except in so far as we would like the data in each cluster to be similar and easily fit by a single GP. We would prefer that the data be somewhat evenly distributed between the clusters, as clusters with few points make for poor GP experts and clusters with too many take much longer to fit.\nIn conclusion, if we stick with the iMGPE, then the primary drain on predictive performance is the quality of the clusters, rather than the number. I would investigate how we could improve the similarity of points within clusters and even out the distribution of points between clusters. If we choose to select a different clustering mechanism, I would look for one with the qualities described above."
  }
]